{"title":"fast.ai Lesson 5","markdown":{"yaml":{"title":"fast.ai Lesson 5","author":"Adnan Jinnah","date":"2022-09-26","toc":true,"number-sections":true,"jupyter":"python3","categories":["fastai"]},"headingText":"Intro","containsRefs":false,"markdown":"\n\n5: From-scratch model\n\n# Lesson Overview\nThis lesson focused on creating a neural network from scratch using Python and PyTorch. It covered much content present in the textbook chapter for the previous lesson. The lecture goes through how to initialise a neural net with one hidden layer, and then how to add more layers easily in order to create a deep learning model. \n\n# The topics covered, briefly\n- How to build a neural network, from scratch.\n- Data cleaning: how to work with missing data and normalise.\n- Sigmoid function for classification.\n- Feature engineering is important for tabular data.\n- What is a binary split/OneR.\n\n# Lecture Notes:\n- For data cleaning, when there is missing data fields, usually just use the mode. Other ways might be better, but the method is usually inconsequential compared to other factors. \n- For a linear model, so when there is only one equation with coefficients linking the inputs and the output, look at the coefficients to understand what inputs most contribute to the output. \n- We normalise the input data so that while training some inputs don't overpower others. For example, if we wanted to predict lifetime duration with income and number of siblings, we have to make both variables around the same size initially so that income doesn't just dominate the output mathematically.\n- The way we prepare input data is often similar for different types of models, e.g. classification and regression.\n- In contrast the way we modify the final output is very much dependent on the model. For classification, we apply a sigmoid function in order to get the outputs to between 0 and 1. Fast.ai does this automatically. \n- For tabular data specifically, feature engineering is really important. What the columns mean and how they relate to the prediction, can literally make and break models. If you can think of a creative way the columns relate and base your model around it, you could find a new way to make competition winning predictions. It's not at all about just trying to teak different factors and hyperparameters to find the best predictions.\n- For quick iteration testing, you can keep the random seed for data creation/sampling the same. But for mature testing, make sure to vary it up because you need to know whether having different initialisations makes a difference, lest you accidentally overfit to them. \n- Ensembles, where each network has different initialisations, can be really helpful to avoid overfitting. - Since random forest models (not deep learning) are hard to mess up, they might be a good baseline against deep learning and or more complicated approaches. \n- A binary split attempts to find the best number for a variable to split the data into halves and make a prediction based on them. For example, if I wanted to predict whether someone lived in area 1 or area 2 by their income, a binary split would calculate a specific income and make all predictions above that income be area 1, and all predictions below that income, be area 2. \n\nGoing through the fundamentals of neural networks and building one from scratch atleast once I think is important  to build a foundational understanding of what you are doing. It clears up that murky feeling that machine learning is some kind of magic. It is not, it really is just a set of logical instructions which actually doesn't require more than high school maths. It reminds me of when I first had a proper maths lecture detailing integration. Previously I knew what it was and what the results meant, but the way the actual method of it worked felt like some sort of magic. One deep fundamental dive into it did well to make the concept click and clearly that murky feeling up felt amazing.\n\nIt's worth just quickly clarifying that a deep learning model is just a neural net with three or more layers. \n\n# Links\n\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson5.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"number-sections":true,"output-file":"2022-09-26-Lesson5Blog.md.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"darkly","title-block-banner":true,"title":"fast.ai Lesson 5","author":"Adnan Jinnah","date":"2022-09-26","jupyter":"python3","categories":["fastai"]},"extensions":{"book":{"multiFile":true}}}}}