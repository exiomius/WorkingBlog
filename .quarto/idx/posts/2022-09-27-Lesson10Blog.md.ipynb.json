{"title":"fast.ai Lesson 10","markdown":{"yaml":{"title":"fast.ai Lesson 10","author":"Adnan Jinnah","date":"2022-09-27","toc":true,"number-sections":true,"jupyter":"python3","categories":["fastai"]},"headingText":"Intro","containsRefs":false,"markdown":"\n\n10: Stable Diffusion 2\n\n# Lesson Overview\nThis lesson recapped conceptually how stable diffusion works, what it is, how classifier free diffusion works, and how three research papers modify it uniquely. Then there is a notebook going through how stable diffusion is implemented in HuggingFace's pipeline, as well as the first notebook of a series to recreate stable diffusion and fast.ai from scratch. \n\nI found the lesson both conceptually difficult and time consuming but very interesting. My understanding of how stable diffusion works and how to implement it has improved greatly.\n\n# Lecture Notes\n\nWe start with a recap of how the core components of stable diffusion works. <br>\nThere are three of them: The unet, CLIP, and VAE.\n\n# unet\nThe unet knows how to find the noise (the parts of the image that don't represent its label) in an image.\n\nWe train it to do so by adding noise using a scheduler to images, then giving the unet the noisy image and getting it to predict what parts are the noise.\n\n## For example, for handwritten digits:\n- 7 + Noise = Noisy 7. \n- Give Noisy 7 to unet, and unet predicts the Noise. \n- It then compares its prediction to the actual noise to get a loss to improve itself. \n- To make the unet work better, we pass in a label: an embedding of 7. For example, we could enter a one-hot encoded vector of it. \n- If we do this, then later on we can get the unet to create 7 of us because it understands what the embeddings of 7 are.\n\n# CLIP\n\nBut we don't want our unet to create handwritten digits, we want to create images, and embeddings for them cannot be one-hot encoded.\n\nSo we use CLIP to create embeddings for prompts, so we can enter an image into our unet with an embedding of its prompt, similar to how we previous entered a handwritten digit image with an embedding of its number.\n\nCLIP consists of two parts: An image encoder and an text encoder.\n\n## To train these encoders: \n- Firstly we find images online that are captioned (has a prompt).\n- We input an image into the image encoder, and this produces embeddings (a feature vector) of the image.\n- We input the image's prompt into the text encoder, and this produces embeddings (a feature vector) of the prompt. \n- We then train both to get these two sets of embeddings to be the same.\n- This is done with contrastive loss: prioritising these two sets of embeddings to be the same for when an image and prompt match, and penalising when they don't.  \n\n![image.png](attachment:image.png)\n\nBoth encoders now can produce embeddings, but we only care about the text encoder. It can take prompts of images and create embeddings for them. With this, we can input into our unet both an image and (an embedding of) its prompt!\n\n# Inference (Image Creation)\nInference is the process of creating an image from a prompt. It's also called denoising.\n\nFirst we input into the unet a image of pure (Gaussian) noise and a prompt of the image we want.\n\nThe unet then predicts what parts of that image is noise. This actually means it thinks \"which pixels from the image should I remove to get the image to look more like the prompt?\".\n\nWe could now just remove all the predicted noise in one go, but this would result in an awful image. Instead, we take the predicted noise and only remove some of it. This creates a less noisy image.\n\nWe then put our less noisy image back into our unet and repeat the process to remove more noise. Then we repeat this until we end up with our final image. Each time we remove noise is called a step. Previously it took about 60 steps to create an image, but a new paper described a method to do it in 3-4.\n\nThe diagram below shows how the image is gradually created as the steps occur:\n\n![image.png](attachment:image.png)\n\n# VAE\n\nUsing a VAE and latent images is a computational shortcut to do stable diffusion much faster.\n\nWhen we trained our unet, we implicitly used full sized images to do so, but this is really computationally expensive!\nA standard 512x512x3 image is 786432 px, which is too many to compute. \n\nWe can compress images though, while retaining the information needed to recreate them. To do so, we put the image through two convolutional layers and get a 64x64x24 image, with 16384 pixels, 48x less than the original!\nTo reconstruct the image, we simply put it through two inverse convolutional layers.\n\nWe call the compression part an encoder, and the reconstruction part a decoder. Both combined are called an autoencoder, but in the case of stable diffusion we call an autoencoder a VAE.\nWe call the compressed images, latents. \n\nWith this in mind, we create latents using the VAE's encoder. <br>\nNow we train our unet on latents instead of full sized images, and do the inference process with latents too. <br>\nWhen our fully denoised latent is created, we simply use the decoder to reconstruct it to full size.\n\n# What is Stable Diffusion?\n\nGeneral diffusion models are machine learning systems that are trained to denoise (do inference) in steps on random Gaussian noise to create a sample of interest such as images.\n\nThe difference between general diffusion models and latent diffusion models is that the latter generates and uses latent (compressed) representations of images. This reduces the memory and computational needs significantly. The difference is an autoencoder (vae), which can reduce memory usage by 48 times for a 512x512 image as (3,512,512) becomes (4,64,64) in latent space.\nIn effect, this speeds up both training and denoising significantly.\n\nStable Diffusion is based on latent diffusion. It was proposed in a paper High-Resolution Image Synthesis with Latent Diffusion Models at https://arxiv.org/abs/2112.10752.\n\n# Research Papers\n\n## Progressive Distillation for Fast Sampling of Diffusion Models:\n\nLooking at the inference process, we can reduce the steps to denoise an image using distillation.\nDistillation is a pretty common technique in deep learning.\n\nLook at step 36 and step 54 of inference below:\n\n![image.png](attachment:image.png)\n\nWhy is step 36 to 54 taking a whole 18 steps when there's not much left to do comparatively to step 0 to 18 for instance?\n\nThe reason is, it takes very long because of a side effect of how the original maths of stable diffusion works. \n\nBut the thing is, after each step, we have an output image to play with! (we only plotted the output images every 6 steps). \n\nWhat if we used a new model, unet B. unet B can take the output of step 36, and try to create the output of step 54. We can then compare the two to train unet B to learn how to get from step 36 to step 54 directly!\n\nWe have a teacher network (it already knows how to do something, but is slow and big), and a student network (it tries to do the same as the teacher but faster and with less memory). \n\nOur teacher model is the original complete stable diffusion model. Our student model is the unet B skipping steps. \n\nGenerally speaking, we start with a noisy latent and the teacher model denoises it for 2 steps (it doesn't create an image in 2 steps, just does 2 steps). Our student model then learns how to take the noisy latent and do the teacher's 2 step denoising in 1 step. <br> \nThen we get the starting noisy latent and get the student model to denoise it for 2 steps. <br>\nAnd make another student learn how to do 2 steps of this in 1 step.\n\nThe original teacher model did some denoising in 2 steps. The first student could do that denoising in 1 step, so 2 steps of it would result in 4 of the original. The second student does the first's in 1 step, so 2 steps of it results in 8 of the original. And so on, I suppose until you end up with 3-4 steps being enough to generate an image!\n\n## On Distillation of Guided Diffusion Models Paper\n\n### Classifier-free guided diffusion models (CFGD)\n\nClassifier-free guided diffusion models (CFGD) is a technique to control how strongly our output image matches our prompt. <br>\n\nSay we want to create a photo of a cute puppy.\nWe put \"a cute puppy\" into CLIP (its text_encoder) to get an embedding of the prompt.\nWe then put this embedding into our unet with a pure noise latent to generate our puppy picture for us.\nThis is normally how we do stable diffusion, but CFGD allows us to control how strongly this generated image matches our cute puppy prompt.\n\nHere's how it works:\nPut an empty \"\" prompt into CLIP too to get another embedding. This embedding is particular because it represents nothing. If we did inference just on this, we would essentially be telling our unet \"generate an image without any guidance, no restrictions, as long as it looks good\". \n\nInstead, we concatenate the 'a cute puppy' prompt embeddings with the blank \"\" prompt embeddings, and put them into our unet with a pure noisy latent.\nThe unet outputs an image representing the real prompt, and another image representing the fake prompt. We combine these two images together. \n(**I don't understand why the unet produces 2 images, shouldn't it just produce 1 image of them combined in the first place? That's how the notebook code did it**)\n\nTo recap, we just did the first step of inference, we took a noisy latent and removed a bit of noise to make it look more the embedding of the prompt \"a cute puppy\" concatenated with the embeddings of the prompt \"\". \n\nWe then go onto doing inference as normal, removing noise step by step until we get our final image. The point of this is that, we can control how much the final image should rely on the real prompt versus the blank one. The latter is high guidance, the former is less. This allows us to control how strictly the real prompt is followed.\n\n### The Paper\n\nThis way of doing CFGD is awkward because the unet has to output two images instead of the usual one, and for other reasons. The paper details a way to skip it. We do teacher student distillation again!\n\nOur normal stable diffusion with CFGD is the teacher model. It does CFGD with a number of different levels of guidance, 2, 4, 5, 12 etc.\nIt does inference by starting with a noisy latent, then creating an image with guidance introduced. \nOur student model is another unet, unet B. Very similarly to how it worked in the previous paper, it looks at the different step outputs created by the teacher model, and learns how to do guidance like the teacher, but in a much more less awkward way. \n\nThere is an extra video on this, walking through the paper. It's useful to watch this to learn how to read papers properly, as in, what's important and such, in particular, the most important part is usually the algorithm.  \n\n## Imagic: Text-Based Real Image Editing with Diffusion Models\n\nThis is another paper that just came out 3 hours ago as of writing!\n\nGive it an input image, and pass in a prompt. It tries to edit the input image to match the prompt. \n\n![image.png](attachment:image.png)\n\nThis paper is for imagen, a different image generation algorithm, but it also works fine for stable diffusion.\n\nGenerally speaking, it works through fine-tuning and optimising embeddings. \n\nWe start with a fully trained stable diffusion model.\n\nOur input image is \n\n![image.png](attachment:image.png)\n\nWe want an image of it spreading its wings.\n\nUse CLIP to get an embedding of our prompt \"a bird spreading its wings\". Use this embedding for inference as usual. This will create a photo of a bird spreading its wings, but not a photo of the bird from out input image spreading its wings. \n\nTo remedy this, we fine tune the prompt embedding to try and get it to create an image similar to the input bird image. We only do this a little and lock it, it cannot change any more. Now we have  an optimised embedding.\n\nNow we fine tune the entire stable diffusion model to generate images that look like our bird from the input image. Now we have a fine-tuned diffusion model.\n\nFinally, we combine the optimised embedding and the original embedding for our target prompt, and pass it through the fine-tuned diffusion model to create our output image!\n\n![image.png](attachment:image.png)\n\nMaybe it would only take like an hour to do this process using stable diffusion on GPUs!\n\n# Notebook: Stable Diffusion with Diffusers: Looking inside the Pipeline\n\nJeremy goes through the notebook: stable_diffusion.ipynb, available on a GitHub depository. \nI watched his walkthrough, made notes, then cloned the notebook and ran through it while commenting. This was a great learning experience, and made my understanding of the concepts much more solid.\n\nThere is a useful function, latents_callback, that shows a diagram of the images getting less noisy over time during inference.\n\nFor example:\n\n![image.png](attachment:image.png)\n\nBecause latents_callback is a parameter in pipeline, to use it I need to know how to create a pipeline out of it's components, which I have not yet covered.\n\n# Notebook: Matrix Multiplication from foundations\n\nNow what we're going to do is rebuild all of these functions from scratch. We're going to learn how to create a framework from some foundations and this will help with implementing new research papers, advanced debugging, and learning how to use Python!\n\n## Our foundations are:\n- Python\n- Matplotlib\n- The Python Standard Libary\n- Jupyter Notebooks and nbdev\n\nTo be clear, after we have created a function, we're allowed to use a module to do it. For example, we don't start with Numpy arrays, but after we've made our own we can use them. \n\nThis is also how machine learning models will work. We cannot train full models on our own, so we will create and train small models, and then allow ourselves to use pretrained models online.\n\nThis is a challenge that will be different, but it's often the part of the course where people learn the most. This year around it will be the best to date, and is more than worth doing.\n\nThe notebooks are from the part 2 repo on GitHub.\n\n## Useful Shortcuts and Tools\n\n- Shift-m in Jupyter allows you to combine cells together.\n- Cntrl-shirt-minus allows you to separate them. \n- Alt-enter inserts cells.\n- Run a function then ? to get a brief description.\n- Run a function then ?? to get documentation.\n- Inside a function, press shift-tab to see parameters available quickly. \n\n## Advice\n\n- Read the Python documentation a lot, it's good.\n- For every single method, Jeremy reads the documentation and practices it.\n\n## Notes\nI made notes on the lecture, then went through the notebook myself and rewrote the notes while running through and testing the cells. \n\n# Links\n- As I am doing this lesson as it is released privately live, I cannot share links to the resources.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"number-sections":true,"output-file":"2022-09-27-Lesson10Blog.md.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"darkly","title-block-banner":true,"title":"fast.ai Lesson 10","author":"Adnan Jinnah","date":"2022-09-27","jupyter":"python3","categories":["fastai"]},"extensions":{"book":{"multiFile":true}}}}}