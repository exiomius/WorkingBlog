{"title":"Masters Project 11/10/2022","markdown":{"yaml":{"title":"Masters Project 11/10/2022","author":"Adnan Jinnah","date":"2022-10-11","toc":true,"number-sections":true,"jupyter":"python3","categories":["Masters"]},"headingText":"Intro","containsRefs":false,"markdown":"\n\nThe third post from a series of posts about my Masters project with the Physics Department at Durham University.\n\n# Meeting:\n## During the meeting we discussed:\n- How to introduce more Physics into the project. This would be done namely using stereo audio, which is audio recorded in 3 dimensions. The idea is that using the Doppler effect and other Physics concepts, we could come up with a better audio collection technique that would make the model classification better. However, it may take a lot of time and effort to understand how to setup multiple microphones and analyse their data, given that there are features such as their synchronisation, batteries, and calibration. In total we have around 90 microphones? Because stereo is 3D, we could use it to classify where a bird singing is, and multiple birds singing. Would also need to account for audio reflecting off surfaces.\n- A previous masters student at Durham, Stuart, worked with the biology department to create his thesis which may contain useful domain knowledge for this project. It can be found at http://etheses.dur.ac.uk/11481/. He is now doing a PhD in Scotland and his further work may be worth looking into, or he may be worth directly getting in contact with to collaborate.\n- The biology department have published a paper 3 years ago about using a ensemble models to classify the cocktail problem. While the models and approach may be outdated, the data preprocessing steps might be similar to what I would have to do now.\n- I would have to have a look at the collected datasets to get an idea about their properties and train accordingly. Some of their data was painstakenly hand labeled.\n- For humans, some bird song is easier classified by image, and some by sound. I suppose this is why ensembles are a good approach. \n- For the aims of this project, I would need to define a custom metric for the biology department's needs. For example, if we really want a certain species of bird, we could penalise false positives of it very harshly. \n- About volunteer interaction: some birds are really hard to classify that even professors struggle. If you were to have a volunteer system, you should have a delegation system to assign hard tasks to experienced users.\n- Because humans have to listen to the same audio file multiple times to recognise birds, I wonder if we can train or apply a model this way.\n- Training on bad quality data, which xeno-canto labels, might be a good idea to improve generalisability.\n- Some bird species have different times at which they sing in the day, so it could be worthwhile including this in the training data somehow.\n- An interesting problem is say, looking at how the frequency of a rare species changes over time in a given area. Since we have a dataset collected over 3 years, this in theory would be possible. \n\n# Problem clarification\n\n## There are two separate and related problems for classifying birdsong:\n- The easier problem, trying to classify birds from an audio recording of their song in a vacuum.\n- The harder problem, trying to classify birds from an audio recording of their song in a noisy environment.\n\nMy Masters project is attempting the latter problem. However some testing and knowledge of the former problem is worthwhile regardless.\n\n# Work done \n\n### Practiced CNNs and the easier classification problem:\n- Because this project will use CNNs, I revised the code for a basic image classifier at https://www.kaggle.com/adnanjinnah/bird-image-classifier/.\n- More interestingly, I found a blog post online https://medium.com/mlearning-ai/bird-sound-calssification-using-fastaudio-fastai-49eea9b5953a that attempts to solve the easier classification problem. It uses the intuitive classification approach present in one of the project's proposal references: convert the audio into an image and use a CNN. The post also shows how to download birdsong audio directly from https://xeno-canto.org/?language=en, a volunteer website dedicated to collecting wildlife sounds. I figured out how to download and label birdsong audio of different species. My code is at https://www.kaggle.com/adnanjinnah/birdsong-easy-1-0/\n\n\n### Did research about modern approaches:\n\nThere is a small google research group with a webpage at https://bird-mixit.github.io/\nThey have published a paper \"IMPROVING BIRD CLASSIFICATION WITH UNSUPERVISED SOUND SEPARATION\" at https://arxiv.org/pdf/2110.03209.pdf. They have not yet released their classification model online, but have released their sound-separation model at https://github.com/google-research/sound-separation/tree/master/models/bird_mixit.\n\nFurthermore, in their paper they talk about a Kaggle competition, BirdCLEF, at https://www.kaggle.com/competitions/birdclef-2022/overview hosted by the Cornell Lab of Ornithology with a $10,000 prize pool. The competition is recent, ending four months ago. While the google team's results are better, this competition contains the code of the participants, including the winner, as well as discussion threads about them. \n\nReverse engineering the competition code may be invaluable to my Masters. To do so however, knowledge from fast.ai part 2 will likely be required as well as a lot of time and effort. Thankfully, Kaggle allows you to post your models and see how they would compare on the leaderboard, so I can continuously try different approaches and evaluate how good they are.\n\nAn important point is that the google team nor Kaggle competition use accuracy as their final metric. While accuracy initially may seem intuitive to understand, there are many issues with it. That's why the competition used a metric called F1, which is explained by deepmind at https://deepai.org/machine-learning-glossary-and-terms/f-score, and another post about interpreting it at https://inside.getyourguide.com/blog/2020/9/30/what-makes-a-good-f1-score.\n\n### Finished the fast.ai part 1 course lectures, questions, and blog posts:\n\nThe last two lessons I completed this week can be found at\nhttps://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson7Blog.md.html \nand https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson8Blog.md.html\n\nAlthough I've technically finished the course, the knowledge hasn't been properly internalised until I spend more time creating models and testing things out. For this reason, the course creator Jeremy says that it's entirely normal to finish the course and spend months afterwards revising and practicing it. For now though, it should be enough to start the second half and continue to learn as I practice.\n\n# Kaggle Competitions:\nIn theory, the biology department could put up a Kaggle competition for their own exact needs with/without prize money and see what users submit. This would however still require knowledge about how to upload their data, create correct training, validation, and test data, set the correct metrics, interpret the data, and check that submissions are valid.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"number-sections":true,"output-file":"2022-10-06-MP3.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"cosmo","title-block-banner":true,"title":"Masters Project 11/10/2022","author":"Adnan Jinnah","date":"2022-10-11","jupyter":"python3","categories":["Masters"]},"extensions":{"book":{"multiFile":true}}}}}