{"title":"fast.ai CLA Lesson 1","markdown":{"yaml":{"title":"fast.ai CLA Lesson 1","author":"Adnan Jinnah","date":"2022-10-17","toc":true,"number-sections":true,"jupyter":"python3","categories":["fastaiCLA"]},"headingText":"Intro","containsRefs":false,"markdown":"\n\nThe first lesson of fast.ai's Computational Linear Algebra course.\n\n# Course Introduction\nAn introductory blog post for this course can be found here https://www.fast.ai/posts/2017-07-17-num-lin-alg.html. <br>\n\nBecause machine learning is largely about manipulating data, and almost all data can be represented as a matrix, understanding linear algebra is often cited as a prerequisite to reading and understanding formal mathematical descriptions of machine learning methods, as well as creating or editing existing methods. <br>\n\nComputational Linear Algebra is a fast.ai course covering linear algebra to be centered around practical applications and algorithms. <br>\n\nThere are four main areas for machine learning in which some linear algebra knowledge can help:\n- Speed (how fast matrix multiplication occurs)\n- Accuracy (how accurately can computers represent numbers)\n- Memory Usage (how to efficiently store matrices)\n- Scalability (how to use more data than you have the memory to store)\n\nThe reason why we are interested in these things is because often the bottleneck to a machine learning algorithm is within these four areas. In other words, knowledge in these areas can be the difference between a great ML approach and an unusable one. One example is in the case of how CNNs create their convolutional layers. While there are many mathematically equivalently orders in which to create these layers, some are evidently significantly faster. When these are applied in bulk, the optimisation makes all the difference. So in order to design or edit algorithms for usage in ML, knowledge in computational linear algebra is essential, particularly in research contexts as new approaches have not yet been optimised or implemented in existing frameworks.\n\n# Lesson Overview\nThis lesson covers the basics for our four main optimisation areas: Speed, Accuracy, Memory Usage and Scalability. \n\n# The topics covered, briefly\n- Accuracy: Number representation, Machine Epsilon, Conditioning and Stability. Approximation Accuracy.\n- Memory Use: Sparse vs Dense Matrices.\n- Speed: Computational Complexity, Vectorisation, Locality (Memory Usage), Scaling.\n\n# Lecture/Notebook Notes \nThere are two key types of matrix computation: Matrix and tensor products (combining matrices), and matrix decompositions (pulling them apart).\n\nConvolutions are a special kind of matrix product, but can also be represented as a neural network where the image pixels are the start nodes, the kernel elements are the weights, and the convolution pixels end nodes.\n\n### Accuracy\n\n**The representation of numbers:**\n\nOn paper, fractions are infinitely written. Computers however cannot store fractions 100% precisely because they are using discrete memory to store infinite precision. We ran iterations of a function that inputs and outputs fractions. Every iteration a very small error is added, harmless for the first few. But over time, these errors result in an entirely wrong answer.\n\nFor IEEE Double precision (an agreed standard): <br>\n\nThe continuous interval between [1,2] in a computer is represented as \n$1, 1+2^{-52},1+2x2^{-52}...,2$ <br>\nSo in this case, we see clearly it doesn't represent infinite precision.\nThe smallest increment, in this case $2^{-52}$, depends on the size of the interval. For a bigger interval, [2,4], it's $2^{-51}$, bigger by a magnitude. <br>\n\n**Machine Epsilon:**\n\nMachine Epsilon is defined as half the distance between 1 and the next larger number. <br> \n\nI believe this means in the case of our [1,2] interval, $\\varepsilon_{machine}=2^{-52}/2$. <br>\n\nBut the notes state:\n\"IEEE standards for double precision specify $\\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}$\", implying that Machine Epsilon is a constant value for a machine, rather than dependent on the interval or calculation involved?\n\nRegardless, we often describe error in terms of $\\varepsilon$. \nFor instance, say we represent a real number $x$ in a computer, so have a approximation $fl(x)$. The difference between $x$ and $fl(x)$ is always smaller than $x*\\varepsilon$.\n\nAs an equation:\n$fl(x)=x \\cdot (1 + \\varepsilon)$, <br>\nthe error is from the $x*\\varepsilon$ term.\n\nFor operations in a computer, +,-,x,/: <br>\n$ x \\circledast y = (x * y)(1 + \\varepsilon)$,<br>\nthe error is from all the terms containing $\\varepsilon$.\n\n**Conditioning and Stability:**\n\nBecause we can't represent numbers exactly, we need to know the errors that occur as a result. There are two defined terms to help with this:\n\nConditioning, about how accurately we can represent the problem. <br>\n**Conditioning**: perturbation behavior of a mathematical problem (e.g. least squares)\n\nStability, about how accurately we can compute the answer to said problem. <br>\n**Stability**: perturbation behavior of an algorithm used to solve that problem on a computer (e.g. least squares algorithms, householder, back substitution, gaussian elimination)\n\n**\"A stable algorithm gives nearly the right answer to nearly the right question.\"** --Trefethen\n\nAn an example for how small problems in accuracy can cause problems, consider how a small difference in matrix values results in very different eigenvalues.\n\nimport numpy as np\nimport scipy.linalg as la \n\nA = np.array([[1., 1000], [0, 1]])\nB = np.array([[1, 1000], [0.001, 1]])\n\nprint(A)\nprint(B)\n\nwA, vrA = la.eig(A)\nwB, vrB = la.eig(B)\n\nprint()\nprint(wA, wB)\n\nHaving 0.001 instead of 0 resulted in the first eigenvalue to be 2 instead of 1!\n\n### Approximation Accuracy\n\nAccepting some decreases in accuracy can speed up computations by orders of magnitude. So often using approximate algorithms is better. \n\nIn ML, some errors in training data representation are actually good because they force generalisability. <br>\n\nAnd sometimes we need not be super concerned about having 100% precise training data representation because the data collected isn't 100% precise in the first place.\n\nBloom filters can tell you a definite no, but not a definite yes, more like a probably yes. To remedy this, we can make a second more precise method to evaluate the items that are probably yes, while just ignoring the ones that are already known to be definitely no.\n\n### Memory Use\n\nSparse vs Dense matrices.\n\nSparse storage is just storing the non-zero elements of your matrix because you know the others are just 0. There are special ways of doing sparse storage.\n\nDense storage is the normal way we do it when we code, we just store everything explicitly.\n\n### Speed\nThe difference in speed between algorithms come from a number of areas by in particular:\n- Computational Complexity\n- Vectorisation\n- Scaling\n- Locality\n\n**Computational complexity** and big $\\mathcal{O}$ notation is about approximating the number of operations you need to do for a particular algorithm. More info: [on Interview Cake](https://www.interviewcake.com/article/java/big-o-notation-time-and-space-complexity) and [practice on Codecademy](https://www.codecademy.com/courses/big-o/0/3). \n\n**Vectorisation** is about applying an operation is multiple elements at once. Numpy replies on vectorized low level linear algebra APIs (BLAS and LAPACK) to do it's matrix operations. \n\n**Locality** is about how data in use is stored. Computers are usually slow because of the way we access data. Generally speaking, the faster the memory (so the faster we access the data), the less of it we have/the more expensive it is. Computers have many varying memory storage types, and each step down to slower memory you go, that memory is atleast an order of magnitude slower than the one before it. \n\nWe want to minimise the time we take to retrieve data in a computation. For example, by keeping items we are going to use multiple times in a computation in fast memory, and keepings items we use rarely in slow memory.\n\nA video to illustrate locality is then shared.\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"3uiEyEKji0M\")\n\nCode optimisation is really important. Even for a simple task, finding the average of 3 pixels, the code would normally be simple, but writing complex code would speed it up by 11x!\nIt's faster because it distributes work across threads (parallelism). Locality is making sure the pixels being used successively is in fast memory (cashe). W/O locality, paralleism can't be great.\n\nWe change the order in that CNN is done and as a result the way we store the pixel data, and get a much faster computational as a result.\nRemoving redundancy in computation also speeds up the computation.\nEach computation technique has potential trade offs. For example, having redundant computation to improve locality. \n\nTemporaries is data stored in a temporary variable in RAM. For example in Numpy, when we compute an equation, Numpy stores each equation variable as temporaries and then retrieves it. This is slow because there's no point storing each variable in the RAM and then immediately having to use it. Simply storing these variables in the cache would be so much faster.\n\n**Scaling:**\n\nWe we want to scale our computation across multiple cores in a computer. This is called parallesiation. Scalable algorithm are algorithms where the input can be broken into smaller pieces where each can be handled by a different core and the end result found by piecing together these pieces.\n\n# Links\n\nThe lecture for this sessions is https://www.youtube.com/watch?v=8iGzBMboA0I&list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&index=1. <br>\nThe lesson resources including the notebook(s): https://github.com/fastai/numerical-linear-algebra.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"number-sections":true,"output-file":"2022-10-17-CLA1.md.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"darkly","title-block-banner":true,"title":"fast.ai CLA Lesson 1","author":"Adnan Jinnah","date":"2022-10-17","jupyter":"python3","categories":["fastaiCLA"]},"extensions":{"book":{"multiFile":true}}}}}