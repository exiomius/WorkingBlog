<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adnan Jinnah">
<meta name="dcterms.date" content="2022-09-27">

<title>RvCode - fast.ai Lesson 10</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RvCode</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/exiomius/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">fast.ai Lesson 10</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">fastai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Adnan Jinnah </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 27, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro"><span class="toc-section-number">1</span>  Intro</a></li>
  <li><a href="#lesson-overview" id="toc-lesson-overview" class="nav-link" data-scroll-target="#lesson-overview"><span class="toc-section-number">2</span>  Lesson Overview</a></li>
  <li><a href="#lecture-notes" id="toc-lecture-notes" class="nav-link" data-scroll-target="#lecture-notes"><span class="toc-section-number">3</span>  Lecture Notes</a></li>
  <li><a href="#unet" id="toc-unet" class="nav-link" data-scroll-target="#unet"><span class="toc-section-number">4</span>  unet</a>
  <ul class="collapse">
  <li><a href="#for-example-for-handwritten-digits" id="toc-for-example-for-handwritten-digits" class="nav-link" data-scroll-target="#for-example-for-handwritten-digits"><span class="toc-section-number">4.1</span>  For example, for handwritten digits:</a></li>
  </ul></li>
  <li><a href="#clip" id="toc-clip" class="nav-link" data-scroll-target="#clip"><span class="toc-section-number">5</span>  CLIP</a>
  <ul class="collapse">
  <li><a href="#to-train-these-encoders" id="toc-to-train-these-encoders" class="nav-link" data-scroll-target="#to-train-these-encoders"><span class="toc-section-number">5.1</span>  To train these encoders:</a></li>
  </ul></li>
  <li><a href="#inference-image-creation" id="toc-inference-image-creation" class="nav-link" data-scroll-target="#inference-image-creation"><span class="toc-section-number">6</span>  Inference (Image Creation)</a></li>
  <li><a href="#vae" id="toc-vae" class="nav-link" data-scroll-target="#vae"><span class="toc-section-number">7</span>  VAE</a></li>
  <li><a href="#what-is-stable-diffusion" id="toc-what-is-stable-diffusion" class="nav-link" data-scroll-target="#what-is-stable-diffusion"><span class="toc-section-number">8</span>  What is Stable Diffusion?</a></li>
  <li><a href="#research-papers" id="toc-research-papers" class="nav-link" data-scroll-target="#research-papers"><span class="toc-section-number">9</span>  Research Papers</a>
  <ul class="collapse">
  <li><a href="#progressive-distillation-for-fast-sampling-of-diffusion-models" id="toc-progressive-distillation-for-fast-sampling-of-diffusion-models" class="nav-link" data-scroll-target="#progressive-distillation-for-fast-sampling-of-diffusion-models"><span class="toc-section-number">9.1</span>  Progressive Distillation for Fast Sampling of Diffusion Models:</a></li>
  <li><a href="#on-distillation-of-guided-diffusion-models-paper" id="toc-on-distillation-of-guided-diffusion-models-paper" class="nav-link" data-scroll-target="#on-distillation-of-guided-diffusion-models-paper"><span class="toc-section-number">9.2</span>  On Distillation of Guided Diffusion Models Paper</a>
  <ul class="collapse">
  <li><a href="#classifier-free-guided-diffusion-models-cfgd" id="toc-classifier-free-guided-diffusion-models-cfgd" class="nav-link" data-scroll-target="#classifier-free-guided-diffusion-models-cfgd"><span class="toc-section-number">9.2.1</span>  Classifier-free guided diffusion models (CFGD)</a></li>
  <li><a href="#the-paper" id="toc-the-paper" class="nav-link" data-scroll-target="#the-paper"><span class="toc-section-number">9.2.2</span>  The Paper</a></li>
  </ul></li>
  <li><a href="#imagic-text-based-real-image-editing-with-diffusion-models" id="toc-imagic-text-based-real-image-editing-with-diffusion-models" class="nav-link" data-scroll-target="#imagic-text-based-real-image-editing-with-diffusion-models"><span class="toc-section-number">9.3</span>  Imagic: Text-Based Real Image Editing with Diffusion Models</a></li>
  </ul></li>
  <li><a href="#notebook-stable-diffusion-with-diffusers-looking-inside-the-pipeline" id="toc-notebook-stable-diffusion-with-diffusers-looking-inside-the-pipeline" class="nav-link" data-scroll-target="#notebook-stable-diffusion-with-diffusers-looking-inside-the-pipeline"><span class="toc-section-number">10</span>  Notebook: Stable Diffusion with Diffusers: Looking inside the Pipeline</a></li>
  <li><a href="#notebook-matrix-multiplication-from-foundations" id="toc-notebook-matrix-multiplication-from-foundations" class="nav-link" data-scroll-target="#notebook-matrix-multiplication-from-foundations"><span class="toc-section-number">11</span>  Notebook: Matrix Multiplication from foundations</a>
  <ul class="collapse">
  <li><a href="#our-foundations-are" id="toc-our-foundations-are" class="nav-link" data-scroll-target="#our-foundations-are"><span class="toc-section-number">11.1</span>  Our foundations are:</a></li>
  <li><a href="#useful-shortcuts-and-tools" id="toc-useful-shortcuts-and-tools" class="nav-link" data-scroll-target="#useful-shortcuts-and-tools"><span class="toc-section-number">11.2</span>  Useful Shortcuts and Tools</a></li>
  <li><a href="#advice" id="toc-advice" class="nav-link" data-scroll-target="#advice"><span class="toc-section-number">11.3</span>  Advice</a></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes"><span class="toc-section-number">11.4</span>  Notes</a></li>
  </ul></li>
  <li><a href="#links" id="toc-links" class="nav-link" data-scroll-target="#links"><span class="toc-section-number">12</span>  Links</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Intro</h1>
<p>10: Stable Diffusion 2</p>
</section>
<section id="lesson-overview" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Lesson Overview</h1>
<p>This lesson recapped conceptually how stable diffusion works, what it is, how classifier free diffusion works, and how three research papers modify it uniquely. Then there is a notebook going through how stable diffusion is implemented in HuggingFace’s pipeline, as well as the first notebook of a series to recreate stable diffusion and fast.ai from scratch.</p>
<p>I found the lesson both conceptually difficult and time consuming but very interesting. My understanding of how stable diffusion works and how to implement it has improved greatly.</p>
</section>
<section id="lecture-notes" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Lecture Notes</h1>
<p>We start with a recap of how the core components of stable diffusion works. <br> There are three of them: The unet, CLIP, and VAE.</p>
</section>
<section id="unet" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> unet</h1>
<p>The unet knows how to find the noise (the parts of the image that don’t represent its label) in an image.</p>
<p>We train it to do so by adding noise using a scheduler to images, then giving the unet the noisy image and getting it to predict what parts are the noise.</p>
<section id="for-example-for-handwritten-digits" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="for-example-for-handwritten-digits"><span class="header-section-number">4.1</span> For example, for handwritten digits:</h2>
<ul>
<li>7 + Noise = Noisy 7.</li>
<li>Give Noisy 7 to unet, and unet predicts the Noise.</li>
<li>It then compares its prediction to the actual noise to get a loss to improve itself.</li>
<li>To make the unet work better, we pass in a label: an embedding of 7. For example, we could enter a one-hot encoded vector of it.</li>
<li>If we do this, then later on we can get the unet to create 7 of us because it understands what the embeddings of 7 are.</li>
</ul>
</section>
</section>
<section id="clip" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> CLIP</h1>
<p>But we don’t want our unet to create handwritten digits, we want to create images, and embeddings for them cannot be one-hot encoded.</p>
<p>So we use CLIP to create embeddings for prompts, so we can enter an image into our unet with an embedding of its prompt, similar to how we previous entered a handwritten digit image with an embedding of its number.</p>
<p>CLIP consists of two parts: An image encoder and an text encoder.</p>
<section id="to-train-these-encoders" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="to-train-these-encoders"><span class="header-section-number">5.1</span> To train these encoders:</h2>
<ul>
<li>Firstly we find images online that are captioned (has a prompt).</li>
<li>We input an image into the image encoder, and this produces embeddings (a feature vector) of the image.</li>
<li>We input the image’s prompt into the text encoder, and this produces embeddings (a feature vector) of the prompt.</li>
<li>We then train both to get these two sets of embeddings to be the same.</li>
<li>This is done with contrastive loss: prioritising these two sets of embeddings to be the same for when an image and prompt match, and penalising when they don’t.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-09-27-Lesson10Blog.md_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>Both encoders now can produce embeddings, but we only care about the text encoder. It can take prompts of images and create embeddings for them. With this, we can input into our unet both an image and (an embedding of) its prompt!</p>
</section>
</section>
<section id="inference-image-creation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Inference (Image Creation)</h1>
<p>Inference is the process of creating an image from a prompt. It’s also called denoising.</p>
<p>First we input into the unet a image of pure (Gaussian) noise and a prompt of the image we want.</p>
<p>The unet then predicts what parts of that image is noise. This actually means it thinks “which pixels from the image should I remove to get the image to look more like the prompt?”.</p>
<p>We could now just remove all the predicted noise in one go, but this would result in an awful image. Instead, we take the predicted noise and only remove some of it. This creates a less noisy image.</p>
<p>We then put our less noisy image back into our unet and repeat the process to remove more noise. Then we repeat this until we end up with our final image. Each time we remove noise is called a step. Previously it took about 60 steps to create an image, but a new paper described a method to do it in 3-4.</p>
<p>The diagram below shows how the image is gradually created as the steps occur:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-09-27-Lesson10Blog.md_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
</section>
<section id="vae" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> VAE</h1>
<p>Using a VAE and latent images is a computational shortcut to do stable diffusion much faster.</p>
<p>When we trained our unet, we implicitly used full sized images to do so, but this is really computationally expensive! A standard 512x512x3 image is 786432 px, which is too many to compute.</p>
<p>We can compress images though, while retaining the information needed to recreate them. To do so, we put the image through two convolutional layers and get a 64x64x24 image, with 16384 pixels, 48x less than the original! To reconstruct the image, we simply put it through two inverse convolutional layers.</p>
<p>We call the compression part an encoder, and the reconstruction part a decoder. Both combined are called an autoencoder, but in the case of stable diffusion we call an autoencoder a VAE. We call the compressed images, latents.</p>
<p>With this in mind, we create latents using the VAE’s encoder. <br> Now we train our unet on latents instead of full sized images, and do the inference process with latents too. <br> When our fully denoised latent is created, we simply use the decoder to reconstruct it to full size.</p>
</section>
<section id="what-is-stable-diffusion" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> What is Stable Diffusion?</h1>
<p>General diffusion models are machine learning systems that are trained to denoise (do inference) in steps on random Gaussian noise to create a sample of interest such as images.</p>
<p>The difference between general diffusion models and latent diffusion models is that the latter generates and uses latent (compressed) representations of images. This reduces the memory and computational needs significantly. The difference is an autoencoder (vae), which can reduce memory usage by 48 times for a 512x512 image as (3,512,512) becomes (4,64,64) in latent space. In effect, this speeds up both training and denoising significantly.</p>
<p>Stable Diffusion is based on latent diffusion. It was proposed in a paper High-Resolution Image Synthesis with Latent Diffusion Models at https://arxiv.org/abs/2112.10752.</p>
</section>
<section id="research-papers" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Research Papers</h1>
<section id="progressive-distillation-for-fast-sampling-of-diffusion-models" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="progressive-distillation-for-fast-sampling-of-diffusion-models"><span class="header-section-number">9.1</span> Progressive Distillation for Fast Sampling of Diffusion Models:</h2>
<p>Looking at the inference process, we can reduce the steps to denoise an image using distillation. Distillation is a pretty common technique in deep learning.</p>
<p>Look at step 36 and step 54 of inference below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-09-27-Lesson10Blog.md_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>Why is step 36 to 54 taking a whole 18 steps when there’s not much left to do comparatively to step 0 to 18 for instance?</p>
<p>The reason is, it takes very long because of a side effect of how the original maths of stable diffusion works.</p>
<p>But the thing is, after each step, we have an output image to play with! (we only plotted the output images every 6 steps).</p>
<p>What if we used a new model, unet B. unet B can take the output of step 36, and try to create the output of step 54. We can then compare the two to train unet B to learn how to get from step 36 to step 54 directly!</p>
<p>We have a teacher network (it already knows how to do something, but is slow and big), and a student network (it tries to do the same as the teacher but faster and with less memory).</p>
<p>Our teacher model is the original complete stable diffusion model. Our student model is the unet B skipping steps.</p>
<p>Generally speaking, we start with a noisy latent and the teacher model denoises it for 2 steps (it doesn’t create an image in 2 steps, just does 2 steps). Our student model then learns how to take the noisy latent and do the teacher’s 2 step denoising in 1 step. <br> Then we get the starting noisy latent and get the student model to denoise it for 2 steps. <br> And make another student learn how to do 2 steps of this in 1 step.</p>
<p>The original teacher model did some denoising in 2 steps. The first student could do that denoising in 1 step, so 2 steps of it would result in 4 of the original. The second student does the first’s in 1 step, so 2 steps of it results in 8 of the original. And so on, I suppose until you end up with 3-4 steps being enough to generate an image!</p>
</section>
<section id="on-distillation-of-guided-diffusion-models-paper" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="on-distillation-of-guided-diffusion-models-paper"><span class="header-section-number">9.2</span> On Distillation of Guided Diffusion Models Paper</h2>
<section id="classifier-free-guided-diffusion-models-cfgd" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="classifier-free-guided-diffusion-models-cfgd"><span class="header-section-number">9.2.1</span> Classifier-free guided diffusion models (CFGD)</h3>
<p>Classifier-free guided diffusion models (CFGD) is a technique to control how strongly our output image matches our prompt. <br></p>
<p>Say we want to create a photo of a cute puppy. We put “a cute puppy” into CLIP (its text_encoder) to get an embedding of the prompt. We then put this embedding into our unet with a pure noise latent to generate our puppy picture for us. This is normally how we do stable diffusion, but CFGD allows us to control how strongly this generated image matches our cute puppy prompt.</p>
<p>Here’s how it works: Put an empty “” prompt into CLIP too to get another embedding. This embedding is particular because it represents nothing. If we did inference just on this, we would essentially be telling our unet “generate an image without any guidance, no restrictions, as long as it looks good”.</p>
<p>Instead, we concatenate the ‘a cute puppy’ prompt embeddings with the blank “” prompt embeddings, and put them into our unet with a pure noisy latent. The unet outputs an image representing the real prompt, and another image representing the fake prompt. We combine these two images together. (<strong>I don’t understand why the unet produces 2 images, shouldn’t it just produce 1 image of them combined in the first place? That’s how the notebook code did it</strong>)</p>
<p>To recap, we just did the first step of inference, we took a noisy latent and removed a bit of noise to make it look more the embedding of the prompt “a cute puppy” concatenated with the embeddings of the prompt ““.</p>
<p>We then go onto doing inference as normal, removing noise step by step until we get our final image. The point of this is that, we can control how much the final image should rely on the real prompt versus the blank one. The latter is high guidance, the former is less. This allows us to control how strictly the real prompt is followed.</p>
</section>
<section id="the-paper" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="the-paper"><span class="header-section-number">9.2.2</span> The Paper</h3>
<p>This way of doing CFGD is awkward because the unet has to output two images instead of the usual one, and for other reasons. The paper details a way to skip it. We do teacher student distillation again!</p>
<p>Our normal stable diffusion with CFGD is the teacher model. It does CFGD with a number of different levels of guidance, 2, 4, 5, 12 etc. It does inference by starting with a noisy latent, then creating an image with guidance introduced. Our student model is another unet, unet B. Very similarly to how it worked in the previous paper, it looks at the different step outputs created by the teacher model, and learns how to do guidance like the teacher, but in a much more less awkward way.</p>
<p>There is an extra video on this, walking through the paper. It’s useful to watch this to learn how to read papers properly, as in, what’s important and such, in particular, the most important part is usually the algorithm.</p>
</section>
</section>
<section id="imagic-text-based-real-image-editing-with-diffusion-models" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="imagic-text-based-real-image-editing-with-diffusion-models"><span class="header-section-number">9.3</span> Imagic: Text-Based Real Image Editing with Diffusion Models</h2>
<p>This is another paper that just came out 3 hours ago as of writing!</p>
<p>Give it an input image, and pass in a prompt. It tries to edit the input image to match the prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-09-27-Lesson10Blog.md_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>This paper is for imagen, a different image generation algorithm, but it also works fine for stable diffusion.</p>
<p>Generally speaking, it works through fine-tuning and optimising embeddings.</p>
<p>We start with a fully trained stable diffusion model.</p>
<p>Our input image is</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-09-27-Lesson10Blog.md_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>We want an image of it spreading its wings.</p>
<p>Use CLIP to get an embedding of our prompt “a bird spreading its wings”. Use this embedding for inference as usual. This will create a photo of a bird spreading its wings, but not a photo of the bird from out input image spreading its wings.</p>
<p>To remedy this, we fine tune the prompt embedding to try and get it to create an image similar to the input bird image. We only do this a little and lock it, it cannot change any more. Now we have an optimised embedding.</p>
<p>Now we fine tune the entire stable diffusion model to generate images that look like our bird from the input image. Now we have a fine-tuned diffusion model.</p>
<p>Finally, we combine the optimised embedding and the original embedding for our target prompt, and pass it through the fine-tuned diffusion model to create our output image!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-09-27-Lesson10Blog.md_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>Maybe it would only take like an hour to do this process using stable diffusion on GPUs!</p>
</section>
</section>
<section id="notebook-stable-diffusion-with-diffusers-looking-inside-the-pipeline" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Notebook: Stable Diffusion with Diffusers: Looking inside the Pipeline</h1>
<p>Jeremy goes through the notebook: stable_diffusion.ipynb, available on a GitHub depository. I watched his walkthrough, made notes, then cloned the notebook and ran through it while commenting. This was a great learning experience, and made my understanding of the concepts much more solid.</p>
<p>There is a useful function, latents_callback, that shows a diagram of the images getting less noisy over time during inference.</p>
<p>For example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-09-27-Lesson10Blog.md_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>Because latents_callback is a parameter in pipeline, to use it I need to know how to create a pipeline out of it’s components, which I have not yet covered.</p>
</section>
<section id="notebook-matrix-multiplication-from-foundations" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Notebook: Matrix Multiplication from foundations</h1>
<p>Now what we’re going to do is rebuild all of these functions from scratch. We’re going to learn how to create a framework from some foundations and this will help with implementing new research papers, advanced debugging, and learning how to use Python!</p>
<section id="our-foundations-are" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="our-foundations-are"><span class="header-section-number">11.1</span> Our foundations are:</h2>
<ul>
<li>Python</li>
<li>Matplotlib</li>
<li>The Python Standard Libary</li>
<li>Jupyter Notebooks and nbdev</li>
</ul>
<p>To be clear, after we have created a function, we’re allowed to use a module to do it. For example, we don’t start with Numpy arrays, but after we’ve made our own we can use them.</p>
<p>This is also how machine learning models will work. We cannot train full models on our own, so we will create and train small models, and then allow ourselves to use pretrained models online.</p>
<p>This is a challenge that will be different, but it’s often the part of the course where people learn the most. This year around it will be the best to date, and is more than worth doing.</p>
<p>The notebooks are from the part 2 repo on GitHub.</p>
</section>
<section id="useful-shortcuts-and-tools" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="useful-shortcuts-and-tools"><span class="header-section-number">11.2</span> Useful Shortcuts and Tools</h2>
<ul>
<li>Shift-m in Jupyter allows you to combine cells together.</li>
<li>Cntrl-shirt-minus allows you to separate them.</li>
<li>Alt-enter inserts cells.</li>
<li>Run a function then ? to get a brief description.</li>
<li>Run a function then ?? to get documentation.</li>
<li>Inside a function, press shift-tab to see parameters available quickly.</li>
</ul>
</section>
<section id="advice" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="advice"><span class="header-section-number">11.3</span> Advice</h2>
<ul>
<li>Read the Python documentation a lot, it’s good.</li>
<li>For every single method, Jeremy reads the documentation and practices it.</li>
</ul>
</section>
<section id="notes" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="notes"><span class="header-section-number">11.4</span> Notes</h2>
<p>I made notes on the lecture, then went through the notebook myself and rewrote the notes while running through and testing the cells.</p>
</section>
</section>
<section id="links" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Links</h1>
<ul>
<li>As I am doing this lesson as it is released privately live, I cannot share links to the resources.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>