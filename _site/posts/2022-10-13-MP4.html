<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adnan Jinnah">
<meta name="dcterms.date" content="2023-06-10">

<title>RvCode - Masters Project 18/10/2022</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RvCode</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/exiomius/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Masters Project 18/10/2022</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Masters</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Adnan Jinnah </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 10, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro"><span class="toc-section-number">1</span>  Intro</a></li>
  <li><a href="#meeting" id="toc-meeting" class="nav-link" data-scroll-target="#meeting"><span class="toc-section-number">2</span>  Meeting:</a>
  <ul class="collapse">
  <li><a href="#during-the-meeting-we-discussed" id="toc-during-the-meeting-we-discussed" class="nav-link" data-scroll-target="#during-the-meeting-we-discussed"><span class="toc-section-number">2.1</span>  During the meeting we discussed:</a></li>
  </ul></li>
  <li><a href="#problem-definition-update" id="toc-problem-definition-update" class="nav-link" data-scroll-target="#problem-definition-update"><span class="toc-section-number">3</span>  Problem definition update</a></li>
  <li><a href="#work-to-do" id="toc-work-to-do" class="nav-link" data-scroll-target="#work-to-do"><span class="toc-section-number">4</span>  Work to do:</a>
  <ul class="collapse">
  <li><a href="#machine-learning-skills" id="toc-machine-learning-skills" class="nav-link" data-scroll-target="#machine-learning-skills"><span class="toc-section-number">4.1</span>  Machine Learning Skills</a></li>
  <li><a href="#machine-learning-theory" id="toc-machine-learning-theory" class="nav-link" data-scroll-target="#machine-learning-theory"><span class="toc-section-number">4.2</span>  Machine Learning Theory</a></li>
  <li><a href="#framework-skills" id="toc-framework-skills" class="nav-link" data-scroll-target="#framework-skills"><span class="toc-section-number">4.3</span>  Framework Skills</a></li>
  <li><a href="#data-handlingpreprocessingphysics" id="toc-data-handlingpreprocessingphysics" class="nav-link" data-scroll-target="#data-handlingpreprocessingphysics"><span class="toc-section-number">4.4</span>  Data handling/preprocessing/Physics</a></li>
  <li><a href="#custom-metrics-creation-and-evaluation-for-models" id="toc-custom-metrics-creation-and-evaluation-for-models" class="nav-link" data-scroll-target="#custom-metrics-creation-and-evaluation-for-models"><span class="toc-section-number">4.5</span>  Custom metrics, creation, and evaluation for models</a></li>
  <li><a href="#machine-learning-explainability-and-communication" id="toc-machine-learning-explainability-and-communication" class="nav-link" data-scroll-target="#machine-learning-explainability-and-communication"><span class="toc-section-number">4.6</span>  Machine learning explainability and communication</a></li>
  <li><a href="#machine-learning-maths." id="toc-machine-learning-maths." class="nav-link" data-scroll-target="#machine-learning-maths."><span class="toc-section-number">4.7</span>  Machine learning maths.</a></li>
  </ul></li>
  <li><a href="#work-done" id="toc-work-done" class="nav-link" data-scroll-target="#work-done"><span class="toc-section-number">5</span>  Work done</a>
  <ul class="collapse">
  <li><a href="#practiced-transformers" id="toc-practiced-transformers" class="nav-link" data-scroll-target="#practiced-transformers"><span class="toc-section-number">5.1</span>  Practiced Transformers</a>
  <ul class="collapse">
  <li><a href="#in-particular-for-audio-classification-it-details-the-process" id="toc-in-particular-for-audio-classification-it-details-the-process" class="nav-link" data-scroll-target="#in-particular-for-audio-classification-it-details-the-process"><span class="toc-section-number">5.1.1</span>  In particular for audio classification it details the process: <br></a></li>
  </ul></li>
  <li><a href="#practiced-trying-to-attempt-birdclef-2022" id="toc-practiced-trying-to-attempt-birdclef-2022" class="nav-link" data-scroll-target="#practiced-trying-to-attempt-birdclef-2022"><span class="toc-section-number">5.2</span>  Practiced Trying to attempt BirdCLEF 2022</a>
  <ul class="collapse">
  <li><a href="#to-summarise" id="toc-to-summarise" class="nav-link" data-scroll-target="#to-summarise"><span class="toc-section-number">5.2.1</span>  To summarise:</a></li>
  <li><a href="#on-the-bright-side-atleast-i-learnt-a-few-things-from-the-struggle" id="toc-on-the-bright-side-atleast-i-learnt-a-few-things-from-the-struggle" class="nav-link" data-scroll-target="#on-the-bright-side-atleast-i-learnt-a-few-things-from-the-struggle"><span class="toc-section-number">5.2.2</span>  On the bright side, atleast I learn’t a few things from the struggle:</a></li>
  </ul></li>
  <li><a href="#finished-fast.ai-lesson-9" id="toc-finished-fast.ai-lesson-9" class="nav-link" data-scroll-target="#finished-fast.ai-lesson-9"><span class="toc-section-number">5.3</span>  Finished fast.ai lesson 9:</a></li>
  <li><a href="#finished-cla-lesson-1" id="toc-finished-cla-lesson-1" class="nav-link" data-scroll-target="#finished-cla-lesson-1"><span class="toc-section-number">5.4</span>  Finished CLA lesson 1:</a></li>
  <li><a href="#useful-datasets-found" id="toc-useful-datasets-found" class="nav-link" data-scroll-target="#useful-datasets-found"><span class="toc-section-number">5.5</span>  Useful Datasets Found</a></li>
  <li><a href="#useful-research-tools" id="toc-useful-research-tools" class="nav-link" data-scroll-target="#useful-research-tools"><span class="toc-section-number">5.6</span>  Useful Research Tools</a></li>
  <li><a href="#a-similar-thesis" id="toc-a-similar-thesis" class="nav-link" data-scroll-target="#a-similar-thesis"><span class="toc-section-number">5.7</span>  A Similar Thesis</a></li>
  <li><a href="#new-ideas" id="toc-new-ideas" class="nav-link" data-scroll-target="#new-ideas"><span class="toc-section-number">5.8</span>  New Ideas:</a>
  <ul class="collapse">
  <li><a href="#to-create-a-soundscape" id="toc-to-create-a-soundscape" class="nav-link" data-scroll-target="#to-create-a-soundscape"><span class="toc-section-number">5.8.1</span>  To create a soundscape:</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Intro</h1>
<p>The fourth post from a series of posts about my Masters project with the Physics Department at Durham University.</p>
</section>
<section id="meeting" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Meeting:</h1>
<section id="during-the-meeting-we-discussed" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="during-the-meeting-we-discussed"><span class="header-section-number">2.1</span> During the meeting we discussed:</h2>
<ul>
<li>Trying to compete directly with Google or other big research teams isn’t a good direction for a one year project. What would be better is a novel direction of something else.</li>
<li>One such approach would be trying to use stero data instead of mono data to solve the cocktail problem.</li>
<li>Most audio data online is mono, for example on xenocanto, but we could try and get in contact with research groups like the biology team and ask for stero data.</li>
<li>One of the reasons why stero data is rarer, is because it requires a special microphone to collect it with.</li>
<li>Different stero microphones have different properties, such as the width between the the two microphones, and this needs to be accounted for when gathering and analysing data.</li>
<li>We could look at stero data, and then calculate the difference between the first channel and second channel to make a third channel. Then try classifying with one channel, and with all three, and seeing if it helps.</li>
<li>What would be interesting only possible with stero data is trying to find the direction of the birds singing. But this might be impossible, because getting labeled data of that is difficult.</li>
<li>The differences between mono and stero data. Besides having 2 channels, there are differences in time delay and attenuation. I need to look into this more.</li>
<li>Investigating whether it’s possible to convert between mono and stero data. It might be impossible to do exactly because there is information missing within the mono data, particularly because intuitively, stero data you can find direction from but mono you cannot. This is an information problem.</li>
<li>Stuart might order a stero microphone to play around with; Robert is asking whether it is possible to borrow one.</li>
<li>Another novel approach would be trying to use stable diffusion to generate spectrograms. The idea being, if there is a lack of stero data we could synthesise our own. There could even be another model added to correct synthesised audio data to be more like real audio data.</li>
<li>A motivation behind this could be the prevalence of an image classification approach in classifying birdsong. Much research uses CNNs for example.</li>
<li>About the Physics content of the project. There needs to be some Physics for the sake of the external marker and external questions at the viva.</li>
<li>Physics content can be added by investigating how to do the image to audio conversion (or vise versa in the case of stable diffusion), because of the transformations and information problem involved, or the prevalence of linear algebra/maths being involved, and even just in the Physics way of thinking of testing hypothesis and different approaches.</li>
</ul>
</section>
</section>
<section id="problem-definition-update" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Problem definition update</h1>
<p>From now on, I will refer to the problem of trying to identify birds in a noisy environment as ‘the cocktail problem’, as in at a cocktail party where many people are speaking and where the environment is noisy, it is hard to tell who is speaking. <br></p>
<p>A soundscape is going to be defined as an audio file that contains various birdsong in a noisy environment. We are trying to classify birds from a soundscape.</p>
</section>
<section id="work-to-do" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Work to do:</h1>
<p>I’ve been thinking about all the things I need to do and learn for this project. Here is an overview of them. It’s honestly a lot of work, and hard to tell how much time each will take until I make more progress. It might take the whole of first term to get a handle on this.</p>
<section id="machine-learning-skills" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="machine-learning-skills"><span class="header-section-number">4.1</span> Machine Learning Skills</h2>
<p>Using frameworks like fast.ai and transformers isn’t as simple as just using their predefined functions and models to do everything. Learning how to find the best hyperparameters, and good validation sets, among many other things, takes a combination of theory and practice to gain intuition. Jeremy from fast.ai said there is no substitute for practice, and provides a lot of guidance on how to do so.</p>
</section>
<section id="machine-learning-theory" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="machine-learning-theory"><span class="header-section-number">4.2</span> Machine Learning Theory</h2>
<p>As well as using frameworks and models, you have to spend time learning the theory behind how they work. For instance, how the components in a CNN work the way they do.</p>
</section>
<section id="framework-skills" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="framework-skills"><span class="header-section-number">4.3</span> Framework Skills</h2>
<p>Understanding the theory, and then having novel ideas to approach the cocktail problem, I need to then implement these ideas by knowing how to create the new code to do so. <br> This involves learning how to edit frameworks and create your own, covered in fast.ai part 2. <br></p>
<p>There’s also learning about https://nbdev.fast.ai/ to create frameworks and their documentation.</p>
</section>
<section id="data-handlingpreprocessingphysics" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="data-handlingpreprocessingphysics"><span class="header-section-number">4.4</span> Data handling/preprocessing/Physics</h2>
<p>Learning how to store data, access it, transform it into the right size and format, edit it, add noise to it, interpret it (bird domain information) etc. <br> There could be much work to be done on transforming the audio data. Fourier and Gabor transformers etc. I found a YouTube playlist of guides on this at https://www.youtube.com/watch?v=RMfeYitdO-c.&nbsp;The fourth initial project reference, “New aspects in birdsong recognition utilizing the gabor transform”, focuses on the gabor transform and likely much Physics too.</p>
</section>
<section id="custom-metrics-creation-and-evaluation-for-models" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="custom-metrics-creation-and-evaluation-for-models"><span class="header-section-number">4.5</span> Custom metrics, creation, and evaluation for models</h2>
<p>The biology department have their own interests and goals of what they want from a model. I would need to talk in detail with them about their priorities, e.g.&nbsp;preferences in confusion matrix metrics, in bird species etc. They might want a model to work with data over a few years to spot trends too.</p>
</section>
<section id="machine-learning-explainability-and-communication" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="machine-learning-explainability-and-communication"><span class="header-section-number">4.6</span> Machine learning explainability and communication</h2>
<p>Learning how to implement and create methods and visualisations to communicate why the models are predicting as they do. This is especially important for marking in the final report.</p>
</section>
<section id="machine-learning-maths." class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="machine-learning-maths."><span class="header-section-number">4.7</span> Machine learning maths.</h2>
<p>To read and implement the latest machine learning papers, some mathematical knowledge is needed. I am contemplating doing yet another free fast.ai course, Computational Linear Algebra, explained here https://www.fast.ai/posts/2017-07-17-num-lin-alg.html, to help with the maths side of things. <br></p>
<p>Alternatively or in addition, the book Deep Learning by Ian Goodfellow provides a mathematical backing and Jeremy recommended reading the first 6 chapters of it to help with understanding and implementing maths in papers.</p>
</section>
</section>
<section id="work-done" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Work done</h1>
<section id="practiced-transformers" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="practiced-transformers"><span class="header-section-number">5.1</span> Practiced Transformers</h2>
<p>A list of transformer tasks is at https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb which is quite useful. <br></p>
<section id="in-particular-for-audio-classification-it-details-the-process" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="in-particular-for-audio-classification-it-details-the-process"><span class="header-section-number">5.1.1</span> In particular for audio classification it details the process: <br></h3>
<ol type="1">
<li>Instantiate a feature extractor and a model from the checkpoint name.</li>
<li>Process the audio signal to be classified with a feature extractor.</li>
<li>Pass the input through the model and take the argmax to retrieve the most likely class.</li>
<li>Convert the class id to a class name with id2label to return an interpretable result.</li>
</ol>
<p>I went through the HuggingFace transformers documentation and did some of the notebooks to understand them. - https://www.kaggle.com/adnanjinnah/audio-classification-hf-1/ - https://www.kaggle.com/adnanjinnah/audio-classification-hf-2/ - https://www.kaggle.com/adnanjinnah/audio-classification-hf-3/ and they covered the 4 step process detailed above.</p>
</section>
</section>
<section id="practiced-trying-to-attempt-birdclef-2022" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="practiced-trying-to-attempt-birdclef-2022"><span class="header-section-number">5.2</span> Practiced Trying to attempt BirdCLEF 2022</h2>
<p>It’s well worth practicing attempts for a competition with the goal exactly as my own. After trying fast.ai’s audio module last week, and thinking it is outdated (the GitHub repo hasn’t been updated in roughly 6 months), I decided to use HuggingFace instead. This is mainly due to Jeremy recommending it as an up to date framework, but also because it is used in fast.ai part 2.</p>
<p>With that in mind, I attempted it at https://www.kaggle.com/adnanjinnah/birdclef-first-attempt/. This attempt was writhe with problems. While it was my first time using HuggingFace audio, the number of problems I encountered and issues involved were too much. I did not manage to get any model to work. I spent the entire time just trying to get the data loaded properly for usage.</p>
<section id="to-summarise" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="to-summarise"><span class="header-section-number">5.2.1</span> To summarise:</h3>
<ul>
<li>HuggingFace’s load_model has several different methods to load audio. They all require the data to be formatted in a particular way. I tried all them with no success.</li>
<li>Kaggle’s competition dataset is set to read only for some reason. This makes it so I cannot directly just edit the files to get them right.</li>
<li>I tired simply downloading the dataset and reuploading it to Kaggle but A. this is inefficient and B. won’t work for the unseen test data.</li>
<li>I tried copying over the dataset from the read-only input folder to the editable output folder, but this is also inefficient and even so:</li>
<li>I couldn’t load the copied data using load_dataset’s audiofolder function. I’m not sure why. I have it formatted in the exact way the documentation shows. The issue may be I need to upload the dataset to HuggingFace’s website first, but this has the same issues as the first attempt.</li>
<li>A way to get around having to copy the data, with is also inefficient but would atleast work with the unseen test data is to tell load_dataset the URLs of the audio files. This didn’t work either, because some of the URLs don’t work in the instant load_dataset wants to access them. I couldn’t find a way to tell load_dataset to ignore or look later at these URLs.</li>
<li>I tried using a different method of load_dataset, this one however seems to require the main .csv file to contain the audio files in array format. Because the .csv file contains a path to the audio files instead of their content, I tried using another module, librosa, to create a column in the .csv file containing the audio. This didn’t work, because of an excess memory error. And also, this is very inefficient.</li>
</ul>
<p>After extensively trying all methods I could find in the documentation with little success, this entire process took around 10 hours. I found tutorials to help with no luck. For now, I’ve given up on trying to get it to work myself. I need to find some resource online or in person to help. In hindsight, I probably should have done this earlier.</p>
</section>
<section id="on-the-bright-side-atleast-i-learnt-a-few-things-from-the-struggle" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="on-the-bright-side-atleast-i-learnt-a-few-things-from-the-struggle"><span class="header-section-number">5.2.2</span> On the bright side, atleast I learn’t a few things from the struggle:</h3>
<ul>
<li>First, how transformers requires a dataset to be formatted in a specific way, and that HuggingFace has a website dedicated to storing datasets in an already formatted way.</li>
<li>Experience in reading through documentation and troubleshooting.</li>
<li>The fact that sometimes URLs don’t work, and that last week’s code had a solution, but I couldn’t implement it into HuggingFace’s load_model.</li>
<li>That different loading methods require paths to audio files or them on the .csv file.</li>
<li>That audio files are stored as a file such as .ogg or as an array.</li>
<li>How librosa is a module to convert audio files into audio files into said arrays.</li>
<li>That memory errors will occur from trying to do too much at once. I could get my last method to work if I figured out a way to split up the data, but regardless this approach is inefficient considering we already have the files so it’s better to find a different method.</li>
<li>How to use os to copy files and folders over, or search and retrieve their file paths.</li>
<li>The fact that, for some datasets like BirdCLEF, there is a metadata.csv file with a column for the paths of the audio files.</li>
<li>That for advanced dataset formatting, for HuggingFace, you can create a .py script to do things exactly as you want.</li>
</ul>
</section>
</section>
<section id="finished-fast.ai-lesson-9" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="finished-fast.ai-lesson-9"><span class="header-section-number">5.3</span> Finished fast.ai lesson 9:</h2>
<p>This lesson was the first of fast.ai part 2 and a very well taught one. In it, Jeremy described conceptually how stable diffusion, an crazy new image generation model, works. Due to it’s difficulty, the lesson took me a full day to complete, but it was well worth it. The ideas and skills I’m being introduced to and learning will prove really helpful for the project going forwards. Next week, the lesson will focus on programming stable diffusion from scratch, and building on that, how to programme your own custom Python machine learning libraries. This is vital because it would allow me not just to copy other people’s code to solve the cocktail problem, but implement my own ideas and test things, perhaps even at a research level.</p>
<p>My post for lesson 9 can be found at: https://exiomius.quarto.pub/blog/posts/2022-10-11-L9Blog.md.html</p>
</section>
<section id="finished-cla-lesson-1" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="finished-cla-lesson-1"><span class="header-section-number">5.4</span> Finished CLA lesson 1:</h2>
<p>Computational Linear Algebra is a fast.ai course covering linear algebra to be centered around practical applications and algorithms. <br> More info and lesson 1 blog can be found here: https://exiomius.quarto.pub/blog/posts/2022-10-17-CLA1.md.html</p>
</section>
<section id="useful-datasets-found" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="useful-datasets-found"><span class="header-section-number">5.5</span> Useful Datasets Found</h2>
<ul>
<li>BirdCLEF 2022 uses data from xeno-carto, implying that last week’s approach to downloading them is a good idea.</li>
<li>I found ESC-50, a dataset of labeled environmental audio recordings at https://dagshub.com/kinkusuma/esc50-dataset, also at https://huggingface.co/datasets/ashraq/esc50. These include sounds like rain, sea waves, animals.</li>
<li>I found that Machine Listening Lab at Queen Mary’s University run a birdsong competition and have many datasets that I could possibly use at http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/.</li>
</ul>
</section>
<section id="useful-research-tools" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="useful-research-tools"><span class="header-section-number">5.6</span> Useful Research Tools</h2>
<ul>
<li>Scholarcy summarises research articles.</li>
<li>https://inciteful.xyz/ is good for finding papers.</li>
<li>I was told that Prostudy is useful for keeping resources stored for a dissertation.</li>
</ul>
</section>
<section id="a-similar-thesis" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="a-similar-thesis"><span class="header-section-number">5.7</span> A Similar Thesis</h2>
<p>My friend’s friend wrote a thesis similar in aim to mine last year. <br></p>
<p>Title: Using mel-frequency cepstral coefficients and principal components analysis to classify bird vocalisations based on citizen science recordings. <br> Student Name: Alex Dyfrig Swainston. <br></p>
<p>I messaged Alex and got a copy, and he said he’s happy to help if I have any questions.</p>
</section>
<section id="new-ideas" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="new-ideas"><span class="header-section-number">5.8</span> New Ideas:</h2>
<p>Here are a few new ideas I had about tackling the cocktail problem.</p>
<p>A big issue is the lack of properly labeled data for soundscapes. The biology department painstakingly handlabeled some soundscapes, but it is a difficult and time consuming task that even great ecologists struggle with. What if there was a way to create our own soundscapes that are already labeled? For instance, we have plenty of data from xeno-canto of individual bird songs with varying amounts of noise. What if I also found some audio files of forest environments, and I created a model to combine xeno-canto bird songs with these to imitate a real soundscape? This way, I could create an endless amount of soundscapes to train on, and the birds within them would be labeled!</p>
<section id="to-create-a-soundscape" class="level3" data-number="5.8.1">
<h3 data-number="5.8.1" class="anchored" data-anchor-id="to-create-a-soundscape"><span class="header-section-number">5.8.1</span> To create a soundscape:</h3>
<ul>
<li>I could download bird song(s),</li>
<li>Cut out various parts of them, e.g.&nbsp;if it’s 3 minutes long, I cut out random intervals of 20-30 seconds to imitate the bird moving or other sounds overpowering their song,</li>
<li>Randomly vary how loud the bird songs are,</li>
<li>Add in enviromental sounds like a forest soundscape (but being careful there are no birds present!),</li>
<li>Use a noise function, (which is used in stable diffusion), to randomly add noise. Alternatively, find a way to make a model that can generate real noise that is recorded by microphones and use that.</li>
</ul>
<p>I could put multiple birdsongs in the same artifical soundscape, and even make them overlap, but I also need to be careful that perhaps I should make the birds singing be realistically in the same environment. I mean I shouldn’t put two birds together that geographically would never meet, or two birds that never sing at the same time of day, or in general enviromental sounds that don’t match the birds present.</p>
<p>Another idea is to add geographical data somehow to the dataset. Perhaps with another input for a satellite image to help.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>