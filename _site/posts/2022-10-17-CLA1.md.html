<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adnan Jinnah">
<meta name="dcterms.date" content="2022-10-17">

<title>RvCode - fast.ai CLA Lesson 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RvCode</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/exiomius/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">fast.ai CLA Lesson 1</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">fastaiCLA</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Adnan Jinnah </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 17, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro"><span class="toc-section-number">1</span>  Intro</a></li>
  <li><a href="#course-introduction" id="toc-course-introduction" class="nav-link" data-scroll-target="#course-introduction"><span class="toc-section-number">2</span>  Course Introduction</a></li>
  <li><a href="#lesson-overview" id="toc-lesson-overview" class="nav-link" data-scroll-target="#lesson-overview"><span class="toc-section-number">3</span>  Lesson Overview</a></li>
  <li><a href="#the-topics-covered-briefly" id="toc-the-topics-covered-briefly" class="nav-link" data-scroll-target="#the-topics-covered-briefly"><span class="toc-section-number">4</span>  The topics covered, briefly</a></li>
  <li><a href="#lecturenotebook-notes" id="toc-lecturenotebook-notes" class="nav-link" data-scroll-target="#lecturenotebook-notes"><span class="toc-section-number">5</span>  Lecture/Notebook Notes</a>
  <ul class="collapse">
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy"><span class="toc-section-number">5.0.1</span>  Accuracy</a></li>
  <li><a href="#approximation-accuracy" id="toc-approximation-accuracy" class="nav-link" data-scroll-target="#approximation-accuracy"><span class="toc-section-number">5.0.2</span>  Approximation Accuracy</a></li>
  <li><a href="#memory-use" id="toc-memory-use" class="nav-link" data-scroll-target="#memory-use"><span class="toc-section-number">5.0.3</span>  Memory Use</a></li>
  <li><a href="#speed" id="toc-speed" class="nav-link" data-scroll-target="#speed"><span class="toc-section-number">5.0.4</span>  Speed</a></li>
  </ul></li>
  <li><a href="#links" id="toc-links" class="nav-link" data-scroll-target="#links"><span class="toc-section-number">6</span>  Links</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Intro</h1>
<p>The first lesson of fast.ai’s Computational Linear Algebra course.</p>
</section>
<section id="course-introduction" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Course Introduction</h1>
<p>An introductory blog post for this course can be found here https://www.fast.ai/posts/2017-07-17-num-lin-alg.html. <br></p>
<p>Because machine learning is largely about manipulating data, and almost all data can be represented as a matrix, understanding linear algebra is often cited as a prerequisite to reading and understanding formal mathematical descriptions of machine learning methods, as well as creating or editing existing methods. <br></p>
<p>Computational Linear Algebra is a fast.ai course covering linear algebra to be centered around practical applications and algorithms. <br></p>
<p>There are four main areas for machine learning in which some linear algebra knowledge can help: - Speed (how fast matrix multiplication occurs) - Accuracy (how accurately can computers represent numbers) - Memory Usage (how to efficiently store matrices) - Scalability (how to use more data than you have the memory to store)</p>
<p>The reason why we are interested in these things is because often the bottleneck to a machine learning algorithm is within these four areas. In other words, knowledge in these areas can be the difference between a great ML approach and an unusable one. One example is in the case of how CNNs create their convolutional layers. While there are many mathematically equivalently orders in which to create these layers, some are evidently significantly faster. When these are applied in bulk, the optimisation makes all the difference. So in order to design or edit algorithms for usage in ML, knowledge in computational linear algebra is essential, particularly in research contexts as new approaches have not yet been optimised or implemented in existing frameworks.</p>
</section>
<section id="lesson-overview" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Lesson Overview</h1>
<p>This lesson covers the basics for our four main optimisation areas: Speed, Accuracy, Memory Usage and Scalability.</p>
</section>
<section id="the-topics-covered-briefly" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The topics covered, briefly</h1>
<ul>
<li>Accuracy: Number representation, Machine Epsilon, Conditioning and Stability. Approximation Accuracy.</li>
<li>Memory Use: Sparse vs Dense Matrices.</li>
<li>Speed: Computational Complexity, Vectorisation, Locality (Memory Usage), Scaling.</li>
</ul>
</section>
<section id="lecturenotebook-notes" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Lecture/Notebook Notes</h1>
<p>There are two key types of matrix computation: Matrix and tensor products (combining matrices), and matrix decompositions (pulling them apart).</p>
<p>Convolutions are a special kind of matrix product, but can also be represented as a neural network where the image pixels are the start nodes, the kernel elements are the weights, and the convolution pixels end nodes.</p>
<section id="accuracy" class="level3" data-number="5.0.1">
<h3 data-number="5.0.1" class="anchored" data-anchor-id="accuracy"><span class="header-section-number">5.0.1</span> Accuracy</h3>
<p><strong>The representation of numbers:</strong></p>
<p>On paper, fractions are infinitely written. Computers however cannot store fractions 100% precisely because they are using discrete memory to store infinite precision. We ran iterations of a function that inputs and outputs fractions. Every iteration a very small error is added, harmless for the first few. But over time, these errors result in an entirely wrong answer.</p>
<p>For IEEE Double precision (an agreed standard): <br></p>
<p>The continuous interval between [1,2] in a computer is represented as <span class="math inline">\(1, 1+2^{-52},1+2x2^{-52}...,2\)</span> <br> So in this case, we see clearly it doesn’t represent infinite precision. The smallest increment, in this case <span class="math inline">\(2^{-52}\)</span>, depends on the size of the interval. For a bigger interval, [2,4], it’s <span class="math inline">\(2^{-51}\)</span>, bigger by a magnitude. <br></p>
<p><strong>Machine Epsilon:</strong></p>
<p>Machine Epsilon is defined as half the distance between 1 and the next larger number. <br></p>
<p>I believe this means in the case of our [1,2] interval, <span class="math inline">\(\varepsilon_{machine}=2^{-52}/2\)</span>. <br></p>
<p>But the notes state: “IEEE standards for double precision specify <span class="math inline">\(\varepsilon_{machine} = 2^{-53} \approx 1.11 \times 10^{-16}\)</span>”, implying that Machine Epsilon is a constant value for a machine, rather than dependent on the interval or calculation involved?</p>
<p>Regardless, we often describe error in terms of <span class="math inline">\(\varepsilon\)</span>. For instance, say we represent a real number <span class="math inline">\(x\)</span> in a computer, so have a approximation <span class="math inline">\(fl(x)\)</span>. The difference between <span class="math inline">\(x\)</span> and <span class="math inline">\(fl(x)\)</span> is always smaller than <span class="math inline">\(x*\varepsilon\)</span>.</p>
<p>As an equation: <span class="math inline">\(fl(x)=x \cdot (1 + \varepsilon)\)</span>, <br> the error is from the <span class="math inline">\(x*\varepsilon\)</span> term.</p>
<p>For operations in a computer, +,-,x,/: <br> $ x y = (x * y)(1 + )$,<br> the error is from all the terms containing <span class="math inline">\(\varepsilon\)</span>.</p>
<p><strong>Conditioning and Stability:</strong></p>
<p>Because we can’t represent numbers exactly, we need to know the errors that occur as a result. There are two defined terms to help with this:</p>
<p>Conditioning, about how accurately we can represent the problem. <br> <strong>Conditioning</strong>: perturbation behavior of a mathematical problem (e.g.&nbsp;least squares)</p>
<p>Stability, about how accurately we can compute the answer to said problem. <br> <strong>Stability</strong>: perturbation behavior of an algorithm used to solve that problem on a computer (e.g.&nbsp;least squares algorithms, householder, back substitution, gaussian elimination)</p>
<p><strong>“A stable algorithm gives nearly the right answer to nearly the right question.”</strong> –Trefethen</p>
<p>An an example for how small problems in accuracy can cause problems, consider how a small difference in matrix values results in very different eigenvalues.</p>
<p>import numpy as np import scipy.linalg as la</p>
<p>A = np.array([[1., 1000], [0, 1]]) B = np.array([[1, 1000], [0.001, 1]])</p>
<p>print(A) print(B)</p>
<p>wA, vrA = la.eig(A) wB, vrB = la.eig(B)</p>
<p>print() print(wA, wB)</p>
<p>Having 0.001 instead of 0 resulted in the first eigenvalue to be 2 instead of 1!</p>
</section>
<section id="approximation-accuracy" class="level3" data-number="5.0.2">
<h3 data-number="5.0.2" class="anchored" data-anchor-id="approximation-accuracy"><span class="header-section-number">5.0.2</span> Approximation Accuracy</h3>
<p>Accepting some decreases in accuracy can speed up computations by orders of magnitude. So often using approximate algorithms is better.</p>
<p>In ML, some errors in training data representation are actually good because they force generalisability. <br></p>
<p>And sometimes we need not be super concerned about having 100% precise training data representation because the data collected isn’t 100% precise in the first place.</p>
<p>Bloom filters can tell you a definite no, but not a definite yes, more like a probably yes. To remedy this, we can make a second more precise method to evaluate the items that are probably yes, while just ignoring the ones that are already known to be definitely no.</p>
</section>
<section id="memory-use" class="level3" data-number="5.0.3">
<h3 data-number="5.0.3" class="anchored" data-anchor-id="memory-use"><span class="header-section-number">5.0.3</span> Memory Use</h3>
<p>Sparse vs Dense matrices.</p>
<p>Sparse storage is just storing the non-zero elements of your matrix because you know the others are just 0. There are special ways of doing sparse storage.</p>
<p>Dense storage is the normal way we do it when we code, we just store everything explicitly.</p>
</section>
<section id="speed" class="level3" data-number="5.0.4">
<h3 data-number="5.0.4" class="anchored" data-anchor-id="speed"><span class="header-section-number">5.0.4</span> Speed</h3>
<p>The difference in speed between algorithms come from a number of areas by in particular: - Computational Complexity - Vectorisation - Scaling - Locality</p>
<p><strong>Computational complexity</strong> and big <span class="math inline">\(\mathcal{O}\)</span> notation is about approximating the number of operations you need to do for a particular algorithm. More info: <a href="https://www.interviewcake.com/article/java/big-o-notation-time-and-space-complexity">on Interview Cake</a> and <a href="https://www.codecademy.com/courses/big-o/0/3">practice on Codecademy</a>.</p>
<p><strong>Vectorisation</strong> is about applying an operation is multiple elements at once. Numpy replies on vectorized low level linear algebra APIs (BLAS and LAPACK) to do it’s matrix operations.</p>
<p><strong>Locality</strong> is about how data in use is stored. Computers are usually slow because of the way we access data. Generally speaking, the faster the memory (so the faster we access the data), the less of it we have/the more expensive it is. Computers have many varying memory storage types, and each step down to slower memory you go, that memory is atleast an order of magnitude slower than the one before it.</p>
<p>We want to minimise the time we take to retrieve data in a computation. For example, by keeping items we are going to use multiple times in a computation in fast memory, and keepings items we use rarely in slow memory.</p>
<p>A video to illustrate locality is then shared.</p>
<p>from IPython.display import YouTubeVideo YouTubeVideo(“3uiEyEKji0M”)</p>
<p>Code optimisation is really important. Even for a simple task, finding the average of 3 pixels, the code would normally be simple, but writing complex code would speed it up by 11x! It’s faster because it distributes work across threads (parallelism). Locality is making sure the pixels being used successively is in fast memory (cashe). W/O locality, paralleism can’t be great.</p>
<p>We change the order in that CNN is done and as a result the way we store the pixel data, and get a much faster computational as a result. Removing redundancy in computation also speeds up the computation. Each computation technique has potential trade offs. For example, having redundant computation to improve locality.</p>
<p>Temporaries is data stored in a temporary variable in RAM. For example in Numpy, when we compute an equation, Numpy stores each equation variable as temporaries and then retrieves it. This is slow because there’s no point storing each variable in the RAM and then immediately having to use it. Simply storing these variables in the cache would be so much faster.</p>
<p><strong>Scaling:</strong></p>
<p>We we want to scale our computation across multiple cores in a computer. This is called parallesiation. Scalable algorithm are algorithms where the input can be broken into smaller pieces where each can be handled by a different core and the end result found by piecing together these pieces.</p>
</section>
</section>
<section id="links" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Links</h1>
<p>The lecture for this sessions is https://www.youtube.com/watch?v=8iGzBMboA0I&amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;index=1. <br> The lesson resources including the notebook(s): https://github.com/fastai/numerical-linear-algebra.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>