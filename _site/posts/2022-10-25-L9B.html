<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adnan Jinnah">
<meta name="dcterms.date" content="2022-10-25">

<title>RvCode - fast.ai Lesson 9B</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RvCode</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/exiomius/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">fast.ai Lesson 9B</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">fastai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Adnan Jinnah </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 25, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro"><span class="toc-section-number">1</span>  Intro</a></li>
  <li><a href="#lesson-overview" id="toc-lesson-overview" class="nav-link" data-scroll-target="#lesson-overview"><span class="toc-section-number">2</span>  Lesson Overview</a></li>
  <li><a href="#the-topics-covered-briefly" id="toc-the-topics-covered-briefly" class="nav-link" data-scroll-target="#the-topics-covered-briefly"><span class="toc-section-number">3</span>  The topics covered, briefly</a></li>
  <li><a href="#lecture-notes" id="toc-lecture-notes" class="nav-link" data-scroll-target="#lecture-notes"><span class="toc-section-number">4</span>  Lecture Notes</a></li>
  <li><a href="#useful-maths-resources" id="toc-useful-maths-resources" class="nav-link" data-scroll-target="#useful-maths-resources"><span class="toc-section-number">5</span>  Useful maths resources</a></li>
  <li><a href="#links" id="toc-links" class="nav-link" data-scroll-target="#links"><span class="toc-section-number">6</span>  Links</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Intro</h1>
<p>(This post is in progress) Part b) of the ninth lesson of fast.ai, covering the maths behind diffusion.</p>
</section>
<section id="lesson-overview" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Lesson Overview</h1>
</section>
<section id="the-topics-covered-briefly" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> The topics covered, briefly</h1>
</section>
<section id="lecture-notes" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Lecture Notes</h1>
<p>The maths can be beautiful, and is important for understanding papers.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>X is the input variable. the superscript 0 implies there is a sequence of Xs. There is a <span class="math inline">\(X^0, X^1, X^2, ... X^N\)</span>. Each X could be an image.</p>
<p>q is a probability density function. We use q instead of p.&nbsp;<br> q() is a function that takes an image, X, and tell us the probability that it is a handwritten digit. <br> q is the magic API/black box that we described from lesson 9. <br></p>
<p>Probability density functions rarely appear in your code. They are very useful tools to work with random quantities. It allows you to represent random quantities as functions. Eventually they end as simple equations in code. Thinking of things in terms of functions is useful because mathmatians have been understanding and working with them for ages.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>This is another PDF, the bar means it is conditional. Given the thing on the right is true, calculate the thing on the left.</p>
<p>Letâ€™s turn the image of 8 into another image using a magic API.</p>
<p>Here is an distribution.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>The N is for normal distribution. <br> The ; groups things together. <br> The thing on the left is what variable the probability distribution is for. <br> The things to the right are the parameters of this probability distribution. <br> In this case, it is the mean and variance of the normal distribution. We will sample random numbers from a normal distribution that has that mean and variance.</p>
<p>Thin tails means you look at the normal distribution at a certain place and ignore the rest.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>Notice how we call the second parameter <span class="math inline">\(\Sigma\)</span>, not <span class="math inline">\(\sigma\)</span>. It is capital because it is for multiple variables/directions. Lowercase is for one variable. Lowercase sigma also represents the standard deviation. sigma^2 being the variance.</p>
<p>??</p>
<p>The capital I in the covariance. We did CLIP in lesson 9. We took the dot product of their embeddings. If we subtract the means first. Imagine we had vectors on both sides.</p>
<p>Weâ€™re not doing one normal distribution. We have many. One for each pixel in an image. They all have different parameters so look/are different. Itâ€™s possible to create an N dimensional space to create a surface over all the N pixel normal distributions. If a value was 0 then it means every pixel is independnt of every other pixel. We start with I, the identity matrix. If we multiply it by a matrix, we get back the original matrix. If we do a scalar, we get a diagnal matrix of that scalar. So multiplying something by IBeta is just mulitplying it by B. Itâ€™s the same as doing a scalar multiplication.</p>
<p>The covariance, the relations between the pixels, we expect to be 0, as unrelated.</p>
<p>??</p>
<p>Looking back at our mean and covariance parameters. <img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid" alt="image.png"></p>
<p>What happens for <span class="math inline">\(X^0\)</span>, if <span class="math inline">\(\beta_{t}\)</span> = 0? we just get <span class="math inline">\(X^0\)</span> for the mean and a 0 variance. We have a normal distriubtion with a mean of our previous? image, and a variance of 0. This is just the same image! A variance of 0 means there is no noise or anything, I guess variance is like varying, there is no varying.</p>
<p>For <span class="math inline">\(X^0\)</span>, if <span class="math inline">\(\beta_{t}\)</span> = 1? We get 0 for the mean and I for the variance. An I variance implies a variance of 1. A normal distribution with (0,1), N(0,1). Our <span class="math inline">\(X^0\)</span> turns into pure noise because none of it was transferred over (from the mean being 0) and there is noise from the variance being non zero.</p>
<p>Between <span class="math inline">\(\beta_{t}\)</span> = 0 and <span class="math inline">\(\beta_{t}\)</span> = 1, we get a output thats <span class="math inline">\(X^0\)</span> with some noise.</p>
<p>The process of <span class="math inline">\(X^0\)</span> getting more and more noisy is called forward diffusion.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>The first input 8 we call <span class="math inline">\(X^0\)</span>. The first PDF takes <span class="math inline">\(X^0\)</span> and doesnt do anything. The second PDF takes <span class="math inline">\(X^0\)</span>, and calculates <span class="math inline">\(X^1\)</span> with it, which is more noisy. The final PDF is pure noise. We see that every successive PDF uses the previous image to calculate its image, like a chain of images. This is called a Markov Process with Gaussian transitions. Process means there is a sequence involved. Markov means that the output at time t only depends on the output at time t-1. The transition is the function that takes the time t-1 output to calculate the output at time t, so the PDF in our case.</p>
<p>Gaussian is the fact that the transition we do is a normal distribution. Sampling from the distriubtion means trying to find a random sample that maximises/has a high likelyhood. We have a magic API, we want to find data points that have a high probability from that API. If our distribution is Gaussian, since itâ€™s so well understood, itâ€™s easy to do this sampling. In other distributions, it can be hard to do this sampling.</p>
<p>Letâ€™s think about this practically. Gaussian distributions are nice. Take normal noise with mean of 0 and variance of 1, this is a unit distribution. To get to another variance with any mean or variance we want, we can simply take the unit distribution and multiply it by the variance and add it to the mean.</p>
<p>Most software has a way to get a sample from N(0,1) and we use the transform to get the distribution we want.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>We cant really sample from</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>But it turns out, if we have a 1000 smalls steps MPwGT that eventually goes to noise, we can represent the process of going backwards, from noise to image, in the same functional form!</p>
<p>p is the thing that goes backwards! q goes forwards, p goes backwards! Functional form:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>P takes the current image, and calculates the previous one. We know itâ€™s normally distributed, and weâ€™re interested in the distribution for the image weâ€™re calculating (the previous one). We donâ€™t know what that distributionâ€™s mean and variance are however.</p>
<p>q is the forward process. p is the backwards/reverse process. p is like finding x in a question, itâ€™s the thing we want to figure out. We want to figure out p, more specifically, the reverse normal distribution, to find q. q is like y, the output, which is the denoised/original image.</p>
<p>How to find these distriubtion parameters?</p>
<p>We can maximise the likeihood function. Try different parameters until we find one that maximises the likelihood. We canâ€™t do this exactly because itâ€™s an hard integral. We have 1000s of steps weâ€™re trying to reverse. There are going to be many distrubtion parameters for each step, so in total so many dist parameters to findâ€¦ Itâ€™s a challenge.</p>
<p>Instead of likelihood, people talk about log likelihood. Log is used as a computational trick. Log is always increasing, because it always increases, the optimal parameters are the same for log likelihood and likelihood. log always takes products to sums. We have joint distributions that are products, and we want them as sums because therye easier to work with. The normal distr has exponential functions, and those disspear with the log.</p>
<p>So letâ€™s optimise log likelihood instead: Thereâ€™s a way to optimise a quantity called ELBO. It stands for evidence lower bound. Evidence is another name for likelihood. Optimising Elbo is almost as good as optimising log likelihood. We use it as a loss function to train two neural networks that predict our mean and variance of the reverse process distributions.</p>
<p>The ELBO loss function. We have a forward process, original to noise, figuring out the distriubtions on the way. ELBO tries to match the backwards distribution(s) weâ€™re trying to optimise to the forward distriubtion(s). There is a specific type of function able to do this. We trying to minimise the difference between the distriubtions in the forward process and the dist in the backwards process.</p>
<p>We start with pure noise, keep calling p over many steps and eventually recover the original image and itâ€™s distribution.</p>
<p>We canâ€™t necceasry take an image, convert it to pure noise, then convert it back. But we can generate images of anything we want.</p>
<p>One more step. 2020 paper DDPM: Denoising Diffusion Probabilistic Model. They assumed the backwards function variance is just a constant, so we need not learn or find it. We assume also the step size, the variance of the noise added at each step, <span class="math inline">\(\beta_{t}\)</span>, is also a constant. We instead just predict the mean of the distriubtions.</p>
<p>This makes the loss just end up as the MSE of the noise. The entire problem just simplifies to become: a network takes in images, and it predicts what part of the network is noise. These assumptions result in models that produce much better images.</p>
<p>Jeremy said in the lesson that we want to figure out the gradient of the likelihood function. If we knew about this gradient, we could produce images with high likelihood.</p>
<p>The gradient:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>The idea is add noise to the images we have. This decreases the likelihood on these images. We make our model learn how to get back to high likelihood images, and use this to get some kind of estimate of our gradient.</p>
<p>There are theroms that state that the denoising process is equbilent to learning the score function. The score function is the gradient of the log likelihood. The general idea is the same.</p>
<p>Denoising allows us to learn the score function</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-10-25-L9B_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>with info about the score function, we can sample from the backwards distribution.</p>
<p>Recap:</p>
<p>Start with a data distribution of images to model. Forward diffusion adds noise to the model. We added noise in a specific way, so the reverse process has the same functional form. We already know how to train a neural netowrk using ELBO for this. Then we can use simplifying assumptions which end up the loss just end up as the MSE of the noise And another way to think about with is the score function approach, the gradient of the log likelihood.</p>
</section>
<section id="useful-maths-resources" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Useful maths resources</h1>
<ul>
<li>Maths cheat sheet https://ourway.keybase.pub/mathematics_cheat_sheet.pdf and wikipedia https://en.wikipedia.org/wiki/Glossary_of_mathematical_symbols#Other_brackets page for maths notation.</li>
<li>pix2tex to convert images of equations to latex https://github.com/lukas-blecher/LaTeX-OCR</li>
<li>Anki deck for Greek letters that are used in science and maths https://ankiweb.net/shared/info/2118139507</li>
<li>Detexify web page to draw symbols to see their latex</li>
</ul>
</section>
<section id="links" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Links</h1>
<ul>
<li>As I am doing this course as it is released privately live, I cannot share links to the lesson.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>