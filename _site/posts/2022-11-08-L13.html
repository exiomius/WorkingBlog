<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adnan Jinnah">
<meta name="dcterms.date" content="2022-11-08">

<title>RvCode - fast.ai Lesson 13</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RvCode</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/exiomius/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">fast.ai Lesson 13</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">fastai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Adnan Jinnah </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 8, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro"><span class="toc-section-number">1</span>  Intro</a></li>
  <li><a href="#lesson-overview" id="toc-lesson-overview" class="nav-link" data-scroll-target="#lesson-overview"><span class="toc-section-number">2</span>  Lesson Overview</a></li>
  <li><a href="#lecture-notes" id="toc-lecture-notes" class="nav-link" data-scroll-target="#lecture-notes"><span class="toc-section-number">3</span>  Lecture Notes</a></li>
  <li><a href="#reviewing-what-a-linear-model-and-relu-is" id="toc-reviewing-what-a-linear-model-and-relu-is" class="nav-link" data-scroll-target="#reviewing-what-a-linear-model-and-relu-is"><span class="toc-section-number">4</span>  Reviewing what a linear model and ReLu is:</a></li>
  <li><a href="#links" id="toc-links" class="nav-link" data-scroll-target="#links"><span class="toc-section-number">5</span>  Links</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Intro</h1>
<p>The thirteenth lesson of fast.ai.</p>
</section>
<section id="lesson-overview" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Lesson Overview</h1>
</section>
<section id="lecture-notes" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Lecture Notes</h1>
<p>watch 3blue1brown https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=4 on neural netwrosk!</p>
<p>03_backprop</p>
</section>
<section id="reviewing-what-a-linear-model-and-relu-is" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Reviewing what a linear model and ReLu is:</h1>
<p>Start by considering a linear model of a single pixel from an mnist image. The y value is likelihood of a image being a digit ‘3’. The x value is the value of the pixel from 0-63. A higher x value means a higher brightness. A linear model’s predictions are just a straight line: it predicts higher x values (higher brightnesses) to have a higher likelihood of being part of an image that is the digit ‘3’. This isn’t particularly useful. We cannot form curvature from a linear model from adding additional linear lines because a linear line plus a linear line is just another linear line. So instead, we add a ReLU! Everything to the left of the ReLU point is crushed to 0, and everything to the right is left alone. We can just keep adding more ReLUs in order to change the curvature to fit any arbitrary curve!</p>
<p>In reality, we don’t just use one pixel from an image to classify it. We use many, which create multidimensional spaces instead of 2D ones. In this case, we analogously create complex multidimensional curves by adding ReLus togther.</p>
<p>Ahead of time we decide how many of these Relu lines to add. To add them, we require matrix multiplication.</p>
<p>I don’t understand this entirely. In our MLP, we go on to create a network architecture Linear,ReLu,Linear. This makes sense because we couldn’t do Linear,Linear as that’s just Linear, but this concept of progressively adding ReLus together I.E, ReLu,ReLu,ReLu doesn’t make sense? Maybe what this actually means is “if you do enough Linear,ReLu pairs,you can create any arbitrary curve.” This makes sense because imagining an empty 2D graph, you create a linear line, then apply ReLu to it to get the ReLu line. Add this to other linear lines with ReLu applied then you create any curve. Yeah, by “adding ReLus together”, we actually mean “adding Linear,ReLu” pairs together!</p>
<p>Mnist:</p>
<p>To add nodes: multi by 784 rows and 10 columns. Each row in 50000 is one image’s pixels. Our output is 50000 by 10. Each row on the output is for an image. The first element is the probability of being ‘1’, the second ‘2’, etc. Not sure if the second matrix has 10 columns one for each classifcaiton or one for each node?</p>
<p>11:40 in Video. In reality, for 50 hidden nodes: We multiply a 50000x784 matrix by a 784x10 matrix. This is because if we take row one, the 784 pixel values of the first image, we multiply them by the 784 values in the 784x10 matrix. This matrix multiplication gives us our output matrix which is 50000x10. The first multiplication is in the first row first column of the output. It’s the first of 10 elements in that first row. If this was a linear model, each of the 10 elements per row in the output matrix represent probabilities. 0th element being for the digit 0, 1st element for the digit 1 etc. This is why the second matrix has to be 784x10, because only then do we get 10 elements per row in the output matrix. The second matrix has 10 columns. Each column allowing us to weight the 784 input pixels.</p>
<p>But this was for a linear model, so I think we do something more complex. Instead we do matrix multiplication twice to get our output matrix. 50000x784 matrix multiplied by 784x50 matrix. This creates 50 hidden layers. Then we truncate those at zero? Then we multiply the resulting 50000x50 matrix by a 50x10 matrix to create our output 50000x10 matrix!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-11-08-L13_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>W starts with random values. w is 50000x50 random values and we add biases, start them at 0. one for each output so 50 of them. w2 is layer two, 50 hidden layers, and 10 classifcation digits, so 50x10. But we’re not going to do cross entropy classifcation yet, we’ll use MSE isntead, so only one output, so 50x1.</p>
<p>The one output is just what number to we tink it is. We don’t give 10 probabilities by one value of what the model think it is. We compare these to the real labels using MSE. This is stupid because it says 9 is further away from 2 on correctness but that’s dumb. (I guess 8 and 9 are less wrong?).</p>
<p>x_valid is 10000x784 through weights and baises 10000x50 hidden activations/layers now through RElu, anything under 0 is 0.</p>
<p>an NLP is just first linear layer (w1,b1), then relu, then second linear layer (w2,b2).</p>
<p>and we get 10000x1 back, because thats prediction of a digit for each of the 10000 entries.</p>
<p>res is 10000 res-y_valid gives 10000x10000! why? because we start at the last dim and go R to L. 10000 and 1 is compatable, so that 1 will be braodcast along the 10000 values, giving 10000 values.</p>
<p>more to the left, there is nothing, so it inserts a 1, which makes 1 and 10000 compatable, so is broadcast among the 10000 and we get with with another 100000</p>
<p>we simply need to add a unit axis ourselves to the right to fix this! or just remove the 1 from 10000x1, .squeeze removes all unit dimensions. now we can get 10000 values as we want.</p>
<p>turn y_valid etc into floats since we’re suing MSE.</p>
<p>mse subtracts, ** 2 and gets the mean, now we have a loss function!</p>
<p>Now we need gradients.</p>
<p>Gradients are slopes. Consider a neural network. It contains a matrix of inputs, and a matrix of weights, then puts the result into a loss function, which we cmpare to the actual.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="attachment:image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>W starts with random values. w is 50000x50 random values and we add biases, start them at 0. one for each output so 50 of them. w2 is layer two, 50 hidden layers, and 10 classifcation digits, so 50x10. But we’re not going to do cross entropy classifcation yet, we’ll use MSE isntead, so only one output, so 50x1.</p>
<p>The one output is just what number to we tink it is. We don’t give 10 probabilities by one value of what the model think it is. We compare these to the real labels using MSE. This is stupid because it says 9 is further away from 2 on correctness but that’s dumb. (I guess 8 and 9 are less wrong?).</p>
<p>x_valid is 10000x784 through weights and baises 10000x50 hidden activations/layers now through RElu, anything under 0 is 0.</p>
<p>an NLP is just first linear layer (w1,b1), then relu, then second linear layer (w2,b2).</p>
<p>and we get 10000x1 back, because thats prediction of a digit for each of the 10000 entries.</p>
<p>res is 10000 res-y_valid gives 10000x10000! why? because we start at the last dim and go R to L. 10000 and 1 is compatable, so that 1 will be braodcast along the 10000 values, giving 10000 values.</p>
<p>more to the left, there is nothing, so it inserts a 1, which makes 1 and 10000 compatable, so is broadcast among the 10000 and we get with with another 100000</p>
<p>we simply need to add a unit axis ourselves to the right to fix this! or just remove the 1 from 10000x1, .squeeze removes all unit dimensions. now we can get 10000 values as we want.</p>
<p>turn y_valid etc into floats since we’re suing MSE.</p>
<p>mse subtracts, ** 2 and gets the mean, now we have a loss function!</p>
<p>Now we need gradients.</p>
<p>Gradients are slopes. Consider a neural network. It contains a matrix of inputs, and a matrix of weights, then puts the result into a loss function, which we cmpare to the actual.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="attachment:image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>The derivartive of the loss w.r.t one parameter in the weights tells us what direction (in opposite direction) to go to imporve loss. SGD is going in this direction in a small amount (with a learning rate), many times.</p>
<p>For 2D x,y dimensions case, where z is the output. the derivative is two values <img src="attachment:image-2.png" class="img-fluid" alt="image-2.png"></p>
<p>What if we’re in the first layer, where we get 784 inputs to the function and 50 outputs? Changing one input will affect all 50 outputs, and all 50 outputs depend on all 784 inputs? How do we express this mathematically? We get a matrix! <img src="attachment:image-3.png" class="img-fluid" alt="image-3.png"> In this case, there are 784 inputs so 784 rows, and each column represents on a row how changing that specific input affects a particular output. Each element is a derivative. Eventually we end up with the loss being a singule number, but we need to manipulate this matrix into one.</p>
<p>This matrix of all the derivatives is called the jacobian. There’s a resource linked by Jeremy for this which might help with understanding papers.</p>
<p>sympy to do symbols nicely!</p>
<p>Why we care about chain rule? Because we want MSE gradient w.r.t the model.</p>
<p>Our steps are loss function between prediction and actuals. l2 is the second layer the second layer is a function of relu activations (using w2 and b2) the relu act are functions of the first layer l1 and l1 is a function of the inputs using (w1 and b1).</p>
<p>We want the loss function derivative and need to go downwards! This is all backpropagation. It heavily uses the chain rule.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="attachment:image-4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image-4.png</figcaption><p></p>
</figure>
</div>
<p>forward pass calculates the loss. (loss = res.pow … should be diff.pow…) the loss is the outputput-target squared etc</p>
<p>backwards pass we store the graidents of each later, in the layer itself. out.g does so is 2 * diff and diff squared, and divide by inp shape is mean gradient of a linear layer, we need the gradients of the outputs to find it because of the chain rule.</p>
<p>for w.g., we got the input weights, multi them by the outputs?</p>
<p>unseuzeze (-1) adds an axis to the end?</p>
<p>each image inivudally contributes to the dervitaitve so we want to add them al lup. the derviatve of a sum of functions is the same of the dervtiatve of the sum of the dertative of the functions. so we do that!</p>
<p>x_train.zero_() sets it all to zero for us so we can rerunthings.</p>
<p>we realise with debug that an einsum can do this. rewatch this part carefully</p>
<p>we can replace some of the code with a matrix multiply!</p>
<p>dont worry about how pytorch does deritavies, we’ll do it ourselves later.</p>
<p>create a class for each function.</p>
<p>dunder call <strong>call</strong> creates a class like a function</p>
<p>the backward pass is interesting because it needs to know some intermedaite calcualtes because of the chain rule.</p>
<p>this is why class relu stores its output and its input, so when we do backward, it can wor, backward uses the chain rule.</p>
<p>class lin cals input and outputs and stores it.</p>
<p>for mse as calculate it and store i</p>
<p>Model calss stores instances of classess call each layer, store x as the result,</p>
<p>whats interesting about this is that we dont have two seperate functions instead of model like a loss func applied to a neural net, we’re calculated the loss inside the model. it’s not better or worse.</p>
<p>fast.ai usually loss as a sperate function, huggingface often combines them.</p>
<p>backward in model stores the inputs outputs etc to calculate it</p>
<p>now we can calculate the model and the loss, and check that each of the gradients we stored earlier are equal to each of our new gradients</p>
<p>Now every class can be separately considered.</p>
<p>As a rule of thump, when we see repeated code, like in our three classes, we can reduce it.</p>
<p>We use Module to reduce this repeated code from the three classes.</p>
<p>Forward doesnt do anything. The point of Module is to be inejeribed. All our backwards wanted self.out, so we def backwards to contain it. * means take all the arguments, and put them into a list. If you call an function with a * it means take this list and expand them into seprate arguments, calling backwards with each of them indivudally.</p>
<p>The old Relu class had to store things manually, the new relu just inhierited module and it’s forward and backwards functions just do whay we want them to.</p>
<p>There’s often opptunrities to manually speed things up. in Mse, inp.suqece()-targ .. is done twice. at memory cost, we can see this calculation as a new variable, diff, as self.diff, and remove the redundant calculation and just use it directly.</p>
<p>We can do this often in neural nets. A comprise behind losing memory for less computation.</p>
<p>Thankfully Pytorch has written classes for us which now we’ve written we can use. They’re called nn.X. So we create our classes inheiriting from nn.Module.</p>
<p>04_minibatch_training</p>
<p>EXCEL FILE.</p>
<p>For MSE, we simply output the most likely (highest probabilitiy) digit the model predicts from an image.</p>
<p>For cross entropy, we instead output 10 numbers for each image. Thats the probability that the image is ‘1’,‘2’…‘10’.</p>
<p>The y matrix (containing the correct labels) are now instead one-hot-encoded rows of each label.</p>
<p>E.g. y^ = ‘0.99,0,0,0,0.01..’ y = ‘1,0,0,0..’ gives low cross entropy loss!</p>
<p>For softmax: say we have 5 categories to predict. The network gets an image to predict. it outputs 5 values, these would be probabilities in the previous example but here they are not. Then we raise each value to e, and sum them Then we divide each value by the sum. This gives us our softmaxes. We will get 5 of them. Softmaxes are probability predictions. They add up to one.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2022-11-08-L13_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>z1 is output 1, z2 is output 2 etc (i=1, i=2). K = 5.</p>
<p>Now we have our 5 softmaxes.</p>
<p>For the loss func, we compare those softmaxes (probability) to the one hot encoded label. we calculate x-entropy for each softmax. It’s log(softmax) * actual. Actual is either 0 or 1. Because only one actual is 0, only one x-entropy is non zero, so the sum of x-entropies is just that one x-entropy.</p>
<p>In this special case where the actuals are one-hot-encoded: Doing actuals * log(softmax) is identicle to simply indexing into a category row and looking at its softmax.</p>
<p>This is review. Go back to part 1 for more detail.</p>
<p>END EXCEL FILE. Back to the notebook.</p>
</section>
<section id="links" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Links</h1>
<ul>
<li>As I am doing this course as it is released privately live, I cannot share links to the lesson.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>