<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adnan Jinnah">
<meta name="dcterms.date" content="2022-10-25">

<title>RvCode - Masters Project 25/10/2022</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RvCode</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/exiomius/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Masters Project 25/10/2022</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Masters</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Adnan Jinnah </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 25, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro"><span class="toc-section-number">1</span>  Intro</a></li>
  <li><a href="#meeting" id="toc-meeting" class="nav-link" data-scroll-target="#meeting"><span class="toc-section-number">2</span>  Meeting:</a>
  <ul class="collapse">
  <li><a href="#during-the-meeting-we-discussed" id="toc-during-the-meeting-we-discussed" class="nav-link" data-scroll-target="#during-the-meeting-we-discussed"><span class="toc-section-number">2.1</span>  During the meeting we discussed:</a></li>
  </ul></li>
  <li><a href="#problem-idea-update" id="toc-problem-idea-update" class="nav-link" data-scroll-target="#problem-idea-update"><span class="toc-section-number">3</span>  Problem idea update</a></li>
  <li><a href="#stable-diffusion-soundscape-generation" id="toc-stable-diffusion-soundscape-generation" class="nav-link" data-scroll-target="#stable-diffusion-soundscape-generation"><span class="toc-section-number">4</span>  Stable Diffusion Soundscape generation</a>
  <ul class="collapse">
  <li><a href="#stable-diffusion-mechanics" id="toc-stable-diffusion-mechanics" class="nav-link" data-scroll-target="#stable-diffusion-mechanics"><span class="toc-section-number">4.1</span>  Stable Diffusion mechanics</a>
  <ul class="collapse">
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="toc-section-number">4.1.1</span>  Training</a></li>
  <li><a href="#prompts-and-labels" id="toc-prompts-and-labels" class="nav-link" data-scroll-target="#prompts-and-labels"><span class="toc-section-number">4.1.2</span>  Prompts (and labels)</a></li>
  <li><a href="#an-ambitious-addition" id="toc-an-ambitious-addition" class="nav-link" data-scroll-target="#an-ambitious-addition"><span class="toc-section-number">4.1.3</span>  An ambitious addition</a></li>
  <li><a href="#sound-to-spectrogram-and-spectrogram-to-sound-conversion." id="toc-sound-to-spectrogram-and-spectrogram-to-sound-conversion." class="nav-link" data-scroll-target="#sound-to-spectrogram-and-spectrogram-to-sound-conversion."><span class="toc-section-number">4.1.4</span>  Sound to Spectrogram and Spectrogram to Sound conversion.</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#alternative-approaches-to-soundscape-generation" id="toc-alternative-approaches-to-soundscape-generation" class="nav-link" data-scroll-target="#alternative-approaches-to-soundscape-generation"><span class="toc-section-number">5</span>  Alternative approaches to soundscape generation</a>
  <ul class="collapse">
  <li><a href="#by-audio-diffusion" id="toc-by-audio-diffusion" class="nav-link" data-scroll-target="#by-audio-diffusion"><span class="toc-section-number">5.1</span>  By audio diffusion:</a>
  <ul class="collapse">
  <li><a href="#harmonai" id="toc-harmonai" class="nav-link" data-scroll-target="#harmonai"><span class="toc-section-number">5.1.1</span>  Harmonai</a></li>
  <li><a href="#the-generative-landscape" id="toc-the-generative-landscape" class="nav-link" data-scroll-target="#the-generative-landscape"><span class="toc-section-number">5.1.2</span>  The Generative Landscape</a></li>
  </ul></li>
  <li><a href="#by-other-generation-methods" id="toc-by-other-generation-methods" class="nav-link" data-scroll-target="#by-other-generation-methods"><span class="toc-section-number">5.2</span>  By other generation methods:</a></li>
  </ul></li>
  <li><a href="#stereo-data-as-a-novel-approach" id="toc-stereo-data-as-a-novel-approach" class="nav-link" data-scroll-target="#stereo-data-as-a-novel-approach"><span class="toc-section-number">6</span>  Stereo data as a novel approach</a></li>
  <li><a href="#unknown-yet-approaches" id="toc-unknown-yet-approaches" class="nav-link" data-scroll-target="#unknown-yet-approaches"><span class="toc-section-number">7</span>  Unknown yet Approaches</a></li>
  <li><a href="#work-done" id="toc-work-done" class="nav-link" data-scroll-target="#work-done"><span class="toc-section-number">8</span>  Work done</a>
  <ul class="collapse">
  <li><a href="#papers-about-birdclef-competitions" id="toc-papers-about-birdclef-competitions" class="nav-link" data-scroll-target="#papers-about-birdclef-competitions"><span class="toc-section-number">8.1</span>  Papers about BirdCLEF competitions</a></li>
  <li><a href="#updated-and-fixed-blog" id="toc-updated-and-fixed-blog" class="nav-link" data-scroll-target="#updated-and-fixed-blog"><span class="toc-section-number">8.2</span>  Updated and Fixed blog</a></li>
  <li><a href="#investigated-durham-computation-lending" id="toc-investigated-durham-computation-lending" class="nav-link" data-scroll-target="#investigated-durham-computation-lending"><span class="toc-section-number">8.3</span>  Investigated Durham Computation Lending</a></li>
  <li><a href="#found-yet-more-datasets" id="toc-found-yet-more-datasets" class="nav-link" data-scroll-target="#found-yet-more-datasets"><span class="toc-section-number">8.4</span>  Found yet more datasets</a></li>
  <li><a href="#found-another-another-thesis" id="toc-found-another-another-thesis" class="nav-link" data-scroll-target="#found-another-another-thesis"><span class="toc-section-number">8.5</span>  Found another another thesis</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Intro</h1>
<p>(This post is still in progress). The fifth post from a series of posts about my Masters project with the Physics Department at Durham University.</p>
</section>
<section id="meeting" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Meeting:</h1>
<section id="during-the-meeting-we-discussed" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="during-the-meeting-we-discussed"><span class="header-section-number">2.1</span> During the meeting we discussed:</h2>
</section>
</section>
<section id="problem-idea-update" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Problem idea update</h1>
<p>Following through on last week, it’s not a good project direction to focus on trying to replicate and beat Google’s paper directly. That’s simply too ambitious given the amount of work it would require to first just understand the paper and second to improve on it. Instead, a novel approach to it, and other competitors, is a better idea.</p>
<p>Something that I’ve been thinking about is an advanced data augmentation technique. <br> In BirdCLEF 2022, the training data is given as audio files of individual bird songs, but the test data are soundscapes. It’s not normal for training data and test data to be two different types. Normally both are the same so that you can create a model effectively to solve your desired problem. What would be ideal is having both soundscapes as the training and test data. <br> The reason why this is not the case I suppose would be the lack of a large number of labeled soundscapes to train on. Labeling enough soundscapes is simply too human time and resource intensive. Not only does listening to each file take several minutes, but requires an expert to label the birdsongs. It might even be too much for even an expert to know 100+ bird species’ song and be able to accuracy access them in noisy environments.</p>
<p>Instead of recording soundscapes and manually labeling them, what if it were possible to create synthetic soundscapes out of already labeled data? These synthetic soundscapes could already then be labeled if we transfer the labels over. In practice, this could follow a similar practice I described in last week’s post.</p>
<p>However, a newer idea sprung to mind. Instead of using a more manual approach to soundscape creation, how about using an AI generation approach?</p>
</section>
<section id="stable-diffusion-soundscape-generation" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Stable Diffusion Soundscape generation</h1>
<p>The basic idea goes as follows: <br> 1. Stable diffusion generates images from a given prompt. 2. A spectrogram is a 2D (or 3D) visualisation of a sound, it is an image. 3. Stable diffusion can generate a spectrogram of a birdsong given the right prompt. 4. If it’s possible to generate bird spectrograms, then it might be possible to generate entire soundscapes too, given the right prompt.</p>
<p>We could then use these soundscapes to train a model on.</p>
<p>The benefit of a novel data augmentation approach is that is doesn’t necessarily compete with already successful models such as Google’s. Instead it can complement them, making them even better.</p>
<section id="stable-diffusion-mechanics" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="stable-diffusion-mechanics"><span class="header-section-number">4.1</span> Stable Diffusion mechanics</h2>
<p>Stable diffusion heavily relies on two things: the data it has been trained on, and the prompt it is given to generate a given image.</p>
<section id="training" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="training"><span class="header-section-number">4.1.1</span> Training</h3>
<p>Currently as far as I can see, stable diffusion model online have been trained on ‘normal’ images in order to generate ‘normal’ artwork. It’s hard to define what normal is, but what it isn’t, is spectrograms of birdsongs and environments. My approach would likely require training a stable diffusion model from scratch on spectograms, of either relevant spectrograms or other audio, because intuitively, fine-tuning pretrained wouldn’t work well enough. Regardless it’s worth trying just too see results however.</p>
<p>To train a model yourself takes two things: enough data and enough computation. There should be enough data online given the size of xeno-canto. Computation is the harder demon. Being at a university fortunately might resolve this for me. There are computing clusters available for research use, so in theory it should be possible to request time for one. Alternatively, it is possible to rent GPU time from companies, but this would likely be very expensive, so would require research funding.</p>
<p>However, there might be a way to decrease computational needs: <br></p>
<p>To lower the computation needed to train a diffusion model, we use latent (compressed) representations of images. In fact, stable diffusion itself is a latent diffusion model rather than a general diffusion model specifically because of this. The autoencoder (vae) in stable diffusion is what does this image compression, and it makes a significant difference, reducing memory requirements for a 512x512x3 image by 48 times, speeding up training and inference (image creation) significantly.</p>
<p>Stable Diffusion is based on latent diffusion. It was proposed in a paper High-Resolution Image Synthesis with Latent Diffusion Models at https://arxiv.org/abs/2112.10752.</p>
<p>What if there is some specific autoencoder approach for spectrogram generation that could decrease computational needs significantly?</p>
</section>
<section id="prompts-and-labels" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="prompts-and-labels"><span class="header-section-number">4.1.2</span> Prompts (and labels)</h3>
<p>The entire point of this soundscape approach is to create labeled soundscapes because there are not enough available. If stable diffusion could create soundscapes, but not labeled ones, the whole approach falls out. Fortunately, the way stable diffusion creates images, using prompts, might also conveniently be the answer the answer to this issue.</p>
<p>An example I found on https://lexica.art/ at https://lexica.art/prompt/ea5b8646-6e6e-4a0e-b618-bae8c796f8cc of a generated image is as follows:</p>
<p>Prompt: “Scifi art by greg rutkowski, a man wearing futuristic riot control gear, claustrophobic and futuristic environment, detailed and intricate environment, high technology, highly detailed portrait, digital painting, artstation, concept art, smooth, sharp foccus ilustration, artstation hq”</p>
<p>Image:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="MP5_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>Not all prompts are like this, in fact, thinking about what prompts to use in itself is another entire process since it’s surprisingly not intuitive or easy to write good prompts.</p>
<p>Regardless, looking at our prompt, it also has words that can be related to labels. <br> “Scifi art”, “man”, “futuristic riot control gear”, “claustrophobic and futuristic environment”, “high technology”, “highly detailed portrait”, etc. Some words aren’t as useful, like “detailed and intricate environment” as it doesn’t give much information. Some words like “by greg rutkowski” are telling of the artist this was based on.</p>
<p>If we had a soundscape generation model, we could use a prompt I’m making up to illustrate like:</p>
<p>Prompt: “Forest enviroment, a Barn owl singing lightly at the start, lightly noisy environment, near a river, a Black grouse singing throughout, highly detailed, soundscape”</p>
<p>This prompt would include information about the labels we want, namely, “a Barn owl singing lightly at the start”,“near a river”, “a Black grouse singing throughout”.</p>
<p>This does mean however, that a model trying to use these soundscapes for training needs to be able to know how to use these labels properly, which is different from using the labels in BirdCLEF 2022 to train for instance. There is also the natural concern of whether the labels from the prompts are correct (enough) to make soundscape generation method good enough to be useful training for real soundscapes.</p>
</section>
<section id="an-ambitious-addition" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="an-ambitious-addition"><span class="header-section-number">4.1.3</span> An ambitious addition</h3>
<p>A spectrogram is a 2D (or 3D) visualisation of a sound, it is an image. <br> In the example given I used a 2D visualisation, but what about doing all of this in 3D? <br></p>
<p>Recently, Google released a paper about 3D image generation! That is creating 3D images from a written prompt. 2 minute papers has a brilliant video on it https://www.youtube.com/watch?v=L3G0dx1Q0R8. They call it “DreamFusion”.</p>
<p>Unofficial open source DreamFusion using stable diffusion is already becoming available. https://github.com/ashawkey/stable-dreamfusion. I really do wonder if using a 3D spectrogram to generate soundscapes would be better than a 2D one.</p>
</section>
<section id="sound-to-spectrogram-and-spectrogram-to-sound-conversion." class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="sound-to-spectrogram-and-spectrogram-to-sound-conversion."><span class="header-section-number">4.1.4</span> Sound to Spectrogram and Spectrogram to Sound conversion.</h3>
<p>Whether or not it’s possible to turn a spectrogram back into audio might not actually matter too much. <br> What I mean is, to classify the birds in a given test spectrogram, we simply convert it into a spectrogram and then use a image model trained on only spectrograms to classify it. I think there are CNN based papers that do this.</p>
</section>
</section>
</section>
<section id="alternative-approaches-to-soundscape-generation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Alternative approaches to soundscape generation</h1>
<section id="by-audio-diffusion" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="by-audio-diffusion"><span class="header-section-number">5.1</span> By audio diffusion:</h2>
<section id="harmonai" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="harmonai"><span class="header-section-number">5.1.1</span> Harmonai</h3>
<p>Stability.ai released stable diffusion. They also work in other areas, including AI in biology and AI in audio.</p>
<p>Last month they released Harmonai, an open source generative audio tool, dance diffusion. Dance diffusion allows you create music. It’s a digital music production tool. More info can be found at a wandb.ai blog post https://wandb.ai/wandb_gen/audio/reports/Harmonai-s-Dance-Diffusion-Open-Source-AI-Audio-Generation-Tool-For-Music-Producers–VmlldzoyNjkwOTM1, at Harmonai’s website https://www.harmonai.org/, and their GitHub https://github.com/Harmonai-org/sample-generator. There’s also a guide to using it at https://drive.google.com/file/d/1nEFEpK27v0nytNXmmYQb06X_RI6kKPve/view.</p>
<p>I haven’t yet throughly investigated the use of dance diffusion, but:</p>
<p>The latter guide detailed an interested model checkpoint (a pretrained model to fine-tune). It is honk-140k, trained on recordings of the Canada Goose from xeno-canto. This implies that it’s possible to generate birdsong with it, once trained.</p>
<p>But whether it’s possible to do labeled soundscape creation from individual labeled audio labels is my concern. About the labels, dance diffusion doesn’t seem to use prompts like stable diffusion, it seems to just create new sounds based on the trained data. This isn’t particularly useful because there is already enough birdsong available online.</p>
<p>Regardless, understanding how dance diffusion generates sound might yield some new ideas about how to use stable diffusion to do. Specifically, how it handles audio data. Perhaps I would find a way to to do labeled soundscape creation with it once I know how it works.</p>
</section>
<section id="the-generative-landscape" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="the-generative-landscape"><span class="header-section-number">5.1.2</span> The Generative Landscape</h3>
<p>There is a course online at https://johnowhitaker.github.io/tglcourse/. It covers all types of diffusion generation, including image and audio.</p>
<p>Lesson 15 at https://johnowhitaker.github.io/tglcourse/dm4.html is about Diffusion for Audio on Class conditioned birdcalls. The course is not yet complete yet, but should be soon.</p>
<p>The course is created by Jonathan Whitaker, who also is contributing to fast.ai part 2, so should be of great quality.</p>
</section>
</section>
<section id="by-other-generation-methods" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="by-other-generation-methods"><span class="header-section-number">5.2</span> By other generation methods:</h2>
<p>This would be doing the process by combining previous audio data together to create soundscapes. <br> Unlike diffusion methods, this isn’t as new as an idea, and has likely been tried and tested before. Because diffusion is so new, I’m more attracted to it as a novel idea.</p>
<p>However there are some things I could learn from these approaches.</p>
<p>The Earth Species Project (https://www.earthspecies.org/) released a paper about BioCPPNet at https://www.nature.com/articles/s41598-021-02790-2. This is about solving the cocktail problem to tell apart sounds from a group of animals of the same species. For example, to tell which individual is speaking from a group of macaques monkeys.</p>
<p>They have a video explaining BioCPPNet at https://www.youtube.com/watch?v=TGWFr-6JCDk. In particular at the 1 minute mark, they state “We implement a supervised training scheme: we construct a synthetic mixture dataset by additively overlapping signals”.</p>
<p>This is synthetic mixture dataset creation, which could be similar to soundscape creation, so could be very useful to understand.</p>
</section>
</section>
<section id="stereo-data-as-a-novel-approach" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Stereo data as a novel approach</h1>
<p>Last week with Stuart and Robert, we discussed using stereo data as our novel approach to the cocktail problem. Whether it is a novel approach and untried is going to take some research online to see if others have already tried this.</p>
</section>
<section id="unknown-yet-approaches" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Unknown yet Approaches</h1>
<p>I watched a video by Andrew Ng, the co-founder and head of Google Brain and the former chief scientist at Baidu. In it, he describes useful guidance on how to do machine learning research.</p>
<p>He states how to find new ideas for approaches to a problem:</p>
<ol type="1">
<li>You should learn how to replicate other papers’ results’. You learn a lot by doing so.</li>
<li>Reading many papers (20-50).</li>
</ol>
<p>Andrew says this is a incredibly reliable process to get new ideas.</p>
<p>The video can be found here https://www.youtube.com/watch?v=hkagmGAu74Y.</p>
</section>
<section id="work-done" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Work done</h1>
<section id="papers-about-birdclef-competitions" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="papers-about-birdclef-competitions"><span class="header-section-number">8.1</span> Papers about BirdCLEF competitions</h2>
<p>An immensely useful find this week are papers describing the different approaches the competition teams for BirdCLEF used.</p>
<p>The most recent: Overview of BirdCLEF 2022: Endangered bird species recognition in soundscape recordings, at http://ceur-ws.org/Vol-3180/paper-154.pdf.</p>
<p>For my soundscape generation approach, perhaps the most useful paragraph is found searching for ‘data augmentation’:</p>
<p>“Sampathkumar &amp; Kowerko [15]: Data augmentation is an important processing step in bird sound recognition because of the domain shift between training and test recordings. In their work, this team focused on evaluating the best augmentation scheme for this task. Most transformations focus on adding different patterns of noise to the source recording, thus emulating noisy soundscape recordings. While the authors find that all augmentations methods improve the baseline experiment, Gaussian noise, loudness normalization and tanh distortion appear to be most impactful.”</p>
<p>Most approaches added noise to the training data to emulate noise in the test soundscape. They did not create synthetic soundscapes entirely like I proposed. Gaussian noise, loudness normalization and tanh distortion appear to be the most useful noise to add.</p>
<p>And in general, the conclusion is useful:</p>
<p>“Despite being set up as a few-shot learning task, few teams decided to employ techniques other than CNNs. Pre-trained neural networks for image recognition still dominated the task, and participants tried to cope with the lack of training data through intensive data augmentation and transfer learning. Surprisingly, there was only a weak correlation between the number of training samples and overall per-species performance. This indicates that other factors - such as repertoire size and call patterns - might outweigh training data quantity. Automatic detection of endangered and rare species remains challenging. Still, this year’s competition demonstrated that passive acoustic monitoring combined with machine learning could already be a powerful monitoring tool for some endangered species. BirdCLEF continues to engage a large number of data scientists from around the world to develop new and effective acoustic analysis solutions that aid avian conservation.”</p>
<p>There’s a lack of training data because BirdCEF doesn’t provide all of the data available on xeno-canto, which isn’t an issue for my project. Somehow more training samples per species didn’t correlate strongly with better species identification, because of other factors. Most teams tried using CNNs, but not with spectrograms.</p>
<p>The competition overview does well to motivate my soundscape creation approach:</p>
<p>“In recent years, research in the domain of bioacoustics shifted towards deep neural networks for sound event recognition [7, 8]. In past editions, we have seen many attempts to utilize convolutional neural network (CNN) classifiers to identify bird calls based on visual representations of these sounds (i.e., spectrograms) [9, 10, 11]. Despite their success for bird sound recognition in focal recordings, the classification performance of CNNs on continuous and omnidirectional soundscape recordings remained low. Passive acoustic monitoring can be a valuable sampling tool for habitat assessments and observations of environmental niches, which often are threatened. However, manual processing of large collections of soundscape data is not desirable, and automated attempts can help to advance this process [12]. Yet, the lack of suitable validation and test data prevented the development of reliable techniques to solve this task.</p>
<p>Bridging the acoustic gap between high-quality training recordings and complex soundscapes with varying ambient noise levels is one of the most challenging tasks in the domain of audio event recognition. This is especially true when the amount of training data is insufficient, as is the case for many rare and endangered bird species around the globe. Despite the vast amounts of data collected on Xeno-canto and other online sound libraries, audio data for endangered birds is still sparse. However, those endangered species are most relevant for conservation, rendering acoustic monitoring of endangered birds particularly difficult.”</p>
<p>I should reading reference [12] of the paper to get a better idea of the soundscape availability problem.</p>
<p>Searching for ‘diffusion’ within the paper yields no results, implying further that indeed using diffusion to generate soundscapes is a novel approach.</p>
</section>
<section id="updated-and-fixed-blog" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="updated-and-fixed-blog"><span class="header-section-number">8.2</span> Updated and Fixed blog</h2>
<p>I was having trouble with my old fastpages based blog not displaying posts with maths and images correctly. To remedy this, I created a blog using Quarto instead. This post is on the new blog, and I transferred all the old posts here too.</p>
</section>
<section id="investigated-durham-computation-lending" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="investigated-durham-computation-lending"><span class="header-section-number">8.3</span> Investigated Durham Computation Lending</h2>
</section>
<section id="found-yet-more-datasets" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="found-yet-more-datasets"><span class="header-section-number">8.4</span> Found yet more datasets</h2>
<p>https://github.com/AgaMiko/bird-recognition-review has useful resources for birdsong classification and yet more datasets.</p>
<p>This blog post, https://towardsdatascience.com/sound-based-bird-classification-965d0ecacb2b, also contains an approach, but also a nice introduction to the problem.</p>
</section>
<section id="found-another-another-thesis" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="found-another-another-thesis"><span class="header-section-number">8.5</span> Found another another thesis</h2>
<p>Bird Species Classification And Acoustic Features Selection Based on Distributed Neural Network with Two Stage Windowing of Short-Term Features at https://arxiv.org/ftp/arxiv/papers/2201/2201.00124.pdf by Nahian Ibn Hasan like last week’s thesis, describes another good introduction to the problem and approach.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>