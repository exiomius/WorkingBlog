<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adnan Jinnah">
<meta name="dcterms.date" content="2022-10-25">

<title>RvCode - Masters Project 25/10/2022</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RvCode</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/exiomius/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Masters Project 25/10/2022</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Masters</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Adnan Jinnah </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 25, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro"><span class="toc-section-number">1</span>  Intro</a></li>
  <li><a href="#meeting" id="toc-meeting" class="nav-link" data-scroll-target="#meeting"><span class="toc-section-number">2</span>  Meeting:</a>
  <ul class="collapse">
  <li><a href="#during-the-meeting-we-discussed" id="toc-during-the-meeting-we-discussed" class="nav-link" data-scroll-target="#during-the-meeting-we-discussed"><span class="toc-section-number">2.1</span>  During the meeting we discussed:</a></li>
  </ul></li>
  <li><a href="#problem-idea-update" id="toc-problem-idea-update" class="nav-link" data-scroll-target="#problem-idea-update"><span class="toc-section-number">3</span>  Problem idea update</a></li>
  <li><a href="#stable-diffusion-soundscape-generation" id="toc-stable-diffusion-soundscape-generation" class="nav-link" data-scroll-target="#stable-diffusion-soundscape-generation"><span class="toc-section-number">4</span>  Stable Diffusion Soundscape generation</a>
  <ul class="collapse">
  <li><a href="#stable-diffusion-mechanics" id="toc-stable-diffusion-mechanics" class="nav-link" data-scroll-target="#stable-diffusion-mechanics"><span class="toc-section-number">4.1</span>  Stable Diffusion mechanics</a>
  <ul class="collapse">
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="toc-section-number">4.1.1</span>  Training</a></li>
  <li><a href="#prompts-and-labels" id="toc-prompts-and-labels" class="nav-link" data-scroll-target="#prompts-and-labels"><span class="toc-section-number">4.1.2</span>  Prompts (and labels)</a></li>
  <li><a href="#an-ambitious-addition" id="toc-an-ambitious-addition" class="nav-link" data-scroll-target="#an-ambitious-addition"><span class="toc-section-number">4.1.3</span>  An ambitious addition</a></li>
  <li><a href="#sound-to-spectrogram-and-spectrogram-to-sound-conversion." id="toc-sound-to-spectrogram-and-spectrogram-to-sound-conversion." class="nav-link" data-scroll-target="#sound-to-spectrogram-and-spectrogram-to-sound-conversion."><span class="toc-section-number">4.1.4</span>  Sound to Spectrogram and Spectrogram to Sound conversion.</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#alternative-approaches-to-soundscape-generation" id="toc-alternative-approaches-to-soundscape-generation" class="nav-link" data-scroll-target="#alternative-approaches-to-soundscape-generation"><span class="toc-section-number">5</span>  Alternative approaches to soundscape generation</a>
  <ul class="collapse">
  <li><a href="#by-audio-diffusion" id="toc-by-audio-diffusion" class="nav-link" data-scroll-target="#by-audio-diffusion"><span class="toc-section-number">5.1</span>  By audio diffusion:</a>
  <ul class="collapse">
  <li><a href="#harmonai" id="toc-harmonai" class="nav-link" data-scroll-target="#harmonai"><span class="toc-section-number">5.1.1</span>  Harmonai</a></li>
  <li><a href="#the-generative-landscape" id="toc-the-generative-landscape" class="nav-link" data-scroll-target="#the-generative-landscape"><span class="toc-section-number">5.1.2</span>  The Generative Landscape</a></li>
  </ul></li>
  <li><a href="#by-other-generation-methods" id="toc-by-other-generation-methods" class="nav-link" data-scroll-target="#by-other-generation-methods"><span class="toc-section-number">5.2</span>  By other generation methods:</a></li>
  </ul></li>
  <li><a href="#stereo-data-as-a-novel-approach" id="toc-stereo-data-as-a-novel-approach" class="nav-link" data-scroll-target="#stereo-data-as-a-novel-approach"><span class="toc-section-number">6</span>  Stereo data as a novel approach</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation"><span class="toc-section-number">6.1</span>  Motivation</a></li>
  <li><a href="#stereo-data-availability" id="toc-stereo-data-availability" class="nav-link" data-scroll-target="#stereo-data-availability"><span class="toc-section-number">6.2</span>  Stereo Data Availability</a></li>
  <li><a href="#papers" id="toc-papers" class="nav-link" data-scroll-target="#papers"><span class="toc-section-number">6.3</span>  Papers</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">6.4</span>  Conclusion</a></li>
  <li><a href="#biosciences-recordings" id="toc-biosciences-recordings" class="nav-link" data-scroll-target="#biosciences-recordings"><span class="toc-section-number">6.5</span>  Biosciences recordings</a></li>
  </ul></li>
  <li><a href="#unknown-yet-approaches" id="toc-unknown-yet-approaches" class="nav-link" data-scroll-target="#unknown-yet-approaches"><span class="toc-section-number">7</span>  Unknown yet Approaches</a></li>
  <li><a href="#work-done" id="toc-work-done" class="nav-link" data-scroll-target="#work-done"><span class="toc-section-number">8</span>  Work done</a>
  <ul class="collapse">
  <li><a href="#papers-about-birdclef-competitions" id="toc-papers-about-birdclef-competitions" class="nav-link" data-scroll-target="#papers-about-birdclef-competitions"><span class="toc-section-number">8.1</span>  Papers about BirdCLEF competitions</a></li>
  <li><a href="#finished-fast.ai-lesson-10" id="toc-finished-fast.ai-lesson-10" class="nav-link" data-scroll-target="#finished-fast.ai-lesson-10"><span class="toc-section-number">8.2</span>  Finished fast.ai lesson 10</a></li>
  <li><a href="#updated-and-fixed-blog" id="toc-updated-and-fixed-blog" class="nav-link" data-scroll-target="#updated-and-fixed-blog"><span class="toc-section-number">8.3</span>  Updated and Fixed blog</a></li>
  <li><a href="#investigated-durham-uni-supercomputers" id="toc-investigated-durham-uni-supercomputers" class="nav-link" data-scroll-target="#investigated-durham-uni-supercomputers"><span class="toc-section-number">8.4</span>  Investigated Durham Uni Supercomputers</a></li>
  <li><a href="#found-yet-more-datasets" id="toc-found-yet-more-datasets" class="nav-link" data-scroll-target="#found-yet-more-datasets"><span class="toc-section-number">8.5</span>  Found yet more datasets</a></li>
  <li><a href="#found-another-another-thesis" id="toc-found-another-another-thesis" class="nav-link" data-scroll-target="#found-another-another-thesis"><span class="toc-section-number">8.6</span>  Found another another thesis</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Intro</h1>
<p>(This post is still in progress). The fifth post from a series of posts about my Masters project with the Physics Department at Durham University.</p>
</section>
<section id="meeting" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Meeting:</h1>
<section id="during-the-meeting-we-discussed" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="during-the-meeting-we-discussed"><span class="header-section-number">2.1</span> During the meeting we discussed:</h2>
</section>
</section>
<section id="problem-idea-update" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Problem idea update</h1>
<p>Following through on last week, it’s not a good project direction to focus on trying to replicate and beat Google’s paper directly. That’s simply too ambitious given the amount of work it would require to first just understand the paper and second to improve on it. Instead, a novel approach to it, and other competitors, is a better idea.</p>
<p>Something that I’ve been thinking about is an advanced data augmentation technique. <br> In BirdCLEF 2022, the training data is given as audio files of individual bird songs, but the test data are soundscapes. It’s not normal for training data and test data to be two different types. Normally both are the same so that you can create a model effectively to solve your desired problem. What would be ideal is having both soundscapes as the training and test data. <br> The reason why this is not the case I suppose would be the lack of a large number of labeled soundscapes to train on. Labeling enough soundscapes is simply too human time and resource intensive. Not only does listening to each file take several minutes, but requires an expert to label the birdsongs. It might even be too much for even an expert to know 100+ bird species’ song and be able to accuracy access them in noisy environments.</p>
<p>Instead of recording soundscapes and manually labeling them, what if it were possible to create synthetic soundscapes out of already labeled data? These synthetic soundscapes could already then be labeled if we transfer the labels over. In practice, this could follow a similar practice I described in last week’s post.</p>
<p>However, a newer idea sprung to mind. Instead of using a more manual approach to soundscape creation, how about using an AI generation approach?</p>
</section>
<section id="stable-diffusion-soundscape-generation" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Stable Diffusion Soundscape generation</h1>
<p>The basic idea goes as follows: <br> 1. Stable diffusion generates images from a given prompt. 2. A spectrogram is a 2D (or 3D) visualisation of a sound, it is an image. 3. Stable diffusion can generate a spectrogram of a birdsong given the right prompt. 4. If it’s possible to generate bird spectrograms, then it might be possible to generate entire soundscapes too, given the right prompt.</p>
<p>We could then use these soundscapes to train a model on.</p>
<p>The benefit of a novel data augmentation approach is that is doesn’t necessarily compete with already successful models such as Google’s. Instead it can complement them, making them even better.</p>
<section id="stable-diffusion-mechanics" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="stable-diffusion-mechanics"><span class="header-section-number">4.1</span> Stable Diffusion mechanics</h2>
<p>Stable diffusion heavily relies on two things: the data it has been trained on, and the prompt it is given to generate a given image.</p>
<section id="training" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="training"><span class="header-section-number">4.1.1</span> Training</h3>
<p>Currently as far as I can see, stable diffusion model online have been trained on ‘normal’ images in order to generate ‘normal’ artwork. It’s hard to define what normal is, but what it isn’t, is spectrograms of birdsongs and environments. My approach would likely require training a stable diffusion model from scratch on spectograms, of either relevant spectrograms or other audio, because intuitively, fine-tuning pretrained wouldn’t work well enough. Regardless it’s worth trying just too see results however.</p>
<p>To train a model yourself takes two things: enough data and enough computation. There should be enough data online given the size of xeno-canto. Computation is the harder demon. Being at a university fortunately might resolve this for me. There are computing clusters available for research use, so in theory it should be possible to request time for one. Alternatively, it is possible to rent GPU time from companies, but this would likely be very expensive, so would require research funding.</p>
<p>However, there might be a way to decrease computational needs: <br></p>
<p>To lower the computation needed to train a diffusion model, we use latent (compressed) representations of images. In fact, stable diffusion itself is a latent diffusion model rather than a general diffusion model specifically because of this. The autoencoder (vae) in stable diffusion is what does this image compression, and it makes a significant difference, reducing memory requirements for a 512x512x3 image by 48 times, speeding up training and inference (image creation) significantly.</p>
<p>Stable Diffusion is based on latent diffusion. It was proposed in a paper High-Resolution Image Synthesis with Latent Diffusion Models at https://arxiv.org/abs/2112.10752.</p>
<p>What if there is some specific autoencoder approach for spectrogram generation that could decrease computational needs significantly?</p>
</section>
<section id="prompts-and-labels" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="prompts-and-labels"><span class="header-section-number">4.1.2</span> Prompts (and labels)</h3>
<p>The entire point of this soundscape approach is to create labeled soundscapes because there are not enough available. If stable diffusion could create soundscapes, but not labeled ones, the whole approach falls out. Fortunately, the way stable diffusion creates images, using prompts, might also conveniently be the answer the answer to this issue.</p>
<p>An example I found on https://lexica.art/ at https://lexica.art/prompt/ea5b8646-6e6e-4a0e-b618-bae8c796f8cc of a generated image is as follows:</p>
<p>Prompt: “Scifi art by greg rutkowski, a man wearing futuristic riot control gear, claustrophobic and futuristic environment, detailed and intricate environment, high technology, highly detailed portrait, digital painting, artstation, concept art, smooth, sharp foccus ilustration, artstation hq”</p>
<p>Image:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="MP5_files/figure-html/image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>Not all prompts are like this, in fact, thinking about what prompts to use in itself is another entire process since it’s surprisingly not intuitive or easy to write good prompts.</p>
<p>Regardless, looking at our prompt, it also has words that can be related to labels. <br> “Scifi art”, “man”, “futuristic riot control gear”, “claustrophobic and futuristic environment”, “high technology”, “highly detailed portrait”, etc. Some words aren’t as useful, like “detailed and intricate environment” as it doesn’t give much information. Some words like “by greg rutkowski” are telling of the artist this was based on.</p>
<p>If we had a soundscape generation model, we could use a prompt I’m making up to illustrate like:</p>
<p>Prompt: “Forest enviroment, a Barn owl singing lightly at the start, lightly noisy environment, near a river, a Black grouse singing throughout, highly detailed, soundscape”</p>
<p>This prompt would include information about the labels we want, namely, “a Barn owl singing lightly at the start”,“near a river”, “a Black grouse singing throughout”.</p>
<p>This does mean however, that a model trying to use these soundscapes for training needs to be able to know how to use these labels properly, which is different from using the labels in BirdCLEF 2022 to train for instance. There is also the natural concern of whether the labels from the prompts are correct (enough) to make soundscape generation method good enough to be useful training for real soundscapes.</p>
</section>
<section id="an-ambitious-addition" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="an-ambitious-addition"><span class="header-section-number">4.1.3</span> An ambitious addition</h3>
<p>A spectrogram is a 2D (or 3D) visualisation of a sound, it is an image. <br> In the example given I used a 2D visualisation, but what about doing all of this in 3D? <br></p>
<p>Recently, Google released a paper about 3D image generation! That is creating 3D images from a written prompt. 2 minute papers has a brilliant video on it https://www.youtube.com/watch?v=L3G0dx1Q0R8. They call it “DreamFusion”.</p>
<p>Unofficial open source DreamFusion using stable diffusion is already becoming available. https://github.com/ashawkey/stable-dreamfusion. I really do wonder if using a 3D spectrogram to generate soundscapes would be better than a 2D one.</p>
</section>
<section id="sound-to-spectrogram-and-spectrogram-to-sound-conversion." class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="sound-to-spectrogram-and-spectrogram-to-sound-conversion."><span class="header-section-number">4.1.4</span> Sound to Spectrogram and Spectrogram to Sound conversion.</h3>
<p>Whether or not it’s possible to turn a spectrogram back into audio might not actually matter too much. <br> What I mean is, to classify the birds in a given test spectrogram, we simply convert it into a spectrogram and then use a image model trained on only spectrograms to classify it. I think there are CNN based papers that do this.</p>
</section>
</section>
</section>
<section id="alternative-approaches-to-soundscape-generation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Alternative approaches to soundscape generation</h1>
<section id="by-audio-diffusion" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="by-audio-diffusion"><span class="header-section-number">5.1</span> By audio diffusion:</h2>
<section id="harmonai" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="harmonai"><span class="header-section-number">5.1.1</span> Harmonai</h3>
<p>Stability.ai released stable diffusion. They also work in other areas, including AI in biology and AI in audio.</p>
<p>Last month they released Harmonai, an open source generative audio tool, dance diffusion. Dance diffusion allows you create music. It’s a digital music production tool. More info can be found at a wandb.ai blog post https://wandb.ai/wandb_gen/audio/reports/Harmonai-s-Dance-Diffusion-Open-Source-AI-Audio-Generation-Tool-For-Music-Producers–VmlldzoyNjkwOTM1, at Harmonai’s website https://www.harmonai.org/, and their GitHub https://github.com/Harmonai-org/sample-generator. There’s also a guide to using it at https://drive.google.com/file/d/1nEFEpK27v0nytNXmmYQb06X_RI6kKPve/view.</p>
<p>I haven’t yet throughly investigated the use of dance diffusion, but:</p>
<p>The latter guide detailed an interested model checkpoint (a pretrained model to fine-tune). It is honk-140k, trained on recordings of the Canada Goose from xeno-canto. This implies that it’s possible to generate birdsong with it, once trained.</p>
<p>But whether it’s possible to do labeled soundscape creation from individual labeled audio labels is my concern. About the labels, dance diffusion doesn’t seem to use prompts like stable diffusion, it seems to just create new sounds based on the trained data. This isn’t particularly useful because there is already enough birdsong available online.</p>
<p>Regardless, understanding how dance diffusion generates sound might yield some new ideas about how to use stable diffusion to do. Specifically, how it handles audio data. Perhaps I would find a way to to do labeled soundscape creation with it once I know how it works.</p>
</section>
<section id="the-generative-landscape" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="the-generative-landscape"><span class="header-section-number">5.1.2</span> The Generative Landscape</h3>
<p>There is a course online at https://johnowhitaker.github.io/tglcourse/. It covers all types of diffusion generation, including image and audio.</p>
<p>Lesson 15 at https://johnowhitaker.github.io/tglcourse/dm4.html is about Diffusion for Audio on Class conditioned birdcalls. The course is not yet complete yet, but should be soon.</p>
<p>The course is created by Jonathan Whitaker, who also is contributing to fast.ai part 2, so should be of great quality.</p>
</section>
</section>
<section id="by-other-generation-methods" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="by-other-generation-methods"><span class="header-section-number">5.2</span> By other generation methods:</h2>
<p>This would be doing the process by combining previous audio data together to create soundscapes. <br> Unlike diffusion methods, this isn’t as new as an idea, and has likely been tried and tested before. Because diffusion is so new, I’m more attracted to it as a novel idea.</p>
<p>However there are some things I could learn from these approaches.</p>
<p>The Earth Species Project (https://www.earthspecies.org/) released a paper about BioCPPNet at https://www.nature.com/articles/s41598-021-02790-2. This is about solving the cocktail problem to tell apart sounds from a group of animals of the same species. For example, to tell which individual is speaking from a group of macaques monkeys.</p>
<p>They have a video explaining BioCPPNet at https://www.youtube.com/watch?v=TGWFr-6JCDk. In particular at the 1 minute mark, they state “We implement a supervised training scheme: we construct a synthetic mixture dataset by additively overlapping signals”.</p>
<p>This is synthetic mixture dataset creation, which could be similar to soundscape creation, so could be very useful to understand.</p>
</section>
</section>
<section id="stereo-data-as-a-novel-approach" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Stereo data as a novel approach</h1>
<p>Last week with Stuart and Robert, we discussed using stereo data as our novel approach to the cocktail problem. Whether it is indeed a novel approach and untried is going to take some research online to see if others have already tried this.</p>
<section id="motivation" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">6.1</span> Motivation</h2>
<p>The paper (likely outdated, 2017) Multi-band Approach to Deep Learning-Based Artificial Stereo Extension, attempts to use machine learning to turn mono audio into stereo audio.</p>
<p>It motivates that: “It is well known that stereophonic sound provides a more pleasant and natural experience than monaural (monophonic) sound on account of the presence of spatial information containing both ambience and/or the distinguished relative positions of objects and events”.</p>
<p>The idea behind using stereo data for birdsong classification is that this extra presence of spatial information, the information about relative positions of objects and events, would help.</p>
</section>
<section id="stereo-data-availability" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="stereo-data-availability"><span class="header-section-number">6.2</span> Stereo Data Availability</h2>
<p>BirdCLEF 2022, and other datasets, use data from xenocanto, so it is worth looking through it. Ideally, there would be a search tag to find stereo data.</p>
<p>On xenocanto records, under Technical details, it states various details. Here is an example from https://xeno-canto.org/757580. ## Xenocanto Technical Details - File type mp3 - Length 24.7 (s) - Sampling rate 44100 (Hz) - Bitrate of mp3 258189 (bps) - Channels 2 (stereo) - Device not specified - Microphone not specified - Automatic recording yes</p>
<p>It does tell us the number of channels and so whether it is stereo or mono. This example is missing the device and microphone, but I found another with a iphone using a Echo Meter Touch 2 Pro. There are also useful properties on xeno-canto like the recording quality and environment, as well as the type of bird sound. Whether it is a flight call or dawn song etc.</p>
<p>Xeno-canto doesn’t just have individual bird recordings, but also soundscapes that I suspect will be mostly unlabeled.</p>
<p>But how about searching for stereo data in bulk?</p>
<p>https://xeno-canto.org/help/search states how to do an advanced search. Entries are tagged, and you search through tags with tag:searchterm. Available tags include the country, the geographic coordinates, whether there are other species in the background, the recording quality. A limitation is that I cannot see how to search for specifically mono or stereo data. There is a tag for the device, the microphone, and the sampling rate, but not the number of channels. I can instead search using the remarks tag (the comments from the uploader) and the mic tag for stereo microphones.</p>
<p>Looking at the API at https://xeno-canto.org/explore/api, it has dvc: recording device used, mic: microphone used, smp: sample rate, but not explicitly the number of channels. Perhaps if I contact xeno-canto they will know of a way to look for just stereo data.</p>
<p>Another website Robert linked to me, freesound.org, explicitly has a stereo tag! https://freesound.org/browse/tags/stereo/. However, it only has 5093 recordings, and if I add ‘bird’ as a tag, only 241. There might however be other datasets for stereo data.</p>
<p>Since there are ways to record stereo data, I will assume the type of microphone used really matters to process delicate information like the relative positions of objects and events. This means that data might be very limited in supply. Either I can train a model on just general stereo audio and fine tune to see if it works for a specific microphone, or train on mono audio and fine tune, or train on some combination and fine tune.</p>
</section>
<section id="papers" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="papers"><span class="header-section-number">6.3</span> Papers</h2>
<p>The first paper in this section was about turning mono data into stereo data. There is also another paper at https://www.researchgate.net/publication/352807819_Identification_of_Fake_Stereo_Audio_Using_SVM_and_CNN, which tries to identify stereo audio data created from mono audio data. In theory you could use the two to improve a model’s ability to generate stereo data from mono data.</p>
<p>Classification of Bird Species using Audio processing and Deep Neural Network, at https://ieeexplore.ieee.org/abstract/document/9917735:</p>
<p>It discusses audio feature extractors like the spectrogram and Inverse Short Time Fourier Transform, as well as different ML approaches as related work.</p>
<p>They (frustratingly) don’t state their dataset’s name, only that it’s on Kaggle and that it’s 23.5gb. I searched through Kaggle and couldn’t easily find it based on that.</p>
<p>Regardless, in their dataset, 11472 audio files were mono, and 9903 stereo. Not a big difference. THere might be enough stereo data available, but not enough from a specific microphone if that is what is needed.</p>
<p>BirdCLEF 2022’s dataset doesn’t even include whether the files are stereo or mono in its metadata.csv file.</p>
</section>
<section id="conclusion" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">6.4</span> Conclusion</h2>
<p>Using stereo data might a good approach to the cocktail problem, but I’m unsure whether it would be novel because stereo data has been available for some time. Looking online for “Stereo vs Mono data for audio classification” is surprisingly dry for papers on Google and Google Scholar?</p>
<p>Because of its delicacy, stereo data might need to be collected from a specific microphone setup to extract it’s delicate information. Xeno-canto doesn’t seem to provide an easy way to search for stereo data, but does allow microphone searches. There is a concern that limiting this project to a specific stereo microphone will limit it’s usefulness for others.</p>
</section>
<section id="biosciences-recordings" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="biosciences-recordings"><span class="header-section-number">6.5</span> Biosciences recordings</h2>
<p>From the biology team, I have received some data of audio recordings.</p>
<p>The data in total 3.54 gb in size. There are 8 files, all .wav. Each file is 16000hz with two channels (stereo). <br> I used audacity to have a look at some of the files.</p>
<p>The recordings are from UK woodlands and from a couple of scrub (the wildlife habitat) sites. They are examples of the kind of typical data the biology team has collected. They’ve collected this data for 3-4 years daily during Jan-Jun for about 3-4 years from about 20 different locations.</p>
<p>What stands out from this dataset to me is the amount of time it has been collected over. If there was a good model to classify birds, it would be really interesting to see how the number of birds changes over the years in these varied locations. This could yield insights into changes in biodiversity due to climate change for example. An alternative project direction would be rather than trying to solve the cocktail problem, to understand and use someone else’s solution to do analysis on it’s results over time.</p>
<p>The audio files are not labeled. However I recall the biology team had a PhD student who handlabeled some data for them. This would be useful to look at, however due to how much time and effort it takes to handlabel soundscapes, there is most likely not enough data to train a model from scratch on.</p>
<p>Steve commented on the dataset: <br> “Attached are a selection of woodland audio files to have an initial play with, all from 2017, one from each site in mid-May. They are 2hrs 15mins long and start approx. 45mins before sunrise. So, the first 30 mins is often quiet, and things get gradually noisier and more complex thereafter. We have these data for 3-4 years, daily from about Jan-Jun for about 20 sites. Sites here include Abernethy Forest RSPB , Durham Uni Woodland, Minsmere RSPB, The Lodge RSPB, RSPB Wood of Cree, RSPB Ynis Hir, all from around mid-May. Also included, for slight contrast are a couple of scrub habitat sites (Green Farm nr Durham, Pinnock Hill near Durham)”</p>
</section>
</section>
<section id="unknown-yet-approaches" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Unknown yet Approaches</h1>
<p>I watched a video by Andrew Ng, the co-founder and head of Google Brain and the former chief scientist at Baidu. In it, he describes useful guidance on how to do machine learning research.</p>
<p>He states how to find new ideas for approaches to a problem:</p>
<ol type="1">
<li>You should learn how to replicate other papers’ results’. You learn a lot by doing so.</li>
<li>Reading many papers (20-50).</li>
</ol>
<p>Andrew says this is a incredibly reliable process to get new ideas.</p>
<p>The video can be found here https://www.youtube.com/watch?v=hkagmGAu74Y.</p>
</section>
<section id="work-done" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Work done</h1>
<section id="papers-about-birdclef-competitions" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="papers-about-birdclef-competitions"><span class="header-section-number">8.1</span> Papers about BirdCLEF competitions</h2>
<p>An immensely useful find this week are papers describing the different approaches the competition teams for BirdCLEF used.</p>
<p>The most recent: Overview of BirdCLEF 2022: Endangered bird species recognition in soundscape recordings, at http://ceur-ws.org/Vol-3180/paper-154.pdf.</p>
<p>For my soundscape generation approach, perhaps the most useful paragraph is found searching for ‘data augmentation’:</p>
<p>“Sampathkumar &amp; Kowerko [15]: Data augmentation is an important processing step in bird sound recognition because of the domain shift between training and test recordings. In their work, this team focused on evaluating the best augmentation scheme for this task. Most transformations focus on adding different patterns of noise to the source recording, thus emulating noisy soundscape recordings. While the authors find that all augmentations methods improve the baseline experiment, Gaussian noise, loudness normalization and tanh distortion appear to be most impactful.”</p>
<p>Most approaches added noise to the training data to emulate noise in the test soundscape. They did not create synthetic soundscapes entirely like I proposed. Gaussian noise, loudness normalization and tanh distortion appear to be the most useful noise to add.</p>
<p>And in general, the conclusion is useful:</p>
<p>“Despite being set up as a few-shot learning task, few teams decided to employ techniques other than CNNs. Pre-trained neural networks for image recognition still dominated the task, and participants tried to cope with the lack of training data through intensive data augmentation and transfer learning. Surprisingly, there was only a weak correlation between the number of training samples and overall per-species performance. This indicates that other factors - such as repertoire size and call patterns - might outweigh training data quantity. Automatic detection of endangered and rare species remains challenging. Still, this year’s competition demonstrated that passive acoustic monitoring combined with machine learning could already be a powerful monitoring tool for some endangered species. BirdCLEF continues to engage a large number of data scientists from around the world to develop new and effective acoustic analysis solutions that aid avian conservation.”</p>
<p>There’s a lack of training data because BirdCEF doesn’t provide all of the data available on xeno-canto, which isn’t an issue for my project. Somehow more training samples per species didn’t correlate strongly with better species identification, because of other factors. Most teams tried using CNNs, but not with spectrograms.</p>
<p>The competition overview does well to motivate my soundscape creation approach:</p>
<p>“In recent years, research in the domain of bioacoustics shifted towards deep neural networks for sound event recognition [7, 8]. In past editions, we have seen many attempts to utilize convolutional neural network (CNN) classifiers to identify bird calls based on visual representations of these sounds (i.e., spectrograms) [9, 10, 11]. Despite their success for bird sound recognition in focal recordings, the classification performance of CNNs on continuous and omnidirectional soundscape recordings remained low. Passive acoustic monitoring can be a valuable sampling tool for habitat assessments and observations of environmental niches, which often are threatened. However, manual processing of large collections of soundscape data is not desirable, and automated attempts can help to advance this process [12]. Yet, the lack of suitable validation and test data prevented the development of reliable techniques to solve this task.</p>
<p>Bridging the acoustic gap between high-quality training recordings and complex soundscapes with varying ambient noise levels is one of the most challenging tasks in the domain of audio event recognition. This is especially true when the amount of training data is insufficient, as is the case for many rare and endangered bird species around the globe. Despite the vast amounts of data collected on Xeno-canto and other online sound libraries, audio data for endangered birds is still sparse. However, those endangered species are most relevant for conservation, rendering acoustic monitoring of endangered birds particularly difficult.”</p>
<p>I should reading reference [12] of the paper to get a better idea of the soundscape availability problem.</p>
<p>Searching for ‘diffusion’ within the paper yields no results, implying further that indeed using diffusion to generate soundscapes is a novel approach.</p>
</section>
<section id="finished-fast.ai-lesson-10" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="finished-fast.ai-lesson-10"><span class="header-section-number">8.2</span> Finished fast.ai lesson 10</h2>
<p>This lesson was really useful in consolidating understanding about how stable diffusion works. It also starts the hard but rewarding journey of programming entire Python modules/frameworks from scratch, a skill likely vital further on in this project. The fast ai lessons for this advanced course are very demanding. They include a 2-2.30 hour lecture and plenty of homework, with the course creator Jeremy stating that he expects each lesson to take around 10 hours.</p>
<p>My blog for this lesson can be found at https://exiomius.quarto.pub/blog/posts/2022-09-27-Lesson10Blog.md.html.</p>
</section>
<section id="updated-and-fixed-blog" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="updated-and-fixed-blog"><span class="header-section-number">8.3</span> Updated and Fixed blog</h2>
<p>I was having trouble with my old fastpages based blog not displaying posts with maths and images correctly. To remedy this, I created a blog using Quarto instead. This post is on the new blog, and I transferred all the old posts here too.</p>
</section>
<section id="investigated-durham-uni-supercomputers" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="investigated-durham-uni-supercomputers"><span class="header-section-number">8.4</span> Investigated Durham Uni Supercomputers</h2>
<p>The computer science department has a list of machines at https://www.durham.ac.uk/departments/academic/computer-science/about-us/facilities/. I met a student who used one for some machine learning research. As I’m a student at both the physics and computer science department, I could possibly use the physics department’s supercomputers too, if they are suited towards ML.</p>
<p>For my purposes, Bede is GPU based and the description states it is ideally suited towards ML. Bede is shared between multiple universities. At Durham, the main contact Dmitry Nikolaenko at durham-bede-support@n8cir.org.uk.</p>
<p>Learning how to use Bede, as its Linux based, and how/where to store the training data etc is going to be a task within itself.</p>
</section>
<section id="found-yet-more-datasets" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="found-yet-more-datasets"><span class="header-section-number">8.5</span> Found yet more datasets</h2>
<p>https://github.com/AgaMiko/bird-recognition-review has useful resources for birdsong classification and yet more datasets.</p>
<p>This blog post, https://towardsdatascience.com/sound-based-bird-classification-965d0ecacb2b, also contains an approach, but also a nice introduction to the problem.</p>
</section>
<section id="found-another-another-thesis" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="found-another-another-thesis"><span class="header-section-number">8.6</span> Found another another thesis</h2>
<p>Bird Species Classification And Acoustic Features Selection Based on Distributed Neural Network with Two Stage Windowing of Short-Term Features at https://arxiv.org/ftp/arxiv/papers/2201/2201.00124.pdf by Nahian Ibn Hasan like last week’s thesis, describes another good introduction to the problem and approach.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>