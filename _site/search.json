[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my website. It’s currently in its early stages and hopefully will become more detailed with time. My name is Adnan, I’m a 4th year integrated Masters Physics and Computer Science student at Durham University. Currently I use this site to share blog posts about my various technical projects. They are on the main page, and ordered by category via the sidebar."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RvCode",
    "section": "",
    "text": "Masters\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIS\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastaiCLA\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-09-15-Lesson1&2Blog.md.html",
    "href": "posts/2022-09-15-Lesson1&2Blog.md.html",
    "title": "fast.ai Lessons 1&2",
    "section": "",
    "text": "2 Lesson Overview\nThe first lesson did well to summarise an introduction to model creation. While it was content I have already covered throughout previous courses, a quick refresher is good. The teaching philosophy behind the course, to learn by doing in conjunction to theory, is excellent in keeping things engaging and enjoyable. The second lesson covered many new software tools and applications very concisely. Although, I had to spend a lot of time troubleshooting various issues with getting things to work on my PC.\n\n\n3 The topics covered, briefly\n\nHow to create and train a deep learning model in FastAI\nHow to better filter and amend training data to get better model performance\nHow to export a ML model as a file\nHow to load ML models and allow websites to use them\nHow to save coding repositories online\nHow to host and create blog posts that include code easily\n\nThis included Kaggle/Google Collab for creating, training and exporting models. Anaconda for Jupyter Notebooks. Visual Studio Code, a programming IDE, to interact with: HuggingFaceSpaces, a website to host models, and GitHub, a website to store one’s code. It took much time and troubleshooting to understand how to install and use these tools, but the result is the understanding of how to quickly produce deep learning models and make them accessible on the internet, as well as produce blogs containing code, with good practices and expandability behind functionality.\n\n\n4 Links\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson2.html, which includes a lecture, a notebook, and a set of questions from the course book. My answers can be found on my GitHub repository below.\nMy machine learning model “NovaOrToast” attempts to classify whether a given cat photo is my older sister’s cat Toast, or my younger sister’s cat Nova. It can be found at https://huggingface.co/spaces/exiomius/NovaOrToast, where it is possible to upload images and get them classified immediately.\nMy repository for this can be found at https://github.com/exiomius/NovaOrToast The code for the model creation and training can be found at https://www.kaggle.com/code/adnanjinnah/nova-or-toast-model-creator/edit. One must use an online GPU service like Kaggle or Google Collab to train models due to their computational intensity. It should be possible to link said services with GitHub, which would be best to keep a centralised codebase.\n\n\n5 The pipeline to upload a machine learning model online\n\nFirstly create, train and export the model using a online IDE such as Google Collab or Kaggle. This is because the training will take up much GPU computation, so it is better to avoid doing it on one’s own machine.\nCreate a Jupyter Notebook and import the fully trained model. Use a Python module called Gradio to create a local webpage of your model to test if it works on your local web adress.\nConvert your Jupyter Notebook into a .py file.\nUsing HuggingFaceSpaces, create a space for your new model.\nUsing Visual Studio Code, clone the repository of your new space, then add in your Jupyter Notebook’s, .py file after conversion.\nCreate a ‘requirements’.txt including the modules used so that HuggingFaceSpaces can install them as required.\nUsing Visual Studio Code, use Git to push your update to HuggingFaceSpaces.\nAfter a few minutes, your model will be avaliable on the HuggingFaceSpaces space you created. The progress behind this will be available to see on your GitHub repository under the ‘actions’ tab.\n\nNot included are many many details of how to specifically do these instructions, as there are too many to throughly convey, especially as much time was spend troubleshooting for my specific machine and operating system (Windows).\n\n\n6 Things to improve:\n\nA better way to convert Jupyter Notebooks to .py files. It seems to work only with a settings.ini file, but I was not able to create one that works myself, so had to copy one on the course director’s GitHub, which works but I cannot figure out how to place the resulting .py file in the correct folder.\nHow to upload Kaggle code to GitHub seamlessly.\n\nA way to automatically update Anaconda’s modules. The default Anaconda update prompts update the app but not always the modules I require. For instance, it took me a while to figure out the Python Pillow module wasn’t updated on Anaconda despite it being a fresh installation."
  },
  {
    "objectID": "posts/2022-09-19-Lesson3Blog.md.html",
    "href": "posts/2022-09-19-Lesson3Blog.md.html",
    "title": "fast.ai Lesson 3",
    "section": "",
    "text": "2 Lesson Overview\nIn this lesson the mechanisms behind deep learning are explored. Unlike the previous lesson, which was focused on applications and many pieces of new software, this lesson was focused on reading through a textbook chapter and understanding how exactly deep learning operates.\nI suppose this knowledge will be required in order to understand how best to create good deep learning models.\n\n\n3 The topics covered, briefly\n\nHow are images represented in a computer.\nHow are files in datasets structured.\nPython functionality: what is list comprehension, and fact that NumPy/PyTorch are much faster than pure python.\nTensors: a list like structure used in deep learning. Ranks indicating dimensions and shapes indicating the length of each axis.\nLoss functions: RMSE and L1 norm. Two loss functions, similar but RMSE penalises larger errors more.\nSGD: stochastic gradient descent. A way to make a model learn by updating its weights automatically.\nThe difference between loss and metrics: Loss is for updating weights, metrics are for human evaluation.\nMini-batches. Each epoch, instead of using one piece of training data to update weights (pure SGD), or using all the training data simultaneously to update the weights (GD), splitting the data into random batches (subsets) of the training data to update the weights.\nThe sigmoid function: a smooth curve between 0 and 1 used in deep learning as a loss function. It is used because it has a meaningful derivative so allows good weight updates.\nDataLoader class: a function to take a dataset and split it into random mini-batches. DataLoaders contains a training a training and validation DataLoader.\nReLU: a rectified linear unit. It’s just a linear function, a line, with negative values = 0.\nActivation functions. Also known as nonlinear functions, these are placed between layers of linear functions in neural networks so that the linear functions do not combine into another linear function.\nThe Universal Approximation Theorem: that two linear layers with an activation function inbetween can approximate any function given the correct weights and enough nodes. In practice however we use more than two linear layers because it works better.\n\nMuch more content was covered in much more detail, including how to program a deep learning model from scratch in Python.\n\n\n4 Links\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson3.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository at https://github.com/exiomius/PDL-Lesson-3."
  },
  {
    "objectID": "posts/2022-09-21-Lesson4Blog.md.html",
    "href": "posts/2022-09-21-Lesson4Blog.md.html",
    "title": "fast.ai Lesson 4",
    "section": "",
    "text": "4: Natural Language (NLP)"
  },
  {
    "objectID": "posts/2022-09-21-Lesson4Blog.md.html#getting-started-with-nlp-for-absolute-beginners-covers",
    "href": "posts/2022-09-21-Lesson4Blog.md.html#getting-started-with-nlp-for-absolute-beginners-covers",
    "title": "fast.ai Lesson 4",
    "section": "3.1 “Getting started with NLP for absolute beginners” covers:",
    "text": "3.1 “Getting started with NLP for absolute beginners” covers:\n\nHow to use Kaggle. Including how to use Kaggle datasets on your own PC and how to submit entries to their competitions.\nHow to use Pandas and Transformers DataFrames to view data, and get it into the correct format for NLP model training.\nHow to use Transformers to train and classify.\nThe difference between training, validation, and testing data. An emphasis has been put on creating good validation data with a quote by Dr Rachel Thomas stating that often a poorly chosen validation dataset results in a disconnect between development and deployment performance. Her article describes this in more detail.\nTraining data is to train. Validation data is get an idea of generalizable performance, but often it is limited in doing so. This is often either because it hasn’t been chosen prudently enough or because one has accidentally overfitted to it. Imagine a model to predict the price of a stock. Randomly selecting points to be validation data is poor because that is not how the model will be used in practice and is a much easier task. Selecting some amount of further price movement to be validation data makes sense, but your model may overfit to the specific movement pattern of that timeframe.\nThe Pearson Correlation Coefficient, r, as a metric is discussed. Emphasis is put on firstly trying metrics on datasets to understand them, rather than delving immediately into the maths. Doing so for example yielding the fact that r is really sensitive to outliers.\nOn outliers. Outliers are important parts of the data and mustn’t be removed without reason. In our housing income and average number of rooms correlation example, there are outliers that could lead to some insights about the data. Perhaps the outliers are from a specific type of house or a specific geographical area. In this case, it may make more sense to use separate models to train on and predict separate clusters of data.\nOn hyperparameters. Learning rate is the most important in this case. The idea is to find the largest value that doesn’t result in failed training. FastAI provides a learning rate finder to help you, but Transformers does not."
  },
  {
    "objectID": "posts/2022-09-21-Lesson4Blog.md.html#iterate-like-a-grandmaster-covers",
    "href": "posts/2022-09-21-Lesson4Blog.md.html#iterate-like-a-grandmaster-covers",
    "title": "fast.ai Lesson 4",
    "section": "3.2 “Iterate like a grandmaster!” covers:",
    "text": "3.2 “Iterate like a grandmaster!” covers:\n\nHow a grandmaster Kaggle competitor works. He focused on creating an effective validation set and iterating rapidly to find changes which improve validation set results. These skills carry over to real projects.\nFor the patent classification, the input is anchor and label is target. In the test data, there are anchors not present in the training data. Thus we should make sure there are anchors in the validation data that are not in the training data.\nPick a learning rate and batch size that fits your GPU. This means picking them so that we can iterate rapidly to test things out.\nPick a reasonable weight decay.\nPick a small number of epochs, like 3, to test with. This is because in practice much of the performance will be made in those. Thus there is no need to run many epochs every time you try a change. If there is not improvement within a few epochs, then your change is likely not very significant. Later on, when you want to more thoroughly evaluate a change, you can use more epochs and cross-validation.\nPick a class to setup your arguments for your trainer. Transformers by default uses one, but FastAI has others.\nYou need a stable validation accuracy from your epochs to know whether your future changes is making improvements. To know whether your predictions are stable, run the model from scratch a few times, say 3, and check how much it varies.\nTo make changes easier, create a function to setup tokenization and a function to setup model creation. Then you can pass parameters quickly to create new models.\nIn this case, previously we tokenised with a special token sep to indicate seperate entities in our input. Simply changing sep to s resulting in a big performance increase.\nInstead of using the same special token to indicate separate entities in our input, using different special tokens for each entity could better inform the model that each entity is different.\nSimply changing all text to lowercase can often help a little too.\nThere’s so many things you could try. In this notebook, most of the iteration was done by changing tokenisation, but also playing around with the other parameters might yield better results. However, instead of trying to optimise the factors already present, there are other ideas you can try. Firstly fine tuning your general language model using just patent data. Or using a model pretrained on legal vocabulary instead of general vocabulary. Using a different type of model, not in terms of architecture but a model created for a different task. One of our columns is ‘context’, which is a code e.g. B7 referring to a patent context. Instead of using the code, we could replace it with a description found online. There’s so many things you can try especially if you think a little creatively.\nRemember for the final submission, to train on your validation data as well."
  },
  {
    "objectID": "posts/2022-09-25-MP1.html",
    "href": "posts/2022-09-25-MP1.html",
    "title": "Masters Project Introductory Post",
    "section": "",
    "text": "2 Title: Data science for biodiversity loss\nSupervisor: Prof Charles Adams. Second Supervisor: Dr Robert Potvliege. Category: General. Type: Computation/Data Analysis/Experiment.\nBiodiversity loss due to human action is increasingly creating an existential threat to all live on Earth. In order to take appropriate action we need better data. However biodiversity data is both more diverse and more difficult to accumulate than say climate data. In this project, we shall look at data analysis on bird song. Although, under ideal conditions it is possible to identify different species [1-3]. In a noisy environment, which is more typical, this becomes more challenging. One approach that we shall pursue is to construct time frequency pattern and then use pattern recognition technique to identify particular events [4].\n[1] scikit‐maad An open‐source and modular toolbox for quantitative soundscape analysis in Python, Ulloa et al, Meth. in Ecology and Evolution, 12, 2334 2021\n[2] Multifractal analysis of birdsong and its correlation structure, R Bishal, GB Mindlin, and N Gupte Phys. Rev. E 105, 014118 2022\n[3] Large-scale analysis of frequency modulation in birdsong data bases, D Stowell, MD Plumbley, Methods Ecol Evol, 5: 901 (2014)\n[4] New aspects in birdsong recognition utilizing the gabor transform, S HEUER , P TAFO, H HOLZMANN, S DAHLKE, Proc. of the 23rd Int. Congress on Acousitics, Aachen, September 9-13, 2019.\n\n\n3 Aim\nIn essence, the (first) aim of this project is to pick apart bird song from a noisy sound recording and identify birds from it.\nIn the future other aims may emerge, such as trying to understand the meaning behind birdsong rather than classify their singers. Perhaps even trying to generate birdsong may be an interesting idea to yield some insights. It would be fascinating seeing if/how birds would respond to generated birdsong.\n\n\n4 Approaches\nThe four above referenced papers are probably the best place to start when looking for initial approaches. While I am comfortable with both Physics and Programming, my preference in comfortability and in interest does lean towards the latter, especially when Machine Learning is involved. Having said that, the first, third and fourth reference all are ML based to a degree.\nI don’t yet have the prerequisite knowledge to understand at a glance the abstracts of the second and third references.\nThe first however is an open-source Python module called scikit-maad, which is instantly recognisable by my familiarity with other popular scikit modules. Furthermore, I feel warmly welcomed by the described online documentation and practical examples around it. Lastly, the module highlights its ability to easily integrate Machine Learning Python packages.\nThe fourth reference details that current approaches convert audio recordings into spectrograms using the Gabor transform, then enter them as images to a CNN for classification. This is a really intuitive ML approach, and one I might try and implement. The paper then details that most approaches focus on finding the best CNN hyperparameters for accuracy, so in contrast the authors attempt to evaluate the parameters for the Gabor transform itself.\nAll in all, I think my first priority is to investigate and implement the first and fourth references. It’s not ideal not being able to easily understand the other papers, but my reasoning is that having gone through the more understandable references first would yield prerequisite knowledge to go back and understand the others.\nCoincidentally, I was talking briefly with my older brother about the project and he commented that the problem is awfully similar to other audio separation problems. Separating out bird song from a noisy forest environment and then identifying them, is similar to separating out instrument sounds from a regular song and identifying them. We both actually have a mutual friend who did his Masters project in the latter. Spotify also appears to have its own development going on for this problem. Looking through how similar these two problems are might prove very useful.\nI will meet with my supervisors next week and discuss our next steps in more detail. The fast.ai part 2 course is releasing soon and I would like to progress through it both for the sake of the project and my own interests."
  },
  {
    "objectID": "posts/2022-09-25-MP2.html",
    "href": "posts/2022-09-25-MP2.html",
    "title": "Masters Project 04/10/2022",
    "section": "",
    "text": "The second post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/2022-09-25-MP2.html#during-the-meeting-we-discussed",
    "href": "posts/2022-09-25-MP2.html#during-the-meeting-we-discussed",
    "title": "Masters Project 04/10/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nWhat I had been learning during the summer, namely the first fast.ai course, the state of exponential growth of machine learning, my website, etc. We are all happy with the progress I have made so far. I will estimate that I spent around 40 hours working this summer, ahead of the 30 hours the department set.\nFor example, I showed how I can upload machine learning models to HuggingFaceSpaces, which lets me send a link easily to Stuart, Robert, the biology dept etc and let them test and play with what I’m doing. This will be very useful as I physically will not need to take my computer over to get feedback on models.\nHow the project will be examined: by extended report (like a dissertation), a seminar, a supervisors mark (on your effort and from a viva), and also a provisional formative 10 page report. An external supervisor will be picked at random from 4 academics Stuart and Robert pick to read your report.\nStuart said to take the formative report seriously, because feedback on your writing style etc is really important. Your supervisors can’t give feedback on your real report later on due to department rules.\nWe will meet with Phil and Steve, from the biology department, next week, meeting at the TLC at 10:55.\nStuart has two audio recording devices that use SD cards to record. He said that he will order SD cards. I could use these to collect sound data if necessary. There is a large amount of data already collected, both by the biology department and available online, but if I wanted data collected in a specific way, then I could do it myself. For example, as we have two devices, I could set both of them up to gather the same audio, and use that to analyse out noise. What I could do, is use the same microphones the biology department did, and use them to remove noise from their whole dataset.\nOn a ambitious note, the biology department has been using community volunteers to look through photos and label animals in them. In a similar way, if this project is really successful, we could get a grant to lend out a specific recording setup to them and get a lot of specific data.\nRobert explained some possible project aims relating to biodiversity. For example, if we could classify birds by audio, we could look for birds that shouldn’t be in the UK or in specific areas. Birds that are migrating in the wrong direction, due to climate change or other environmental issues. This could tell us about biodiversity. If we had data of audio collected over months or years, then about how biodiversity is changing over time. I commented that if the biology department also recorded the GPS location of their recording device, it could be possible to map out how the number of specific birds changes in various areas.\nI asked Stuart whether any of his other supervision students are working on a similar problem to me. While he has many students, we are all doing varied projects which is interesting but hinders collaboration. For now, there should be a PhD student working with the biology team with I could work with.\nI explained that I want my diss to be written in a way it can be understood by anyone with no prerequisite knowledge. This stems from me expressing that the fundamental mechanisms in which machine learning works is actually very simple, it’s high school level maths and can be explained in one or two pages. This approach is good because it will make examination and marking easier.\nI explained that for frequent progress updates, I want to continue writing blogs on this website and link it to the teams channel. Right now one blog a week after the week’s meeting is probably a good pace.\nWe agreed on my plan to spend half my time working directly on the problem, and the other half going through the second fast.ai course starting Tuesday next week."
  },
  {
    "objectID": "posts/2022-09-25-MP2.html#this-week-i-want-to",
    "href": "posts/2022-09-25-MP2.html#this-week-i-want-to",
    "title": "Masters Project 04/10/2022",
    "section": "4.1 This week I want to:",
    "text": "4.1 This week I want to:\n\nFinish fast.ai part 1. Namely the questions and blog posts for the last 2 lessons.\nGo and investigate some of the project’s proposal’s references starting with scikit-maad and a CNN classifier.\nAsk for a reimbursement for fast.ai part 2"
  },
  {
    "objectID": "posts/2022-09-26-Lesson5Blog.md.html",
    "href": "posts/2022-09-26-Lesson5Blog.md.html",
    "title": "fast.ai Lesson 5",
    "section": "",
    "text": "2 Lesson Overview\nThis lesson focused on creating a neural network from scratch using Python and PyTorch. It covered much content present in the textbook chapter for the previous lesson. The lecture goes through how to initialise a neural net with one hidden layer, and then how to add more layers easily in order to create a deep learning model.\n\n\n3 The topics covered, briefly\n\nHow to build a neural network, from scratch.\nData cleaning: how to work with missing data and normalise.\nSigmoid function for classification.\nFeature engineering is important for tabular data.\nWhat is a binary split/OneR.\n\n\n\n4 Lecture Notes:\n\nFor data cleaning, when there is missing data fields, usually just use the mode. Other ways might be better, but the method is usually inconsequential compared to other factors.\nFor a linear model, so when there is only one equation with coefficients linking the inputs and the output, look at the coefficients to understand what inputs most contribute to the output.\nWe normalise the input data so that while training some inputs don’t overpower others. For example, if we wanted to predict lifetime duration with income and number of siblings, we have to make both variables around the same size initially so that income doesn’t just dominate the output mathematically.\nThe way we prepare input data is often similar for different types of models, e.g. classification and regression.\nIn contrast the way we modify the final output is very much dependent on the model. For classification, we apply a sigmoid function in order to get the outputs to between 0 and 1. Fast.ai does this automatically.\nFor tabular data specifically, feature engineering is really important. What the columns mean and how they relate to the prediction, can literally make and break models. If you can think of a creative way the columns relate and base your model around it, you could find a new way to make competition winning predictions. It’s not at all about just trying to teak different factors and hyperparameters to find the best predictions.\nFor quick iteration testing, you can keep the random seed for data creation/sampling the same. But for mature testing, make sure to vary it up because you need to know whether having different initialisations makes a difference, lest you accidentally overfit to them.\nEnsembles, where each network has different initialisations, can be really helpful to avoid overfitting. - Since random forest models (not deep learning) are hard to mess up, they might be a good baseline against deep learning and or more complicated approaches.\nA binary split attempts to find the best number for a variable to split the data into halves and make a prediction based on them. For example, if I wanted to predict whether someone lived in area 1 or area 2 by their income, a binary split would calculate a specific income and make all predictions above that income be area 1, and all predictions below that income, be area 2.\n\nGoing through the fundamentals of neural networks and building one from scratch atleast once I think is important to build a foundational understanding of what you are doing. It clears up that murky feeling that machine learning is some kind of magic. It is not, it really is just a set of logical instructions which actually doesn’t require more than high school maths. It reminds me of when I first had a proper maths lecture detailing integration. Previously I knew what it was and what the results meant, but the way the actual method of it worked felt like some sort of magic. One deep fundamental dive into it did well to make the concept click and clearly that murky feeling up felt amazing.\nIt’s worth just quickly clarifying that a deep learning model is just a neural net with three or more layers.\n\n\n5 Links\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson5.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6."
  },
  {
    "objectID": "posts/2022-09-26-Lesson6Blog.md.html",
    "href": "posts/2022-09-26-Lesson6Blog.md.html",
    "title": "fast.ai Lesson 6",
    "section": "",
    "text": "6: Random Forests"
  },
  {
    "objectID": "posts/2022-09-26-Lesson6Blog.md.html#the-lecture-than-goes-through-a-more-in-depth-notebooks-than-iterate-like-a-grandmaster-called-road-to-the-top-part-1-and-part-2",
    "href": "posts/2022-09-26-Lesson6Blog.md.html#the-lecture-than-goes-through-a-more-in-depth-notebooks-than-iterate-like-a-grandmaster-called-road-to-the-top-part-1-and-part-2",
    "title": "fast.ai Lesson 6",
    "section": "4.1 The lecture than goes through a more in depth notebooks than Iterate like a grandmaster called road to the top part 1 and part 2:",
    "text": "4.1 The lecture than goes through a more in depth notebooks than Iterate like a grandmaster called road to the top part 1 and part 2:\n\nJust squish images, it’s better than cropping. Padding would take more precious CPU power for iteration? (See point below). TTA, augmenting input images, can improve accuracy too.\nThere is another notebook: https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning, showing the best models for vision to use for quick iteration and testing.\nThe fastai learning rate suggestions are a bit conservative so you can pick higher than them.\nCreate different notebooks for different approaches. Duplicate and rename them.\nKaggle’s GPUs aren’t amazing, but for Jeremy his home PC ran training so much faster. The problem wasn’t GPU, it was CPU, the Kaggle CPU indicator showed it as full. This is a image input problem. Loading them up requires CPU power. Simply resizing the images by 1/4 quickened up iteration by 4x but without sacrificing accuracy! Perhaps larger image sizes aren’t so important. But later on, with some data augmentation, using larger image sizes helped.\nSince the GPU was barely used, you might as well switch to a model that’s more demanding. It probably won’t take much longer.\nKeep upto date with new model architecture. ConvNext is a great default speed and performance for now."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html",
    "href": "posts/2022-09-27-Lesson10Blog.md.html",
    "title": "fast.ai Lesson 10",
    "section": "",
    "text": "10: Stable Diffusion 2"
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#for-example-for-handwritten-digits",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#for-example-for-handwritten-digits",
    "title": "fast.ai Lesson 10",
    "section": "4.1 For example, for handwritten digits:",
    "text": "4.1 For example, for handwritten digits:\n\n7 + Noise = Noisy 7.\nGive Noisy 7 to unet, and unet predicts the Noise.\nIt then compares its prediction to the actual noise to get a loss to improve itself.\nTo make the unet work better, we pass in a label: an embedding of 7. For example, we could enter a one-hot encoded vector of it.\nIf we do this, then later on we can get the unet to create 7 of us because it understands what the embeddings of 7 are."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#to-train-these-encoders",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#to-train-these-encoders",
    "title": "fast.ai Lesson 10",
    "section": "5.1 To train these encoders:",
    "text": "5.1 To train these encoders:\n\nFirstly we find images online that are captioned (has a prompt).\nWe input an image into the image encoder, and this produces embeddings (a feature vector) of the image.\nWe input the image’s prompt into the text encoder, and this produces embeddings (a feature vector) of the prompt.\nWe then train both to get these two sets of embeddings to be the same.\nThis is done with contrastive loss: prioritising these two sets of embeddings to be the same for when an image and prompt match, and penalising when they don’t.\n\n\n\n\nimage.png\n\n\nBoth encoders now can produce embeddings, but we only care about the text encoder. It can take prompts of images and create embeddings for them. With this, we can input into our unet both an image and (an embedding of) its prompt!"
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#progressive-distillation-for-fast-sampling-of-diffusion-models",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#progressive-distillation-for-fast-sampling-of-diffusion-models",
    "title": "fast.ai Lesson 10",
    "section": "9.1 Progressive Distillation for Fast Sampling of Diffusion Models:",
    "text": "9.1 Progressive Distillation for Fast Sampling of Diffusion Models:\nLooking at the inference process, we can reduce the steps to denoise an image using distillation. Distillation is a pretty common technique in deep learning.\nLook at step 36 and step 54 of inference below:\n\n\n\nimage.png\n\n\nWhy is step 36 to 54 taking a whole 18 steps when there’s not much left to do comparatively to step 0 to 18 for instance?\nThe reason is, it takes very long because of a side effect of how the original maths of stable diffusion works.\nBut the thing is, after each step, we have an output image to play with! (we only plotted the output images every 6 steps).\nWhat if we used a new model, unet B. unet B can take the output of step 36, and try to create the output of step 54. We can then compare the two to train unet B to learn how to get from step 36 to step 54 directly!\nWe have a teacher network (it already knows how to do something, but is slow and big), and a student network (it tries to do the same as the teacher but faster and with less memory).\nOur teacher model is the original complete stable diffusion model. Our student model is the unet B skipping steps.\nGenerally speaking, we start with a noisy latent and the teacher model denoises it for 2 steps (it doesn’t create an image in 2 steps, just does 2 steps). Our student model then learns how to take the noisy latent and do the teacher’s 2 step denoising in 1 step.  Then we get the starting noisy latent and get the student model to denoise it for 2 steps.  And make another student learn how to do 2 steps of this in 1 step.\nThe original teacher model did some denoising in 2 steps. The first student could do that denoising in 1 step, so 2 steps of it would result in 4 of the original. The second student does the first’s in 1 step, so 2 steps of it results in 8 of the original. And so on, I suppose until you end up with 3-4 steps being enough to generate an image!"
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#on-distillation-of-guided-diffusion-models-paper",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#on-distillation-of-guided-diffusion-models-paper",
    "title": "fast.ai Lesson 10",
    "section": "9.2 On Distillation of Guided Diffusion Models Paper",
    "text": "9.2 On Distillation of Guided Diffusion Models Paper\n\n9.2.1 Classifier-free guided diffusion models (CFGD)\nClassifier-free guided diffusion models (CFGD) is a technique to control how strongly our output image matches our prompt. \nSay we want to create a photo of a cute puppy. We put “a cute puppy” into CLIP (its text_encoder) to get an embedding of the prompt. We then put this embedding into our unet with a pure noise latent to generate our puppy picture for us. This is normally how we do stable diffusion, but CFGD allows us to control how strongly this generated image matches our cute puppy prompt.\nHere’s how it works: Put an empty “” prompt into CLIP too to get another embedding. This embedding is particular because it represents nothing. If we did inference just on this, we would essentially be telling our unet “generate an image without any guidance, no restrictions, as long as it looks good”.\nInstead, we concatenate the ‘a cute puppy’ prompt embeddings with the blank “” prompt embeddings, and put them into our unet with a pure noisy latent. The unet outputs an image representing the real prompt, and another image representing the fake prompt. We combine these two images together. (I don’t understand why the unet produces 2 images, shouldn’t it just produce 1 image of them combined in the first place? That’s how the notebook code did it)\nTo recap, we just did the first step of inference, we took a noisy latent and removed a bit of noise to make it look more the embedding of the prompt “a cute puppy” concatenated with the embeddings of the prompt ““.\nWe then go onto doing inference as normal, removing noise step by step until we get our final image. The point of this is that, we can control how much the final image should rely on the real prompt versus the blank one. The latter is high guidance, the former is less. This allows us to control how strictly the real prompt is followed.\n\n\n9.2.2 The Paper\nThis way of doing CFGD is awkward because the unet has to output two images instead of the usual one, and for other reasons. The paper details a way to skip it. We do teacher student distillation again!\nOur normal stable diffusion with CFGD is the teacher model. It does CFGD with a number of different levels of guidance, 2, 4, 5, 12 etc. It does inference by starting with a noisy latent, then creating an image with guidance introduced. Our student model is another unet, unet B. Very similarly to how it worked in the previous paper, it looks at the different step outputs created by the teacher model, and learns how to do guidance like the teacher, but in a much more less awkward way.\nThere is an extra video on this, walking through the paper. It’s useful to watch this to learn how to read papers properly, as in, what’s important and such, in particular, the most important part is usually the algorithm."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#imagic-text-based-real-image-editing-with-diffusion-models",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#imagic-text-based-real-image-editing-with-diffusion-models",
    "title": "fast.ai Lesson 10",
    "section": "9.3 Imagic: Text-Based Real Image Editing with Diffusion Models",
    "text": "9.3 Imagic: Text-Based Real Image Editing with Diffusion Models\nThis is another paper that just came out 3 hours ago as of writing!\nGive it an input image, and pass in a prompt. It tries to edit the input image to match the prompt.\n\n\n\nimage.png\n\n\nThis paper is for imagen, a different image generation algorithm, but it also works fine for stable diffusion.\nGenerally speaking, it works through fine-tuning and optimising embeddings.\nWe start with a fully trained stable diffusion model.\nOur input image is\n\n\n\nimage.png\n\n\nWe want an image of it spreading its wings.\nUse CLIP to get an embedding of our prompt “a bird spreading its wings”. Use this embedding for inference as usual. This will create a photo of a bird spreading its wings, but not a photo of the bird from out input image spreading its wings.\nTo remedy this, we fine tune the prompt embedding to try and get it to create an image similar to the input bird image. We only do this a little and lock it, it cannot change any more. Now we have an optimised embedding.\nNow we fine tune the entire stable diffusion model to generate images that look like our bird from the input image. Now we have a fine-tuned diffusion model.\nFinally, we combine the optimised embedding and the original embedding for our target prompt, and pass it through the fine-tuned diffusion model to create our output image!\n\n\n\nimage.png\n\n\nMaybe it would only take like an hour to do this process using stable diffusion on GPUs!"
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#our-foundations-are",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#our-foundations-are",
    "title": "fast.ai Lesson 10",
    "section": "11.1 Our foundations are:",
    "text": "11.1 Our foundations are:\n\nPython\nMatplotlib\nThe Python Standard Libary\nJupyter Notebooks and nbdev\n\nTo be clear, after we have created a function, we’re allowed to use a module to do it. For example, we don’t start with Numpy arrays, but after we’ve made our own we can use them.\nThis is also how machine learning models will work. We cannot train full models on our own, so we will create and train small models, and then allow ourselves to use pretrained models online.\nThis is a challenge that will be different, but it’s often the part of the course where people learn the most. This year around it will be the best to date, and is more than worth doing.\nThe notebooks are from the part 2 repo on GitHub."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#useful-shortcuts-and-tools",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#useful-shortcuts-and-tools",
    "title": "fast.ai Lesson 10",
    "section": "11.2 Useful Shortcuts and Tools",
    "text": "11.2 Useful Shortcuts and Tools\n\nShift-m in Jupyter allows you to combine cells together.\nCntrl-shirt-minus allows you to separate them.\nAlt-enter inserts cells.\nRun a function then ? to get a brief description.\nRun a function then ?? to get documentation.\nInside a function, press shift-tab to see parameters available quickly."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#advice",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#advice",
    "title": "fast.ai Lesson 10",
    "section": "11.3 Advice",
    "text": "11.3 Advice\n\nRead the Python documentation a lot, it’s good.\nFor every single method, Jeremy reads the documentation and practices it."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#notes",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#notes",
    "title": "fast.ai Lesson 10",
    "section": "11.4 Notes",
    "text": "11.4 Notes\nI made notes on the lecture, then went through the notebook myself and rewrote the notes while running through and testing the cells."
  },
  {
    "objectID": "posts/2022-09-27-Lesson7Blog.md.html",
    "href": "posts/2022-09-27-Lesson7Blog.md.html",
    "title": "fast.ai Lesson 7",
    "section": "",
    "text": "7: Collaborative Filtering"
  },
  {
    "objectID": "posts/2022-09-27-Lesson7Blog.md.html#going-through-road-to-the-top-part-3",
    "href": "posts/2022-09-27-Lesson7Blog.md.html#going-through-road-to-the-top-part-3",
    "title": "fast.ai Lesson 7",
    "section": "4.1 Going through road to the top, part 3:",
    "text": "4.1 Going through road to the top, part 3:\n\nA larger model has more parameters so can find more features, but the problem is that it takes GPU memory that isn’t as flexible as CPU memory.\nHow to use as large a model as you like without worrying about memory. For example, Kaggle has 16Gb GPUs.\nYou can first find out how much memory a model uses. What’s important is that training for longer does not actually require more GPU memory.\nGradient accumulation is how: Run smaller batch sizes, but modify them as to act and train as if we were using the same normal batch size for all the training data.\nGradient accumulation results are identical to using a higher memory GPU for certain models. It is for convText and Transformers (NLP). If a model uses batch normalisation, then it won’t exactly, it will have different results, but probably still good ones.\nPick a batch size that fits your GPU memory, and generally higher and a multiple of 8 is better. Generally (not always) if you double batch size, half your learning rate.\nWe can use ensembles of good models of different architectures, and get even better results. Furthermore, we can add in bagging too to train them on different sets of the training data.\nAt the start it may feel random as to why certain approaches/models are better, but over time as you develop intuition, it will feel be less random and more systematic.\nGenerally, it makes sense to iterate on small models then switch to large models, but there’s a better way of ensuring this performance converts correctly. This is covered in the second course."
  },
  {
    "objectID": "posts/2022-09-27-Lesson7Blog.md.html#going-through-road-to-the-top-part-4",
    "href": "posts/2022-09-27-Lesson7Blog.md.html#going-through-road-to-the-top-part-4",
    "title": "fast.ai Lesson 7",
    "section": "4.2 Going through road to the top, part 4:",
    "text": "4.2 Going through road to the top, part 4:\n\nWe want a model to now predict two things instead of one I.E, two dependent variables instead of one. E.g. from a rice photo, the type of rice (10 types) and the disease it may have (10 types), so there are 20 categories.\nThis requires an understanding of making custom loss functions and a deeper look into how cross entropy loss works.\nMake a learner just for the first dependent variable, disease, and create a specific metric function for it.\nCross entropy loss: Jeremy states it is really important to understand and so goes into the maths using a separate excel sheet. He tries to predict if a image is a cat,dog,plane,fish or building, so there are 5 classification categories. It outputs 5 numbers (relating to probabilities of each category). CEL first finds the softmax value for each of them. Then it compares the actual value (1 for the correct category, 0 for not) to the softmax value for each category. It multiplies the log of the probability prediction for the correct category by the actual value.\nFurther info: https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/\nBinary cross entropy is just cross entropy for 1 category: is a cat or not. Careful here, it’s not for 2 categories e.g. cat or dog.\nThe loss functions in our python environment has two types. The F function type and the nn class type. The latter has more parameters to play with.\nChange the last node outputs to be the number of categories predicted instead of the usual 1 for classification.\nYou encode the loss function for the model to know what/how many categories to predict. You sum loss functions for multiple category types and their sub categories.\nThis new model, that can predict 20 categories, actually is better than a model that just predicts disease type! This is because the training to do other types of predictions helps. Sometimes this approach is better, sometimes not!"
  },
  {
    "objectID": "posts/2022-09-27-Lesson7Blog.md.html#collaborative-filtering-deep-dive",
    "href": "posts/2022-09-27-Lesson7Blog.md.html#collaborative-filtering-deep-dive",
    "title": "fast.ai Lesson 7",
    "section": "4.3 Collaborative Filtering Deep Dive:",
    "text": "4.3 Collaborative Filtering Deep Dive:\n\nCollaborative filtering is a key part of recommender systems.\nWe use the Movielens dataset of movie ratings with 3 columns: UserID, MovieID, and rating.\nImagine a matrix of the users and their movie ratings. There are missing values for unrated/unseen movies. Collaborative filtering is just trying to fill in these missing values to complete the matrix.\nThe problem is predicting how a user will rate an unrated movie for them. We want to match up the user’s movie preferences with the movie’s features to predict this.\nBut we don’t know their preferences and the movie features, these are called latent variables. We only have their ID, their previous ratings, and those of other users. We can however infer a user’s preferences and a movie’s features from this data.\nLet’s assume there are 5 latent factors, say like for a movie, it’s genre, length etc, we don’t set these, we calculate them and then can try and interpret what they are.\nOn choosing the number of latent factors, its hard. Fast.ai has a function to calculate this based on Jeremy’s intuition, but you can play around too.\nUse SGD to optimise these latent factors after we set a loss function.\nWhat is embedding? Just looking something up in an array. An embedding matrix is the array that is looked up. Matrix multiplication in an embedding matrix is the same as looking up index values in a list say as a function in excel. Think about a dot product with a one-hot encoded vector, it just returns the value you’re looking up.\nWe then cover how to create a collaborative filtering model from scratch using python, PyTorch class definition, and features.\nWe create a DotProduct class to define embeddings and looking up values for UserIDs and MovieIDs.\nSome of our user rating predictions can greater than 5, the maximum. Take our predictions and squish them with a sigmoid to fix this.\nWe noticed that some users just relates all movies highly, while some users have a range of ratings. Let’s incorporate this into our model predictions. To do so, we make another inference variable, a movie bias and a user bias, reflecting that for movies, they tend to be especially related well or badly, and that for some users, they can rate all movies generally as good or bad.\nIt’s not covered, but I think we could try and cluster users instead to try and incorporate user and movie types/preferences?\nWe can use L2 regularisation (weight decay), to avoid overfitting. This adds the sum of the square of the weights to the loss function. This also solves the issue of having useless interfered variables, because they won’t contribute. I suppose this makes getting the exact number of inference variables less important.\nIn fast.ai, usually defaults are good, but for tabular data, it’s hard to know good defaults, so it’s good to test yourself."
  },
  {
    "objectID": "posts/2022-09-27-Lesson8Blog.md.html",
    "href": "posts/2022-09-27-Lesson8Blog.md.html",
    "title": "fast.ai Lesson 8",
    "section": "",
    "text": "2 Lesson Overview\nThis is the last lesson of fast.ai part 1. As of writing, fast.ai part 2 starts next Tuesday, and I have already registered for it. I’m very excited for the second half of the course, and really thankful that things fell right into place with my Masters project so that I can dedicate the time and effort towards taking it seriously.\nThis lesson covers embedding matrices like the previous lesson. The emphasis on embedding matrices makes me believe they are quite important. CNNs are also covered, and again like basic neural networks, they end up fundamentally being pleasantly and surprisingly simple. Lastly Jeremy gives some advice about what to do before the second half of the course.\n\n\n3 The topics covered, briefly\n\nMore about embedding matrices: about inference parameters, PCA and embedding distances.\nA combined approach of a dot product model and a neural network is the best for collaborative filtering.\nTransferring the embedding matrix from a neural network to other models can give a nice performance boost.\nHow CNNs work for image classification: how convolutions are made with filter matrices, how these filter parameters are found with SGD, what stride is, max/avg pooling, how dropout can improve generalisation.\nChannels refer to both the input data, but also to the number of activations per grid after a convolution. This second definition is also how features and filters are defined.\nWhat padding is: to avoid losing data on edges.\nWhat stride is: stride 1 is making the conv layer the same grid size as the previous; stride 2 is skipping over.\nWhy we double channels/filters/features after a stride-2 conv layer.\nRefactoring is defining functions for your neural network layers to state its parameters explicitly. E.g. defining a function for convolution layers.\nWhat is a “receptive field”? The area of the image involved in the calculation of (convolutional) layers.\nVarious visualisation plots to understand how the training process is going.\nAbout learning rates: the benefits and drawbacks of both high and low learning rates. 1cycle and cyclical momentum as smart techniques to modify your learning rate dynamically.\nWhy activations near zero are problematic. Most obviously, having a final layer of just 0s has no information and so is useless for classification.\nBatch Normalisation as a technique to lessen the number of activations near zero.\n\n\n\n4 Lecture Notes\n\nJeremy is back to the previous collaborative filtering notebook:\nFor the embedding matrix, PyTorch keeps track of neural network parameters/weights for you.\nmovie_bias is the interference parameter we created in order to add bias for movies, user_bias is the one we created in order to add bias for users.\nHow does our model work? To predict, it trains as normal on the input ratings and output movie ratings, but then we add meaningful interference parameters, movie_bias and user_bias for each movie and user respectively, to adjust with domain specific knowledge. Some movies just everyone likes, so are biased, some users just like every movie, so are biased.\nVisualising embeddings: Plotting a principle component analysis of PCA component 1 and 2, gives a compressed view of how our latent factors affect eachother. It gives us a graph to interpret how the top latent factors are related to eachother. Domain specific knowledge would tell us what there PCA components/inference factors are, and we can try and understand why they relate the way they do.\nEmbedding distance: calculate how far apart each embedding vector is from a specific movie, aka how similar each movie is compared to that movie based on the latent factors.\nUsing deep learning for collaborative filtering instead: We use a neural net to try and enter the missing values in the matrix.\nWe use fast.ai to try and find the embedding matrix side/number of latent variables. Fast.ai does it based on Jeremy’s intuition, there’s not a easy way to know how many we should use, although weight decay might give some leeway in having too many.\nIn practice, a combined approach of a neural net and dot product approach is best for collaborative filtering.\nIn collaborative filtering, often a small number of users and movies overwealm the rest. For, anime some users watch so much anime compared to other users, and this makes predictions for normal users biased towards predicting for these enthusiastic users. It’s a higher level task to try and resolve this.\nEmbedding in NLP: an embedding matrix is just a matrix of latent variables for every word. And as before, the embedding distance calculates the embedding distance between a word and other words, telling us how similar they are based on latent variables.\nFor tabular data, using a tabular model/learner for it, creates and uses an embedding matrix.\nCollaborative filtering, NLP, tabular data neural nets, all also use embedding matrices.\nUsing embedding, latent variables, may be a substitute to doing a lot of feature engineering.\nCreate and train a neural net, take its embeddings and use it with other models like random forests, it can give a nice performance boost. This is akin to letting the neural network find relationships in the data, and using those relationships to boost another model’s performance.\nOne latent variable found to help predict retail store sales was distance in real life. Actually the embedding matrix’s distances for that latent variable matched the distances in real life! It managed to learn that real world distance was important for predicting retail sales and reflected that! Latent variables can find amazing relationships about the data!\nJeremy then goes on to cover how CNNs function:\nConvolutions: A CNN is similar to the neural networks before, but for computer vision they have a particular difference: they can separate out the horizontal and vertical lines of images into convolutions.\nHow? It has a filter matrix, say a 3x3 one, and moves around, dot producting and RELUing each 3x3 subset matrix of the original image, producing a smaller image. This is called a convolution.\nWhy does this work? Because of the values in the filter. E.g. we used a 3x3 matrix with the top elements = 1, middle elements = 0, bottom = -1. The dot product of this will only give the highest value when there’s horizontal image in the top row, anything in the middle, and no image in the bottom, aka whenever there’s a horizontal part of an image! In vertical parts, the bottom row will cancel out the top and produce 0s, aka the convolution layer will have nothing.\nThe 3x3 matrix is called a filter.\nWe them use another filter, but this time with two filter matrices, one for our vertical layer and one for our horizontal layer. We do this to combine their features.\nIn practice, if we wanted to use a CNN to find the best way to make convolutions to classify an image, we wouldn’t know what filter matrix values to use. Previously we just hard coded for a vertical and horizontal convolution, but we want the CNN to find more complex and useful convolutions.\nThe way we find these filter matrix/kernel values is to set them as parameters and use SGD to optimise. In our example, it will find the good kernel parameters and thus convolutions for digit classification.\nNowadays we do a stride convolution, e.g. stride 2, to skip values of the original image. This reduces the grid size by 2x2. The grid size is not reduced by the kernel size, it is reduced determined by the stride we set.\nWhen we’re done with stride convolutions, with about a 7x7 image at the end, we do an average pool: we average the final values (called activations).\nA max pool instead finds the max value of each activation rather than averaging them.\nSay we’re trying to identify a picture of a bear. If the bear is a small part of the picture, max pool is much better than avg pool for classification. By checking each end activation for a bear, rather than having one prediction of bear or not bear for all the activations combined, it can spot the bear’s small presence.\n\nDepending on how you want your model to work, you should pick max or avg pool accordingly. fast.ai does this for you, it actually does an average of max/avg pool and tries to find the best for you.\nAll the kernel multiplies done for a convolution layer is mathematically equivalent to just one big matrix multiplication.\nDropout: we can add a dropout mask which can improve generalisation.\nWe multiply the mask by our filtered image before we do convolutions to delete parts of it. Higher dropout means the image is harder to see as there is less of it.\nThe motivation is as follows: a human is able to look at a image with pieces missing and still classify it. A model should be able to as well. If we use dropout, perhaps it forces the model to learn more fundamental features about our images, more resilient and generalisable ones.\nThere are many more activation functions/activations than ReLU, but in practice it doesn’t make a huge difference so it’s not worth working on it too much.\nJeremy then advices a few things to do before the second half of the course:\nRead the book Meta Learning: How to Learn Deep Learning.\nWatch the videos again and code and experiment as you go.\nSpend time on the forums.\nGet together with others to study together.\nBuild projects.\n\n\n\n5 Questions\nWhat is a “feature”?\nA transformation of the data which is designed to make it easier to model. Feature engineering is just making new transformations. For example, in the titanic dataset, we could make a new feature, the number of family members a person has, and this could help the model’s predictions.\nWrite out the convolutional kernel matrix for a top edge detector.\n-pass-\nWrite out the mathematical operation applied by a 3×3 kernel to a single pixel in an image.\nI’m confused, a kernel doesn’t apply to a single pixel, a 3x3 kernel is applied to 9 pixels, albeit centered on one. The maths for this is just a dot product.\nWhat is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\n0\nWhat is “padding”?\nThe sides of the image will have the kernel try and use pixels out of bounds. We don’t want to simply lose these sides, so we apply padding, usually just 0 pixels so the kernel has values to use.\nWhat is “stride”?\nStride-1 is just simply applying the kernel to a centered pixel, then moving one pixel away and doing it again. Stride-2 is moving the centered pixel two pixels away every time. Stride-1 is useful to add convolutional layers without changing input size, noting that it’s the stride, not the kernel size, that affects the convolution layer’s dimensions. Stride-2 is useful to decrease the size of our outputs.\nCreate a nested list comprehension to complete any task that you choose.\nNested loops follow the syntax: print(((i for j in range(1,5)) for i in range(1,5))) which is just for j in range(1,5): for i in range (1,5): print(i) print((i for i in list1 for j in list2))\nWhat are the shapes of the input and weight parameters to PyTorch’s 2D convolution?\ninput:: input tensor of shape (minibatch, in_channels, iH, iW) weight:: filters of shape (out_channels, in_channels, kH, kW) iH and iW are just the height and width of the image. kH,kW are just the height and width of the kernel. in_channels and out_channels are just the number of input and output channels.\nWhat is a “channel”?\nA channel is a single basic colour in an image. RGB images have 3 channels, red green and blue. I imagine a 3 channel image like 3 sets of pixel maps, each for a channel, or one pixel map with 3 separate values we can operate on. However channels don’t just refer to the input data as described, but also to the number of activations per grid after a convolution. This second definition is also how features are defined.\nWhat is the relationship between a convolution and a matrix -multiplication?\nA convolution mathematically is just a special matrix of the kernel’s values, multiplied by the pixels, with a bias matrix term added. I.E. kM + b\nWhat is a “convolutional neural network”?\nFor a neural network, when we use convolutions instead or in addition to linear layers.\nWhat is the benefit of refactoring parts of your neural network definition?\nRefactoring is defining functions for your neural network layers to state its parameters explicitly. E.g. defining a function for convolution layers. Refactoring makes it much less likely you’ll accidentally make errors in your architecture and also makes it easier for your user to understand how your layers are constructed.\nWhat is Flatten? Where does it need to be included in the MNIST CNN? Why?\nThe final Conv2d layer has a output tensor shape of 64x2x1x1, but we need to remove the extra 1x1 layer to get 64x2x1, so we use flatten to do so. This is similar to squeeze in PyTorch.\nWhat does “NCHW” mean?\nOur input’ shape is 64x1x28x28: batch,channel,height,width. Meaning a batch of 64 one channel (colour) images of 28x28. NCHW refers to this.\nWhy does the third layer of the MNIST CNN have 77(1168-16) multiplications?\nThe third layer output has the shape 64x16x4x4, so has 16 channels, and so has one bias for each channel. The input shape from the previous layer is 64x8x7x7. For each input pixel (7x7), we multiply by the number of parameters minus the bias weights (1168-16).\nWhat is a “receptive field”?\nThe area of the image involved in the calculation of (convolutional) layers. For example, if we select a pixel on the second convolutional layer, we can see the pixels used to calculate it in the first layer, and then further still the pixels used to calculate those in the original image.\nWhat is the size of the receptive field of an activation after two stride 2 convolutions? Why?\nMathematically, if you apply a 3x3 kernel with 2 stride to a 7x7 area, then do it again, you get one pixel left.\nRun conv-example.xlsx yourself and experiment with trace precedents.\n-pass-\nHave a look at Jeremy or Sylvain’s list of recent Twitter “like”s, and see if you find any interesting resources or ideas there.\nJeremy retweeted a kaggle competition where the top teams and winner used fast.ai for their image classification!\nHow is a color image represented as a tensor?\nA rank 3 tensor. e.g. (3, 1000, 846), 3 colour channels, 1000x846 image size. You then can go into the 3 colour channels and print their individual pixel maps.\nHow does a convolution work with a color input?\nA convolution takes an image with a certain number of channels, and outputs an image with a different number of channels. For a channel 3 image, we have 3 different kernels, and apply them, then add the result together.\nWhat method can we use to see that data in DataLoaders?\ndls.show_batch\nWhy do we double the number of filters after each stride-2 conv?\nA stride-2 conv halves the grid size from 14x14 to 7x7. Say the original image was 14x14, then the convolution would be only 7x7. We double the number of filters to avoid this. Filters are just channels, also called features. So say we apply stride-2 to a 14x14 3 channel image, we then end up with a 7x7 grid size with 6 channels.\nWhy do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\nNeural networks can only create useful features if the number of outputs from them is lower than the number of inputs. It has to condense information to create useful features. That’s why we use a large kernel, because it keeps the number of outputs meaningfully lower than the number of inputs, particularly because if we double the number of filters each time we have a stride-2 layer, we’d be using nine pixels to calculate eight outputs.\nWhat information does ActivationStats save for each layer?\nIt records the mean, standard deviation, and history of the activations for each layer, so that we can look into our model’s training process and improve it.\nHow can we access a learner’s callback after training?\nlearn.activation_stats.plot_layer_stats(0) prints out useful plots of the first (0) layer for us.\nWhat are the three statistics plotted by plot_layer_stats? What does the x-axis represent?\nThe mean, std, and % of activations with a value near 0. x-axis is just the frequency (number) of activations.\nWhy are activations near zero problematic?\nWe don’t want activations to be 0 or near it. Multiplying by 0 gives 0, which means if a early layer has some 0 activations then latter values will too. Multiplying by 0 means we have computation occuring that doesn’t do anything. If our final layer is just 0s, then it’s not very useful to classification since it’s just empty.\nWhat are the upsides and downsides of training with a larger batch size?\nLarger batches have a more accurate gradient to update the loss with, although they also incur fewer batches per epoch, with means the model weights are updated less frequently. The latter can result in slower training, but it depends on your GPU and other factors.\nWhy should we avoid using a high learning rate at the start of training?\nBecause the initial weights were initialised randomly, a high starting learning rate could result in the training being ruined at the start very quickly by going in a very wrong direction. Also: a lower learning rate throughout would result in ending training at a local minima, while a high learning rate would skip over them.\nWhat is 1cycle training?\nTo avoid the previous problem, we want to start with a low learning rate, but when the training has settled some, we want to increase the learning rate to speed up the training, but near the end we want a low learning rate again to avoid accidentily skipping over loss minima.\nThis is where 1cycle training comes in: it splits the learning rate into a warmup and annealing phase. The former starts low and increases to a maximum, and the latter decreases from the maximum back to the lowest.\nWhat are the benefits of training with a high learning rate?\nA high learning rate trains quickly and can avoid being stuck in local loss minima.\nWhy do we want to use a low learning rate at the end of training?\nBecause we want to converge on a final loss minima, but a high learning rate would skip over it.\nWhat is “cyclical momentum”?\nMomentum is when the optimiser goes in the direction of the gradients as usual, but follows the direction of the previous loss updates.\nCyclical momentum is varying momentum in the opposite direction of the learning rate. When we are at high learning rates, use less momentum, when at low learning rates, use more momentum.\nWhat callback tracks hyperparameter values during training (along with other information)?\nplot_sched\nWhat does one column of pixels in the color_dim plot represent?\nEach vertical slice of the color_dim plot, the plot with the colours changing, represents the histogram of activations for a single batch.\nSo looking at colour_dim shows us how the activations in the CNN layers shows as training progresses. More specifically, as each vertical line represents the activation histogram for each batch, we see how the activations change after each batch.\nWhat does “bad training” look like in color_dim? Why?\nDark blue is bad training, at the start all activations are zero. Yellow is better, as there are near-zero activations instead of just zero. But we see over training it gets better as the colour changes so we have less zero activations! But our training is bad because it oscillates in colour, it doesn’t just get better.\nWhat trainable parameters does a batch normalization layer contain?\nFor our model to work, we need to fix the initial large zero/near-zero activations and maintain not so many during training. Batch normalisation is a solution to this, it has two parameters, gamma and beta to do so.\nWhat statistics are used to normalize in batch normalization during training? How about during validation?\nDuring training we normalise the training data with the mean and standard deviation. During validation we instead use the running mean of the stats calculated during training.\nWhy do models with batch normalization layers generalize better?\nWe’re not entirely sure yet but probably because each batch adds some more randomness to the training process. Each mini-batch will have a somewhat different mean and std from the others, so the activations will be normalised differently each time. Thus we force the model to cope with with variations and it improves generalisability.\n\n\n6 Links\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson8.html, which includes a lecture, notebooks, and a set of questions from the course book."
  },
  {
    "objectID": "posts/2022-10-06-MP3.html",
    "href": "posts/2022-10-06-MP3.html",
    "title": "Masters Project 11/10/2022",
    "section": "",
    "text": "The third post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/2022-10-06-MP3.html#during-the-meeting-we-discussed",
    "href": "posts/2022-10-06-MP3.html#during-the-meeting-we-discussed",
    "title": "Masters Project 11/10/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nHow to introduce more Physics into the project. This would be done namely using stereo audio, which is audio recorded in 3 dimensions. The idea is that using the Doppler effect and other Physics concepts, we could come up with a better audio collection technique that would make the model classification better. However, it may take a lot of time and effort to understand how to setup multiple microphones and analyse their data, given that there are features such as their synchronisation, batteries, and calibration. In total we have around 90 microphones? Because stereo is 3D, we could use it to classify where a bird singing is, and multiple birds singing. Would also need to account for audio reflecting off surfaces.\nA previous masters student at Durham, Stuart, worked with the biology department to create his thesis which may contain useful domain knowledge for this project. It can be found at http://etheses.dur.ac.uk/11481/. He is now doing a PhD in Scotland and his further work may be worth looking into, or he may be worth directly getting in contact with to collaborate.\nThe biology department have published a paper 3 years ago about using a ensemble models to classify the cocktail problem. While the models and approach may be outdated, the data preprocessing steps might be similar to what I would have to do now.\nI would have to have a look at the collected datasets to get an idea about their properties and train accordingly. Some of their data was painstakenly hand labeled.\nFor humans, some bird song is easier classified by image, and some by sound. I suppose this is why ensembles are a good approach.\nFor the aims of this project, I would need to define a custom metric for the biology department’s needs. For example, if we really want a certain species of bird, we could penalise false positives of it very harshly.\nAbout volunteer interaction: some birds are really hard to classify that even professors struggle. If you were to have a volunteer system, you should have a delegation system to assign hard tasks to experienced users.\nBecause humans have to listen to the same audio file multiple times to recognise birds, I wonder if we can train or apply a model this way.\nTraining on bad quality data, which xeno-canto labels, might be a good idea to improve generalisability.\nSome bird species have different times at which they sing in the day, so it could be worthwhile including this in the training data somehow.\nAn interesting problem is say, looking at how the frequency of a rare species changes over time in a given area. Since we have a dataset collected over 3 years, this in theory would be possible."
  },
  {
    "objectID": "posts/2022-10-06-MP3.html#there-are-two-separate-and-related-problems-for-classifying-birdsong",
    "href": "posts/2022-10-06-MP3.html#there-are-two-separate-and-related-problems-for-classifying-birdsong",
    "title": "Masters Project 11/10/2022",
    "section": "3.1 There are two separate and related problems for classifying birdsong:",
    "text": "3.1 There are two separate and related problems for classifying birdsong:\n\nThe easier problem, trying to classify birds from an audio recording of their song in a vacuum.\nThe harder problem, trying to classify birds from an audio recording of their song in a noisy environment.\n\nMy Masters project is attempting the latter problem. However some testing and knowledge of the former problem is worthwhile regardless."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html",
    "href": "posts/2022-10-11-L9Blog.md.html",
    "title": "fast.ai Lesson 9",
    "section": "",
    "text": "9: Introduction to Stable Diffusion"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#part-1-overview",
    "href": "posts/2022-10-11-L9Blog.md.html#part-1-overview",
    "title": "fast.ai Lesson 9",
    "section": "4.1 Part 1, Overview:",
    "text": "4.1 Part 1, Overview:\n\nGPU needs have increased for this course. Funnily enough, Collab prices have skyrocked as everyone is using it for SD. There are four options: Colab, Paperspace Gradient, Lambda Labs, Jarvis Labs. Lambda GPU is the cheapest with an offer:$1.10/hour. Lambda is cheaper than everything else but it nor collab allow you to pause.\nAn issue is, if we use paperspace or lambda, the pipeline we downloaded will be saved, and sometimes it can take hours to download, but if we use collab then it won’t.\nTraining GPUs need 16/24 gb memory. My laptop is only 12gb, and it’s probably not simple to optimise it for ML. I might be able to do everything that isn’t training on my own PC, but I need a web service.\nStable diffusion is moving so quickly, this lecture is already outdated. Literally in the last 24hrs before the lecture, 2 crazy new papers came out.\nThe first paper reduced the number of steps required for SD to 250 from 1000!\nThe second paper changed SD to make it 10x-20x faster!\nI wonder if this is a result directly from SD’s open source nature. The research community may well be able to improve SD much faster then say OpenAI can improve DALL-E.\nBut after this first lesson, we go into the foundations which won’t change often, and we’d be able to understand the updated research’s details ourselves.\nWe can fine-tune SD using DreamBooth, to put any object or person into an image! strmr.com is a service to do so, which costs $3.\nJeremy links the Diffusion-nbs notebook and repo. It contains many things to play with. There’s notebooks with tons of parameters we don’t yet understand. I think this is the philosophy that, to understand a black box we should first get intuition for it’s inputs and outputs, then kneed into the details.\nIt’s not actually easy to know what prompts to give SD. The best way to learn is to look at other people’s prompts and outputs. lexica.art gives plenty examples."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#part-2-notebook",
    "href": "posts/2022-10-11-L9Blog.md.html#part-2-notebook",
    "title": "fast.ai Lesson 9",
    "section": "4.2 Part 2, Notebook:",
    "text": "4.2 Part 2, Notebook:\n\nFirst start by cloning the Stable Diffusion notebook.\nDiffusers is the HuggingFace module for this. HuggingFace has had really good packages for a while now, so using them is a good idea generally.\nWe use the SD pipeline. A pipeline is similar to a fast.ai learner. It does many things automatically for us. We can also save pipelines like we save learners.\nUnlike leaners, we can save pipelines and upload them into HuggingFace’s cloud server. Like this, we can upload our own piplines and share them, or browse others’ and download them.\nIf we use the same pipeline with the same random seed, we get the same result. This is just like MineCraft world seeds.\nGenerally, diffusion works as follows: we start with random noise, and each step we get slightly less noisy and towards the result we want. Through many steps, E.g, 50, we get our image. Although after yesterday’s research it’s now only 3-4 steps!\nBut why don’t we just do it in one step? Our models aren’t smart enough, but considering we started with 1000 steps and now it’s 3-4, maybe at some point they will be.\nWe took the exact same prompt, four times, and pass them into our pipeline.\nThe pipeline has a variable, guidance_scale, g, meaning intuitively: ‘to what degree should we focus on the caption versus just creating the image’.\nToo low guidance won’t make images of the prompt, too high could be strange depending on the implementation of SD.\nIt works by making an image without guidance, then image with guidance, and finding the average of them. ?\nNegative prompts work similarly. Make two images, and subtract one from another. Say we want to remove blue from the image. Subtract an image from the prompt ‘blue’ from it.\nInstead of passing prompts in SD, we can pass images!\nTo do so, we use image to image pipelines. Instead of starting diffusion with random noise from scratch, it starts with a noisy version of the input image, using it as a guiding point. It uses a parameter, strength, which is similar to guidance I think, to exert how strongly SD should follow the input image image. This creates a variant of our original image as we’d like.\nWhat if we did this multiple times? Let’s take an image and pass it into SD, then pass the resulting image into SD again! We can use this to change styles of images for example!\nFine-tuning: We take a pretrained model (from a pipeline), cut off the head, and fine tune it with our own images and captions. This will let us create images of things the pretrained model hasn’t encountered, like pictures of our friends. Lambdalabs have a blog post guide for it.\nTextual inversion is a special type of fine-tuning. Create a new model embedding for some concept in some images we have. E.g. ‘watercolour’, and add that token to the text model, and train embeddings for it using our images. Then we can have use the prompt “Woman in the style of ” to create it!\nDreambooth is similar to textual inversion. Instead of making a new embedding, it finds a existing token in the embeddings that is barely used, and fine tunes it.\nNot sure why/if Textual inversion or Dreambooth is better.\nTextual inversion example. Say we have images of a teddy bear and we want to create a novel image of it riding a horse. Our model couldn’t end up generating it. It was however able to generate a picture of our teddy sitting on a carpet though. The point is, sometimes textual inversion can fail."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#part-3-conceptual-overview",
    "href": "posts/2022-10-11-L9Blog.md.html#part-3-conceptual-overview",
    "title": "fast.ai Lesson 9",
    "section": "4.3 Part 3, Conceptual Overview:",
    "text": "4.3 Part 3, Conceptual Overview:\n\n4.3.1 SD Model:\n\nImagine that we wanted to generate hand written digits using SD. Start by assuming there’s some function,f, that takes an image,X1, and returns the probability that it’s a handwritten digit, P(X1).\nWe get various probabilities returned from f. Say 0.98, 0.4, 0.02.\nWe can use f to generate handwritten digits! If you have a function that can classify/match images to labels/vectors, you can create a model to do the inverse, create a new image to match the labels/vectors.\nImagine we had a mess of an image that is supposed to be an handwritten digit. It’s 28x28, 784 px. We pick one of these pixels, and make it a little bit lighter or darker. We then pass it into f and see how the probability changes. If it increases, then we’ve gotten something that’s more like a handwritten digit. If we do this enough times for every pixel, then we could create an image of a digit, even from starting from pure noise!\nWe should optimize this process using gradients.\nStart by finding the gradient of the probability that our input image, X, is a digit, w.r.t the gradient of X’s pixels. The gradient represents, ‘how much does the probability X is a digit increase as we increase the pixels’ values’. Our gradient will have 784 values, one for each pixel.\nThen we change the image pixels using this gradient. Take every pixel and subtract it by it’s own gradient w.r.t the probability, multiplied by a constant (like a learning rate), C. In maths: X + C*(Gradient)\nThis method, finding the derivative of every pixel, so finding 784 derivatives, is called the finite differencing method of differencing.\nBut we need not do this, it’s slow and computationally expense. Instead, simply use f.backward() in Python to use analytics derivatives to do it super quick. X.grad from f.backward() gives us all 784 gradients.\nThe we simply repeat X + C*(Gradient) enough times to get our result: turning a noise image into a handwritten digit.\n\n\n\n4.3.2 The Probability Function, f\n\nBut how do we get our probability function f?\nUsing a neural network, of course!\nWe first get training data, that is, images of handwritten digits with a random amount of noise added to them.\nWe then train a neural network to try and predict how much noise is in a training image, I.E, what part of the image is noise and what part is not.\nA way to think about it is sketched below, we input the first image, and our network tries to predict the third. For the first example, the digit 9 has no noise. 7 has some noise. 3 has much noise, 6 further so.\n\n\n\n\nimage.png\n\n\n\nWe use can use MSE as our loss function for this network.\nWe can calculate it by adding the LHS together (remember that we have the non-noisy input images), and seeing how it compares to the input noisy image.\nBut wait, this process can actually just be used to generate an image for us instead of using a probability function f. \n\n\n\n4.3.3 Unets\n\nWe’ll call use a unet for this network from now on and call it that.\nIf we pass an image of pure noise into our unet, it returns info about what parts of an image it thinks is the noise for a handwritten digit image. Like, ‘if we left behind these pixels that aren’t noise, it would look a little bit more than a handwritten digit’.\nIf we did this multiple times, we could turn pure noise into a handwritten digit!\nNoise itself is defined such that, if we subtract the noisy input image by the noise, we will get an image without/with less noise.\n\n\n\n4.3.4 Compression\n\nThe issue is that we have too many pixels to do this practically.\nFor example, a standard 512x512x3 image is 786432 px! And if we had millions of images, this is impossible unless you’re Google!\nBut we can do this more efficiently. Storing the exact pixel values isn’t the most efficient way to store them. For instance, if we had a line of just blue pixels, why not store just info saying that. Jpg files are an example of compressing images.\nLet’s compress our image in a particular way. Put the image through a convolution layer of stride 2, and it’ll result in an 256x256x6 image. Then again, 128x128x12, then again, 64x64x24.\nNow a few resnet blocks to get 64x64x4. The number of pixels left is 16384, which is 48x less than the original!\nThis is pointless if we can’t reconstruct our full sized image well.\nWe can go backwards! Take our 64x64x4 image into a inverse convolution, giving us 128x128x12, then again and again back to 512x512x3, our original image.\nThis entire thing, both the compress (encoder) and decompress (decoder) is one neural network called an autoencoder, or in this case, a VAE.\nThe reconstruction part, the decoder, will give us random noise back, so we need a loss function to optimise it.\nOur loss function is just a comparison between the input image and output image using MSE. If reconstruction is done 100% correctly, then the MSE is just 0.\nWe can save decoder separately, and share it with others. This lets us share full images of things using only the compressed image. They don’t need the encoder.\nBut wait, if the compressed image had all the information needed to reconstruct the original, then there’s no point using the original massive image for training! The compressed version has all the information we need!"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#so-to-train-our-unet",
    "href": "posts/2022-10-11-L9Blog.md.html#so-to-train-our-unet",
    "title": "fast.ai Lesson 9",
    "section": "4.4 So, to train our unet:",
    "text": "4.4 So, to train our unet:\n\nWe put all the training images (which we’ve added noise to) through the autoencoder to compress them,\nWe then train the unet on these compressed noisy images.\n\nThe compressed images are called latents. The output of the unet is now the noise present in the latents. To get back to our original images, we subtract the noise from the latents, then use the decoder to get them back into full size.\n\n4.4.1 Guidance\nWith this new unet, VAE combination in mind, we can produce handwritten digits sure, but how do we get it to produce images based on prompts?"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#firstly-lets-understand-how-to-add-guidance-extra-information-to-the-unet-to-improve-it-using-our-handwritten-digit-example.",
    "href": "posts/2022-10-11-L9Blog.md.html#firstly-lets-understand-how-to-add-guidance-extra-information-to-the-unet-to-improve-it-using-our-handwritten-digit-example.",
    "title": "fast.ai Lesson 9",
    "section": "4.5 Firstly let’s understand how to add guidance (extra information) to the unet to improve it using our handwritten digit example.",
    "text": "4.5 Firstly let’s understand how to add guidance (extra information) to the unet to improve it using our handwritten digit example.\n\nPreviously we just inputted noisy handwritten digits as inputs, and made the unet find the noise.\nBut now, in addition to giving just noisy handwritten digits as inputs, we can also give something like labels to each input image. E.g, inputting a noisy 3 digit and a label of 3.\nIt’s not exactly a label we give, we give a onehot encoded vector that specifies what digit it is. I think the way we create this vector is arbitrary. If we kept the labels consistent for all the training data then it shouldn’t matter? E.g. the number eight label is just a vector thats (0,0,0,0,0,0,0,1,0,0).\nNow it will learn both how to predict the noise in an image, but what the image is supposed to be. It should be better at predicting noise now it has this extra info (called guidance).\nFor example, input a noisy 3 digit and a label of 3, the unet will ‘think’, ‘the noise is everything that doesn’t represent the number 3’."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#with-that-in-mind-we-can-understand-how-to-produce-a-picture-based-on-a-prompt.",
    "href": "posts/2022-10-11-L9Blog.md.html#with-that-in-mind-we-can-understand-how-to-produce-a-picture-based-on-a-prompt.",
    "title": "fast.ai Lesson 9",
    "section": "4.6 With that in mind, we can understand how to produce a picture based on a prompt.",
    "text": "4.6 With that in mind, we can understand how to produce a picture based on a prompt.\n\nSay we wanted to create a picture of ‘a cute teddy bear’.\nUnlike handwritten digits, it’s not easy to know how to construct the guidance/labels to add to each noisy input image.\nThis is because, unlike handwritten digits where there’s a small finite amount, we couldn’t possibly encode every possible input for every possible word in English. You could technically make ‘a cute teddy bear’ into a one-hot vector, but it would be impossible to make and store all possible prompts into one-hot vectors.\nSo instead, we have to create yet another model to assist us!\n\n\n4.6.1 CLIP\n\nOur new model takes a prompt, e.g. ‘a cute teddy bear’, and outputs some vector that represents that prompt.\nIf we had a good vectors to represent our prompts, it’s like generating handwritten digits, we have a good noisy image label so our unet can generate images for us.\nTo create our new model, we can look Online. For accessibility, people have already captioned (created prompts) of images.\nSay we find an image online captioned ‘a graceful swan’.\nWe have two encoders, a textencoder, that takes a caption as input and generates a string (a UTF-8 encoding) of it, and a imageencoder, which takes an image as input and generates a string.\nThe details behind what these strings are will be passed for now.\nFor ‘a graceful swan’:\nTextencoder takes ‘a graceful swan’ as input and creates it’s own embeddings/features to create its string.\nImageencoder takes the image of it as input and creates it’s own embeddings/features to create its string.\nWe want the embeddings created for ‘a graceful swan’ from these two encoders to be the same. This signifies that both encoders are producing and using similar features/embeddings to encode image and text.\nBecause the embeddings are vectors, we can dot product them. We want their dot product to be really big, because that signifies that they are similar.\nWith this, we can create an embedding matrix, for all the training images, 4 in this example:\n\n\n\n\nimage.png\n\n\n\nWe not only want matching prompts and images to have a high dot product, but non matching images to have a low dot product.\nThe latter is because, say for our ‘a graceful swan’ prompt, our textencoder will have features for it, and a non-matching image say the one of Jeremy, will have the imageencoder have features of his image. These features shouldn’t match, because they are representing different things.\nIn short, the diagonals should be high and non diagonal should be low.\nWe can then create a loss function for this! It’s simply: add up all the diagonal and subtract all the non diagonal.\nIf we do this, we will end up with a really good textencoder, that can take prompts, and produce good labels/vectors to represent it.\nOur setup is called multimodel because we have >1 model.\nThis pair in particular is called CLIP.\nThe loss function described is called in the embedding matrix is called contrastive loss.\nFinally, we can now take our ‘a cute teddy’ prompt, put it into textencoder, get features representing it, and use them to act as a label/vector!"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#to-summarise",
    "href": "posts/2022-10-11-L9Blog.md.html#to-summarise",
    "title": "fast.ai Lesson 9",
    "section": "4.7 To summarise:",
    "text": "4.7 To summarise:\n\nCLIP takes prompts as input and produces embeddings for them. Prompts with similar meanings gives us similar embeddings. We use Clip’s embeddings as guidance, a sort of label to noisy input images into our unet, which is pivotal for it to be able to generate images.\n\n\n4.7.1 Overview of the 4 models:\n\nUnets take noisy latents (compressed images) in and outputs their noise. We can subtract the two to get the latents back.\nVAE’s decoder takes latents and decompresses them back into full original image size.\nCLIP Text Encoder takes prompts and produces embeddings. We use these embeddings as guidance to train the unet.\nSD is the process of starting with noise and producing a full image. It works by starting with pure noise, then using a unet to identify noise (defined as pixels that aren’t our target) and subtracting it repeatedly.\n\n\n\n4.7.2 Noise in Training Data\n\nRecall, that there must be noise in our unet input images.\nThe language involved is weird, but just ignore it.\nThere’s a concept called “time steps”. It has nothing to do with time. It’s about have varying levels of noise in the unet input images.\nWe can create a noising ‘schedule’. E.g, We randomly pick a number of 1 to 1000, Say 4, and we lookup how much noise to use if we happen to get it following this graph:\n\n\n\n\nimage.png\n\n\n\nThe timestep 1000 has no noise.\nThis entire concept is just a way to pick how much random noise to add to images to use them for unet training.\nThis t plotted above is called a timestep, and again, it has nothing to do with time.\nThe y axis is sigma, or B, and all it means is the amount of noise.\nFor every minibatch image, we randomly pick a timestep to pick an random amount of noise, and then train with it."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#steps",
    "href": "posts/2022-10-11-L9Blog.md.html#steps",
    "title": "fast.ai Lesson 9",
    "section": "4.8 Steps",
    "text": "4.8 Steps\nRecall in the lecture, when we tried to create an image from a prompt in one step, it didn’t look good:\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#steps-1",
    "href": "posts/2022-10-11-L9Blog.md.html#steps-1",
    "title": "fast.ai Lesson 9",
    "section": "4.9 Steps",
    "text": "4.9 Steps\n\nThis occurred because having only one step means we subtracted all the noise in one go.\nDoing it in multiple steps resolves the issue.\nBut then we run into the issue: what constant should we use to dictate how much noise we should subtract per step? How should we add noise? These are research questions and the properties of a diffusion sampler.\nBut think about the noise subtraction equation, P = P - C * N, the picture is the picture - a constant multiplied by the noise.\nIt looks a lot like a deep learning optimiser! The constant is just a learning rate. Why not use LR concepts like momentum! Or Adam!\nThe thing is, diffusion models’ unets’ don’t just train on an input image and guidance, they take timesteps t as a variable too too.\nThe idea is that the unet will get better at removing noise if it has the extra info of how much noise we’re adding (t).\nJeremy doubts this is needed because neural nets can easily solve the noise problem.\nIf we remove t, then the differential equations approach is much simpler and becomes an optimiser problem we’re already famimilar with!\nFor unets also, MSE is usually used in ML because it’s easy. But what if we use more sophisticated loss functions. Like perceptual loss.\nThere’s a lot of novel research in this area, particularly in abandoning t and thinking of everything as an optimiser."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html",
    "href": "posts/2022-10-13-MP4.html",
    "title": "Masters Project 18/10/2022",
    "section": "",
    "text": "The fourth post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#during-the-meeting-we-discussed",
    "href": "posts/2022-10-13-MP4.html#during-the-meeting-we-discussed",
    "title": "Masters Project 18/10/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nTrying to compete directly with Google or other big research teams isn’t a good direction for a one year project. What would be better is a novel direction of something else.\nOne such approach would be trying to use stero data instead of mono data to solve the cocktail problem.\nMost audio data online is mono, for example on xenocanto, but we could try and get in contact with research groups like the biology team and ask for stero data.\nOne of the reasons why stero data is rarer, is because it requires a special microphone to collect it with.\nDifferent stero microphones have different properties, such as the width between the the two microphones, and this needs to be accounted for when gathering and analysing data.\nWe could look at stero data, and then calculate the difference between the first channel and second channel to make a third channel. Then try classifying with one channel, and with all three, and seeing if it helps.\nWhat would be interesting only possible with stero data is trying to find the direction of the birds singing. But this might be impossible, because getting labeled data of that is difficult.\nThe differences between mono and stero data. Besides having 2 channels, there are differences in time delay and attenuation. I need to look into this more.\nInvestigating whether it’s possible to convert between mono and stero data. It might be impossible to do exactly because there is information missing within the mono data, particularly because intuitively, stero data you can find direction from but mono you cannot. This is an information problem.\nStuart might order a stero microphone to play around with; Robert is asking whether it is possible to borrow one.\nAnother novel approach would be trying to use stable diffusion to generate spectrograms. The idea being, if there is a lack of stero data we could synthesise our own. There could even be another model added to correct synthesised audio data to be more like real audio data.\nA motivation behind this could be the prevalence of an image classification approach in classifying birdsong. Much research uses CNNs for example.\nAbout the Physics content of the project. There needs to be some Physics for the sake of the external marker and external questions at the viva.\nPhysics content can be added by investigating how to do the image to audio conversion (or vise versa in the case of stable diffusion), because of the transformations and information problem involved, or the prevalence of linear algebra/maths being involved, and even just in the Physics way of thinking of testing hypothesis and different approaches."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#machine-learning-skills",
    "href": "posts/2022-10-13-MP4.html#machine-learning-skills",
    "title": "Masters Project 18/10/2022",
    "section": "4.1 Machine Learning Skills",
    "text": "4.1 Machine Learning Skills\nUsing frameworks like fast.ai and transformers isn’t as simple as just using their predefined functions and models to do everything. Learning how to find the best hyperparameters, and good validation sets, among many other things, takes a combination of theory and practice to gain intuition. Jeremy from fast.ai said there is no substitute for practice, and provides a lot of guidance on how to do so."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#machine-learning-theory",
    "href": "posts/2022-10-13-MP4.html#machine-learning-theory",
    "title": "Masters Project 18/10/2022",
    "section": "4.2 Machine Learning Theory",
    "text": "4.2 Machine Learning Theory\nAs well as using frameworks and models, you have to spend time learning the theory behind how they work. For instance, how the components in a CNN work the way they do."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#framework-skills",
    "href": "posts/2022-10-13-MP4.html#framework-skills",
    "title": "Masters Project 18/10/2022",
    "section": "4.3 Framework Skills",
    "text": "4.3 Framework Skills\nUnderstanding the theory, and then having novel ideas to approach the cocktail problem, I need to then implement these ideas by knowing how to create the new code to do so.  This involves learning how to edit frameworks and create your own, covered in fast.ai part 2. \nThere’s also learning about https://nbdev.fast.ai/ to create frameworks and their documentation."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#data-handlingpreprocessingphysics",
    "href": "posts/2022-10-13-MP4.html#data-handlingpreprocessingphysics",
    "title": "Masters Project 18/10/2022",
    "section": "4.4 Data handling/preprocessing/Physics",
    "text": "4.4 Data handling/preprocessing/Physics\nLearning how to store data, access it, transform it into the right size and format, edit it, add noise to it, interpret it (bird domain information) etc.  There could be much work to be done on transforming the audio data. Fourier and Gabor transformers etc. I found a YouTube playlist of guides on this at https://www.youtube.com/watch?v=RMfeYitdO-c. The fourth initial project reference, “New aspects in birdsong recognition utilizing the gabor transform”, focuses on the gabor transform and likely much Physics too."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#custom-metrics-creation-and-evaluation-for-models",
    "href": "posts/2022-10-13-MP4.html#custom-metrics-creation-and-evaluation-for-models",
    "title": "Masters Project 18/10/2022",
    "section": "4.5 Custom metrics, creation, and evaluation for models",
    "text": "4.5 Custom metrics, creation, and evaluation for models\nThe biology department have their own interests and goals of what they want from a model. I would need to talk in detail with them about their priorities, e.g. preferences in confusion matrix metrics, in bird species etc. They might want a model to work with data over a few years to spot trends too."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#machine-learning-explainability-and-communication",
    "href": "posts/2022-10-13-MP4.html#machine-learning-explainability-and-communication",
    "title": "Masters Project 18/10/2022",
    "section": "4.6 Machine learning explainability and communication",
    "text": "4.6 Machine learning explainability and communication\nLearning how to implement and create methods and visualisations to communicate why the models are predicting as they do. This is especially important for marking in the final report."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#machine-learning-maths.",
    "href": "posts/2022-10-13-MP4.html#machine-learning-maths.",
    "title": "Masters Project 18/10/2022",
    "section": "4.7 Machine learning maths.",
    "text": "4.7 Machine learning maths.\nTo read and implement the latest machine learning papers, some mathematical knowledge is needed. I am contemplating doing yet another free fast.ai course, Computational Linear Algebra, explained here https://www.fast.ai/posts/2017-07-17-num-lin-alg.html, to help with the maths side of things. \nAlternatively or in addition, the book Deep Learning by Ian Goodfellow provides a mathematical backing and Jeremy recommended reading the first 6 chapters of it to help with understanding and implementing maths in papers."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#practiced-transformers",
    "href": "posts/2022-10-13-MP4.html#practiced-transformers",
    "title": "Masters Project 18/10/2022",
    "section": "5.1 Practiced Transformers",
    "text": "5.1 Practiced Transformers\nA list of transformer tasks is at https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb which is quite useful. \n\n5.1.1 In particular for audio classification it details the process: \n\nInstantiate a feature extractor and a model from the checkpoint name.\nProcess the audio signal to be classified with a feature extractor.\nPass the input through the model and take the argmax to retrieve the most likely class.\nConvert the class id to a class name with id2label to return an interpretable result.\n\nI went through the HuggingFace transformers documentation and did some of the notebooks to understand them. - https://www.kaggle.com/adnanjinnah/audio-classification-hf-1/ - https://www.kaggle.com/adnanjinnah/audio-classification-hf-2/ - https://www.kaggle.com/adnanjinnah/audio-classification-hf-3/ and they covered the 4 step process detailed above."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#practiced-trying-to-attempt-birdclef-2022",
    "href": "posts/2022-10-13-MP4.html#practiced-trying-to-attempt-birdclef-2022",
    "title": "Masters Project 18/10/2022",
    "section": "5.2 Practiced Trying to attempt BirdCLEF 2022",
    "text": "5.2 Practiced Trying to attempt BirdCLEF 2022\nIt’s well worth practicing attempts for a competition with the goal exactly as my own. After trying fast.ai’s audio module last week, and thinking it is outdated (the GitHub repo hasn’t been updated in roughly 6 months), I decided to use HuggingFace instead. This is mainly due to Jeremy recommending it as an up to date framework, but also because it is used in fast.ai part 2.\nWith that in mind, I attempted it at https://www.kaggle.com/adnanjinnah/birdclef-first-attempt/. This attempt was writhe with problems. While it was my first time using HuggingFace audio, the number of problems I encountered and issues involved were too much. I did not manage to get any model to work. I spent the entire time just trying to get the data loaded properly for usage.\n\n5.2.1 To summarise:\n\nHuggingFace’s load_model has several different methods to load audio. They all require the data to be formatted in a particular way. I tried all them with no success.\nKaggle’s competition dataset is set to read only for some reason. This makes it so I cannot directly just edit the files to get them right.\nI tired simply downloading the dataset and reuploading it to Kaggle but A. this is inefficient and B. won’t work for the unseen test data.\nI tried copying over the dataset from the read-only input folder to the editable output folder, but this is also inefficient and even so:\nI couldn’t load the copied data using load_dataset’s audiofolder function. I’m not sure why. I have it formatted in the exact way the documentation shows. The issue may be I need to upload the dataset to HuggingFace’s website first, but this has the same issues as the first attempt.\nA way to get around having to copy the data, with is also inefficient but would atleast work with the unseen test data is to tell load_dataset the URLs of the audio files. This didn’t work either, because some of the URLs don’t work in the instant load_dataset wants to access them. I couldn’t find a way to tell load_dataset to ignore or look later at these URLs.\nI tried using a different method of load_dataset, this one however seems to require the main .csv file to contain the audio files in array format. Because the .csv file contains a path to the audio files instead of their content, I tried using another module, librosa, to create a column in the .csv file containing the audio. This didn’t work, because of an excess memory error. And also, this is very inefficient.\n\nAfter extensively trying all methods I could find in the documentation with little success, this entire process took around 10 hours. I found tutorials to help with no luck. For now, I’ve given up on trying to get it to work myself. I need to find some resource online or in person to help. In hindsight, I probably should have done this earlier.\n\n\n5.2.2 On the bright side, atleast I learn’t a few things from the struggle:\n\nFirst, how transformers requires a dataset to be formatted in a specific way, and that HuggingFace has a website dedicated to storing datasets in an already formatted way.\nExperience in reading through documentation and troubleshooting.\nThe fact that sometimes URLs don’t work, and that last week’s code had a solution, but I couldn’t implement it into HuggingFace’s load_model.\nThat different loading methods require paths to audio files or them on the .csv file.\nThat audio files are stored as a file such as .ogg or as an array.\nHow librosa is a module to convert audio files into audio files into said arrays.\nThat memory errors will occur from trying to do too much at once. I could get my last method to work if I figured out a way to split up the data, but regardless this approach is inefficient considering we already have the files so it’s better to find a different method.\nHow to use os to copy files and folders over, or search and retrieve their file paths.\nThe fact that, for some datasets like BirdCLEF, there is a metadata.csv file with a column for the paths of the audio files.\nThat for advanced dataset formatting, for HuggingFace, you can create a .py script to do things exactly as you want."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#finished-fast.ai-lesson-9",
    "href": "posts/2022-10-13-MP4.html#finished-fast.ai-lesson-9",
    "title": "Masters Project 18/10/2022",
    "section": "5.3 Finished fast.ai lesson 9:",
    "text": "5.3 Finished fast.ai lesson 9:\nThis lesson was the first of fast.ai part 2 and a very well taught one. In it, Jeremy described conceptually how stable diffusion, an crazy new image generation model, works. Due to it’s difficulty, the lesson took me a full day to complete, but it was well worth it. The ideas and skills I’m being introduced to and learning will prove really helpful for the project going forwards. Next week, the lesson will focus on programming stable diffusion from scratch, and building on that, how to programme your own custom Python machine learning libraries. This is vital because it would allow me not just to copy other people’s code to solve the cocktail problem, but implement my own ideas and test things, perhaps even at a research level.\nMy post for lesson 9 can be found at: https://exiomius.quarto.pub/blog/posts/2022-10-11-L9Blog.md.html"
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#finished-cla-lesson-1",
    "href": "posts/2022-10-13-MP4.html#finished-cla-lesson-1",
    "title": "Masters Project 18/10/2022",
    "section": "5.4 Finished CLA lesson 1:",
    "text": "5.4 Finished CLA lesson 1:\nComputational Linear Algebra is a fast.ai course covering linear algebra to be centered around practical applications and algorithms.  More info and lesson 1 blog can be found here: https://exiomius.quarto.pub/blog/posts/2022-10-17-CLA1.md.html"
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#useful-datasets-found",
    "href": "posts/2022-10-13-MP4.html#useful-datasets-found",
    "title": "Masters Project 18/10/2022",
    "section": "5.5 Useful Datasets Found",
    "text": "5.5 Useful Datasets Found\n\nBirdCLEF 2022 uses data from xeno-carto, implying that last week’s approach to downloading them is a good idea.\nI found ESC-50, a dataset of labeled environmental audio recordings at https://dagshub.com/kinkusuma/esc50-dataset, also at https://huggingface.co/datasets/ashraq/esc50. These include sounds like rain, sea waves, animals.\nI found that Machine Listening Lab at Queen Mary’s University run a birdsong competition and have many datasets that I could possibly use at http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#useful-research-tools",
    "href": "posts/2022-10-13-MP4.html#useful-research-tools",
    "title": "Masters Project 18/10/2022",
    "section": "5.6 Useful Research Tools",
    "text": "5.6 Useful Research Tools\n\nScholarcy summarises research articles.\nhttps://inciteful.xyz/ is good for finding papers.\nI was told that Prostudy is useful for keeping resources stored for a dissertation."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#a-similar-thesis",
    "href": "posts/2022-10-13-MP4.html#a-similar-thesis",
    "title": "Masters Project 18/10/2022",
    "section": "5.7 A Similar Thesis",
    "text": "5.7 A Similar Thesis\nMy friend’s friend wrote a thesis similar in aim to mine last year. \nTitle: Using mel-frequency cepstral coefficients and principal components analysis to classify bird vocalisations based on citizen science recordings.  Student Name: Alex Dyfrig Swainston. \nI messaged Alex and got a copy, and he said he’s happy to help if I have any questions."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#new-ideas",
    "href": "posts/2022-10-13-MP4.html#new-ideas",
    "title": "Masters Project 18/10/2022",
    "section": "5.8 New Ideas:",
    "text": "5.8 New Ideas:\nHere are a few new ideas I had about tackling the cocktail problem.\nA big issue is the lack of properly labeled data for soundscapes. The biology department painstakingly handlabeled some soundscapes, but it is a difficult and time consuming task that even great ecologists struggle with. What if there was a way to create our own soundscapes that are already labeled? For instance, we have plenty of data from xeno-canto of individual bird songs with varying amounts of noise. What if I also found some audio files of forest environments, and I created a model to combine xeno-canto bird songs with these to imitate a real soundscape? This way, I could create an endless amount of soundscapes to train on, and the birds within them would be labeled!\n\n5.8.1 To create a soundscape:\n\nI could download bird song(s),\nCut out various parts of them, e.g. if it’s 3 minutes long, I cut out random intervals of 20-30 seconds to imitate the bird moving or other sounds overpowering their song,\nRandomly vary how loud the bird songs are,\nAdd in enviromental sounds like a forest soundscape (but being careful there are no birds present!),\nUse a noise function, (which is used in stable diffusion), to randomly add noise. Alternatively, find a way to make a model that can generate real noise that is recorded by microphones and use that.\n\nI could put multiple birdsongs in the same artificial soundscape, and even make them overlap, but I also need to be careful that perhaps I should make the birds singing be realistically in the same environment. I mean I shouldn’t put two birds together that geographically would never meet, or two birds that never sing at the same time of day, or in general enviromental sounds that don’t match the birds present.\nAnother idea is to add geographical data somehow to the dataset. Perhaps with another input for a satellite image to help."
  },
  {
    "objectID": "posts/2022-10-17-CLA1.md.html",
    "href": "posts/2022-10-17-CLA1.md.html",
    "title": "fast.ai CLA Lesson 1",
    "section": "",
    "text": "2 Course Introduction\nAn introductory blog post for this course can be found here https://www.fast.ai/posts/2017-07-17-num-lin-alg.html. \nBecause machine learning is largely about manipulating data, and almost all data can be represented as a matrix, understanding linear algebra is often cited as a prerequisite to reading and understanding formal mathematical descriptions of machine learning methods, as well as creating or editing existing methods. \nComputational Linear Algebra is a fast.ai course covering linear algebra to be centered around practical applications and algorithms. \nThere are four main areas for machine learning in which some linear algebra knowledge can help: - Speed (how fast matrix multiplication occurs) - Accuracy (how accurately can computers represent numbers) - Memory Usage (how to efficiently store matrices) - Scalability (how to use more data than you have the memory to store)\nThe reason why we are interested in these things is because often the bottleneck to a machine learning algorithm is within these four areas. In other words, knowledge in these areas can be the difference between a great ML approach and an unusable one. One example is in the case of how CNNs create their convolutional layers. While there are many mathematically equivalently orders in which to create these layers, some are evidently significantly faster. When these are applied in bulk, the optimisation makes all the difference. So in order to design or edit algorithms for usage in ML, knowledge in computational linear algebra is essential, particularly in research contexts as new approaches have not yet been optimised or implemented in existing frameworks.\n\n\n3 Lesson Overview\nThis lesson covers the basics for our four main optimisation areas: Speed, Accuracy, Memory Usage and Scalability.\n\n\n4 The topics covered, briefly\n\nAccuracy: Number representation, Machine Epsilon, Conditioning and Stability. Approximation Accuracy.\nMemory Use: Sparse vs Dense Matrices.\nSpeed: Computational Complexity, Vectorisation, Locality (Memory Usage), Scaling.\n\n\n\n5 Lecture/Notebook Notes\nThere are two key types of matrix computation: Matrix and tensor products (combining matrices), and matrix decompositions (pulling them apart).\nConvolutions are a special kind of matrix product, but can also be represented as a neural network where the image pixels are the start nodes, the kernel elements are the weights, and the convolution pixels end nodes.\n\n5.0.1 Accuracy\nThe representation of numbers:\nOn paper, fractions are infinitely written. Computers however cannot store fractions 100% precisely because they are using discrete memory to store infinite precision. We ran iterations of a function that inputs and outputs fractions. Every iteration a very small error is added, harmless for the first few. But over time, these errors result in an entirely wrong answer.\nFor IEEE Double precision (an agreed standard): \nThe continuous interval between [1,2] in a computer is represented as \\(1, 1+2^{-52},1+2x2^{-52}...,2\\)  So in this case, we see clearly it doesn’t represent infinite precision. The smallest increment, in this case \\(2^{-52}\\), depends on the size of the interval. For a bigger interval, [2,4], it’s \\(2^{-51}\\), bigger by a magnitude. \nMachine Epsilon:\nMachine Epsilon is defined as half the distance between 1 and the next larger number. \nI believe this means in the case of our [1,2] interval, \\(\\varepsilon_{machine}=2^{-52}/2\\). \nBut the notes state: “IEEE standards for double precision specify \\(\\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}\\)”, implying that Machine Epsilon is a constant value for a machine, rather than dependent on the interval or calculation involved?\nRegardless, we often describe error in terms of \\(\\varepsilon\\). For instance, say we represent a real number \\(x\\) in a computer, so have a approximation \\(fl(x)\\). The difference between \\(x\\) and \\(fl(x)\\) is always smaller than \\(x*\\varepsilon\\).\nAs an equation: \\(fl(x)=x \\cdot (1 + \\varepsilon)\\),  the error is from the \\(x*\\varepsilon\\) term.\nFor operations in a computer, +,-,x,/:  $ x y = (x * y)(1 + )$, the error is from all the terms containing \\(\\varepsilon\\).\nConditioning and Stability:\nBecause we can’t represent numbers exactly, we need to know the errors that occur as a result. There are two defined terms to help with this:\nConditioning, about how accurately we can represent the problem.  Conditioning: perturbation behavior of a mathematical problem (e.g. least squares)\nStability, about how accurately we can compute the answer to said problem.  Stability: perturbation behavior of an algorithm used to solve that problem on a computer (e.g. least squares algorithms, householder, back substitution, gaussian elimination)\n“A stable algorithm gives nearly the right answer to nearly the right question.” –Trefethen\nAn an example for how small problems in accuracy can cause problems, consider how a small difference in matrix values results in very different eigenvalues.\nimport numpy as np import scipy.linalg as la\nA = np.array([[1., 1000], [0, 1]]) B = np.array([[1, 1000], [0.001, 1]])\nprint(A) print(B)\nwA, vrA = la.eig(A) wB, vrB = la.eig(B)\nprint() print(wA, wB)\nHaving 0.001 instead of 0 resulted in the first eigenvalue to be 2 instead of 1!\n\n\n5.0.2 Approximation Accuracy\nAccepting some decreases in accuracy can speed up computations by orders of magnitude. So often using approximate algorithms is better.\nIn ML, some errors in training data representation are actually good because they force generalisability. \nAnd sometimes we need not be super concerned about having 100% precise training data representation because the data collected isn’t 100% precise in the first place.\nBloom filters can tell you a definite no, but not a definite yes, more like a probably yes. To remedy this, we can make a second more precise method to evaluate the items that are probably yes, while just ignoring the ones that are already known to be definitely no.\n\n\n5.0.3 Memory Use\nSparse vs Dense matrices.\nSparse storage is just storing the non-zero elements of your matrix because you know the others are just 0. There are special ways of doing sparse storage.\nDense storage is the normal way we do it when we code, we just store everything explicitly.\n\n\n5.0.4 Speed\nThe difference in speed between algorithms come from a number of areas by in particular: - Computational Complexity - Vectorisation - Scaling - Locality\nComputational complexity and big \\(\\mathcal{O}\\) notation is about approximating the number of operations you need to do for a particular algorithm. More info: on Interview Cake and practice on Codecademy.\nVectorisation is about applying an operation is multiple elements at once. Numpy replies on vectorized low level linear algebra APIs (BLAS and LAPACK) to do it’s matrix operations.\nLocality is about how data in use is stored. Computers are usually slow because of the way we access data. Generally speaking, the faster the memory (so the faster we access the data), the less of it we have/the more expensive it is. Computers have many varying memory storage types, and each step down to slower memory you go, that memory is atleast an order of magnitude slower than the one before it.\nWe want to minimise the time we take to retrieve data in a computation. For example, by keeping items we are going to use multiple times in a computation in fast memory, and keepings items we use rarely in slow memory.\nA video to illustrate locality is then shared.\nfrom IPython.display import YouTubeVideo YouTubeVideo(“3uiEyEKji0M”)\nCode optimisation is really important. Even for a simple task, finding the average of 3 pixels, the code would normally be simple, but writing complex code would speed it up by 11x! It’s faster because it distributes work across threads (parallelism). Locality is making sure the pixels being used successively is in fast memory (cashe). W/O locality, paralleism can’t be great.\nWe change the order in that CNN is done and as a result the way we store the pixel data, and get a much faster computational as a result. Removing redundancy in computation also speeds up the computation. Each computation technique has potential trade offs. For example, having redundant computation to improve locality.\nTemporaries is data stored in a temporary variable in RAM. For example in Numpy, when we compute an equation, Numpy stores each equation variable as temporaries and then retrieves it. This is slow because there’s no point storing each variable in the RAM and then immediately having to use it. Simply storing these variables in the cache would be so much faster.\nScaling:\nWe we want to scale our computation across multiple cores in a computer. This is called parallesiation. Scalable algorithm are algorithms where the input can be broken into smaller pieces where each can be handled by a different core and the end result found by piecing together these pieces.\n\n\n\n6 Links\nThe lecture for this sessions is https://www.youtube.com/watch?v=8iGzBMboA0I&list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&index=1.  The lesson resources including the notebook(s): https://github.com/fastai/numerical-linear-algebra."
  },
  {
    "objectID": "posts/AIS1.html",
    "href": "posts/AIS1.html",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "",
    "text": "This is the first post in a series about AI safety (AIS). I am apart of the effective altruism AI discussion group at Durham, and these posts will be detailing my experience and notes about the topics we discuss. (This post is still in progress)"
  },
  {
    "objectID": "posts/AIS1.html#core-reading-1-four-background-claims",
    "href": "posts/AIS1.html#core-reading-1-four-background-claims",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.1 Core Reading 1: Four Background Claims",
    "text": "3.1 Core Reading 1: Four Background Claims"
  },
  {
    "objectID": "posts/AIS1.html#core-reading-2-agi-safety-from-first-principles-ngo-2020",
    "href": "posts/AIS1.html#core-reading-2-agi-safety-from-first-principles-ngo-2020",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.2 Core Reading 2: AGI safety from first principles (Ngo, 2020)",
    "text": "3.2 Core Reading 2: AGI safety from first principles (Ngo, 2020)\n(from section 1 to end of 2.1)"
  },
  {
    "objectID": "posts/AIS1.html#core-reading-3-more-is-different-for-ai-steinhardt-2022",
    "href": "posts/AIS1.html#core-reading-3-more-is-different-for-ai-steinhardt-2022",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.3 Core Reading 3: More is different for AI (Steinhardt, 2022)",
    "text": "3.3 Core Reading 3: More is different for AI (Steinhardt, 2022)\n(only introduction, second post, third post)"
  },
  {
    "objectID": "posts/AIS1.html#core-reading-4-most-important-century-series-summary-karnofsky-2021a",
    "href": "posts/AIS1.html#core-reading-4-most-important-century-series-summary-karnofsky-2021a",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.4 Core Reading 4: “Most important century” series summary (Karnofsky, 2021a)",
    "text": "3.4 Core Reading 4: “Most important century” series summary (Karnofsky, 2021a)"
  },
  {
    "objectID": "posts/AIS1.html#core-reading-5-forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell-karnofsky-2021b",
    "href": "posts/AIS1.html#core-reading-5-forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell-karnofsky-2021b",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.5 Core Reading 5: Forecasting transformative AI: the “biological anchors” method in a nutshell (Karnofsky, 2021b)",
    "text": "3.5 Core Reading 5: Forecasting transformative AI: the “biological anchors” method in a nutshell (Karnofsky, 2021b)"
  },
  {
    "objectID": "posts/AIS1.html#exercise-1",
    "href": "posts/AIS1.html#exercise-1",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.6 Exercise 1",
    "text": "3.6 Exercise 1\nAs discussed in Ngo (2020), Legg and Hutter define intelligence as “an agent’s ability to achieve goals in a wide range of environments”: a definition of intelligence in terms of the outcomes it leads to. An alternative approach is to define intelligence in terms of the cognitive skills (memory, planning, etc) which intelligent agents used to achieve their desired outcomes. What are the key cognitive skills which should feature in such a definition of intelligence?"
  },
  {
    "objectID": "posts/AIS1.html#discussion-prompts-2",
    "href": "posts/AIS1.html#discussion-prompts-2",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.7 Discussion Prompts 2",
    "text": "3.7 Discussion Prompts 2\nOne intuition for how to think about very smart AIs: imagine speeding up human intellectual development by a factor of X. If an AI could do the same quality of research as a top scientist, but 10 or 100 times faster, and with the ability to make thousands of copies, how would you use it?"
  },
  {
    "objectID": "posts/AIS1.html#discussion-prompts-3",
    "href": "posts/AIS1.html#discussion-prompts-3",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.8 Discussion Prompts 3",
    "text": "3.8 Discussion Prompts 3\nHow frequently do humans build technologies where some of the details of why they work aren’t understood by anyone? What, if anything, makes AI different from other domains? Would it be very surprising if we built AGI without understanding very much about how its thinking process works?"
  },
  {
    "objectID": "posts/AIS1.html#discussion-prompts-5",
    "href": "posts/AIS1.html#discussion-prompts-5",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.9 Discussion Prompts 5",
    "text": "3.9 Discussion Prompts 5\nWhat are the most plausible ways for the hypothesis “we will eventually build AGIs which have transformative impacts on the world” to be false? How likely are they?"
  },
  {
    "objectID": "posts/MP5.html",
    "href": "posts/MP5.html",
    "title": "Masters Project 25/10/2022",
    "section": "",
    "text": "(This post is still in progress). The fifth post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/MP5.html#during-the-meeting-we-discussed",
    "href": "posts/MP5.html#during-the-meeting-we-discussed",
    "title": "Masters Project 25/10/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:"
  },
  {
    "objectID": "Test Posts/post-with-code/index.html",
    "href": "Test Posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "Test Posts/welcome/index.html",
    "href": "Test Posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/MP5.html#stable-diffusion-mechanics",
    "href": "posts/MP5.html#stable-diffusion-mechanics",
    "title": "Masters Project 25/10/2022",
    "section": "4.1 Stable Diffusion mechanics",
    "text": "4.1 Stable Diffusion mechanics\nStable diffusion heavily relies on two things: the data it has been trained on, and the prompt it is given to generate a given image.\n\n4.1.1 Training\nCurrently as far as I can see, stable diffusion model online have been trained on ‘normal’ images in order to generate ‘normal’ artwork. It’s hard to define what normal is, but what it isn’t, is spectrograms of birdsongs and environments. My approach would likely require training a stable diffusion model from scratch on spectograms, of either relevant spectrograms or other audio, because intuitively, fine-tuning pretrained wouldn’t work well enough. Regardless it’s worth trying just too see results however.\nTo train a model yourself takes two things: enough data and enough computation. There should be enough data online given the size of xeno-canto. Computation is the harder demon. Being at a university fortunately might resolve this for me. There are computing clusters available for research use, so in theory it should be possible to request time for one. Alternatively, it is possible to rent GPU time from companies, but this would likely be very expensive, so would require research funding.\nHowever, there might be a way to decrease computational needs: \nTo lower the computation needed to train a diffusion model, we use latent (compressed) representations of images. In fact, stable diffusion itself is a latent diffusion model rather than a general diffusion model specifically because of this. The autoencoder (vae) in stable diffusion is what does this image compression, and it makes a significant difference, reducing memory requirements for a 512x512x3 image by 48 times, speeding up training and inference (image creation) significantly.\nStable Diffusion is based on latent diffusion. It was proposed in a paper High-Resolution Image Synthesis with Latent Diffusion Models at https://arxiv.org/abs/2112.10752.\nWhat if there is some specific autoencoder approach for spectrogram generation that could decrease computational needs significantly?\n\n\n4.1.2 Prompts (and labels)\nThe entire point of this soundscape approach is to create labeled soundscapes because there are not enough available. If stable diffusion could create soundscapes, but not labeled ones, the whole approach falls out. Fortunately, the way stable diffusion creates images, using prompts, might also conveniently be the answer the answer to this issue.\nAn example I found on https://lexica.art/ at https://lexica.art/prompt/ea5b8646-6e6e-4a0e-b618-bae8c796f8cc of a generated image is as follows:\nPrompt: “Scifi art by greg rutkowski, a man wearing futuristic riot control gear, claustrophobic and futuristic environment, detailed and intricate environment, high technology, highly detailed portrait, digital painting, artstation, concept art, smooth, sharp foccus ilustration, artstation hq”\nImage:\n\n\n\nimage.png\n\n\nNot all prompts are like this, in fact, thinking about what prompts to use in itself is another entire process since it’s surprisingly not intuitive or easy to write good prompts.\nRegardless, looking at our prompt, it also has words that can be related to labels.  “Scifi art”, “man”, “futuristic riot control gear”, “claustrophobic and futuristic environment”, “high technology”, “highly detailed portrait”, etc. Some words aren’t as useful, like “detailed and intricate environment” as it doesn’t give much information. Some words like “by greg rutkowski” are telling of the artist this was based on.\nIf we had a soundscape generation model, we could use a prompt I’m making up to illustrate like:\nPrompt: “Forest enviroment, a Barn owl singing lightly at the start, lightly noisy environment, near a river, a Black grouse singing throughout, highly detailed, soundscape”\nThis prompt would include information about the labels we want, namely, “a Barn owl singing lightly at the start”,“near a river”, “a Black grouse singing throughout”.\nThis does mean however, that a model trying to use these soundscapes for training needs to be able to know how to use these labels properly, which is different from using the labels in BirdCLEF 2022 to train for instance. There is also the natural concern of whether the labels from the prompts are correct (enough) to make soundscape generation method good enough to be useful training for real soundscapes.\n\n\n4.1.3 An ambitious addition\nA spectrogram is a 2D (or 3D) visualisation of a sound, it is an image.  In the example given I used a 2D visualisation, but what about doing all of this in 3D? \nRecently, Google released a paper about 3D image generation! That is creating 3D images from a written prompt. 2 minute papers has a brilliant video on it https://www.youtube.com/watch?v=L3G0dx1Q0R8. They call it “DreamFusion”.\nUnofficial open source DreamFusion using stable diffusion is already becoming available. https://github.com/ashawkey/stable-dreamfusion. I really do wonder if using a 3D spectrogram to generate soundscapes would be better than a 2D one.\n\n\n4.1.4 Sound to Spectrogram and Spectrogram to Sound conversion.\nWhether or not it’s possible to turn a spectrogram back into audio might not actually matter too much.  What I mean is, to classify the birds in a given test spectrogram, we simply convert it into a spectrogram and then use a image model trained on only spectrograms to classify it. I think there are CNN based papers that do this."
  },
  {
    "objectID": "posts/MP5.html#by-audio-diffusion-harmonai",
    "href": "posts/MP5.html#by-audio-diffusion-harmonai",
    "title": "Masters Project 25/10/2022",
    "section": "5.1 By audio diffusion: Harmonai",
    "text": "5.1 By audio diffusion: Harmonai\nStability.ai released stable diffusion. They also work in other areas, including AI in biology and AI in audio.\nLast month they released Harmonai, an open source generative audio tool, dance diffusion. Dance diffusion allows you create music. It’s a digital music production tool. More info can be found at a wandb.ai blog post https://wandb.ai/wandb_gen/audio/reports/Harmonai-s-Dance-Diffusion-Open-Source-AI-Audio-Generation-Tool-For-Music-Producers–VmlldzoyNjkwOTM1, at Harmonai’s website https://www.harmonai.org/, and their GitHub https://github.com/Harmonai-org/sample-generator. There’s also a guide to using it at https://drive.google.com/file/d/1nEFEpK27v0nytNXmmYQb06X_RI6kKPve/view.\nI haven’t yet throughly investigated the use of dance diffusion, but:\nThe latter guide detailed an interested model checkpoint (a pretrained model to fine-tune). It is honk-140k, trained on recordings of the Canada Goose from xeno-canto. This implies that it’s possible to generate birdsong with it, once trained.\nBut whether it’s possible to do labeled soundscape creation from individual labeled audio labels is my concern. About the labels, dance diffusion doesn’t seem to use prompts like stable diffusion, it seems to just create new sounds based on the trained data. This isn’t particularly useful because there is already enough birdsong available online.\nRegardless, understanding how dance diffusion generates sound might yield some new ideas about how to use stable diffusion to do. Specifically, how it handles audio data. Perhaps I would find a way to to do labeled soundscape creation with it once I know how it works."
  },
  {
    "objectID": "posts/MP5.html#by-manual-generation",
    "href": "posts/MP5.html#by-manual-generation",
    "title": "Masters Project 25/10/2022",
    "section": "5.2 By manual generation:",
    "text": "5.2 By manual generation:\nThis would be manually doing the process by combining previous audio data together to create soundscapes.  Unlike diffusion methods, this isn’t a new idea, and has likely been tried and tested before. Because diffusion is so new, I’m more attracted to it as a novel idea."
  },
  {
    "objectID": "posts/MP5.html#by-other-generation-methods",
    "href": "posts/MP5.html#by-other-generation-methods",
    "title": "Masters Project 25/10/2022",
    "section": "5.2 By other generation methods:",
    "text": "5.2 By other generation methods:\nThis would be doing the process by combining previous audio data together to create soundscapes.  Unlike diffusion methods, this isn’t as new as an idea, and has likely been tried and tested before. Because diffusion is so new, I’m more attracted to it as a novel idea.\nHowever there are some things I could learn from these approaches.\nThe Earth Species Project (https://www.earthspecies.org/) released a paper about BioCPPNet at https://www.nature.com/articles/s41598-021-02790-2. This is about solving the cocktail problem to tell apart sounds from a group of animals of the same species. For example, to tell which individual is speaking from a group of macaques monkeys.\nThey have a video explaining BioCPPNet at https://www.youtube.com/watch?v=TGWFr-6JCDk. In particular at the 1 minute mark, they state “We implement a supervised training scheme: we construct a synthetic mixture dataset by additively overlapping signals”.\nThis is synthetic mixture dataset creation, which could be similar to soundscape creation, so could be very useful to understand."
  },
  {
    "objectID": "posts/MP5.html#updated-and-fixed-blog",
    "href": "posts/MP5.html#updated-and-fixed-blog",
    "title": "Masters Project 25/10/2022",
    "section": "8.2 Updated and Fixed blog",
    "text": "8.2 Updated and Fixed blog\nI was having trouble with my old fastpages based blog not displaying posts with maths and images correctly. To remedy this, I created a blog using Quarto instead. This post is on the new blog, and I transferred all the old posts here too."
  },
  {
    "objectID": "posts/MP5.html#investigated-durham-computation-lending",
    "href": "posts/MP5.html#investigated-durham-computation-lending",
    "title": "Masters Project 25/10/2022",
    "section": "8.3 Investigated Durham Computation Lending",
    "text": "8.3 Investigated Durham Computation Lending"
  },
  {
    "objectID": "posts/MP5.html#found-yet-more-datasets",
    "href": "posts/MP5.html#found-yet-more-datasets",
    "title": "Masters Project 25/10/2022",
    "section": "8.4 Found yet more datasets",
    "text": "8.4 Found yet more datasets\nhttps://github.com/AgaMiko/bird-recognition-review has useful resources for birdsong classification and yet more datasets.\nThis blog post, https://towardsdatascience.com/sound-based-bird-classification-965d0ecacb2b, also contains an approach, but also a nice introduction to the problem."
  },
  {
    "objectID": "posts/MP5.html#another-anot",
    "href": "posts/MP5.html#another-anot",
    "title": "Masters Project 25/10/2022",
    "section": "8.4 Another anot",
    "text": "8.4 Another anot"
  },
  {
    "objectID": "posts/MP5.html#papers-about-birdclef-competitions",
    "href": "posts/MP5.html#papers-about-birdclef-competitions",
    "title": "Masters Project 25/10/2022",
    "section": "8.1 Papers about BirdCLEF competitions",
    "text": "8.1 Papers about BirdCLEF competitions\nAn immensely useful find this week are papers describing the different approaches the competition teams for BirdCLEF used.\nThe most recent: Overview of BirdCLEF 2022: Endangered bird species recognition in soundscape recordings, at http://ceur-ws.org/Vol-3180/paper-154.pdf.\nFor my soundscape generation approach, perhaps the most useful paragraph is found searching for ‘data augmentation’:\n“Sampathkumar & Kowerko [15]: Data augmentation is an important processing step in bird sound recognition because of the domain shift between training and test recordings. In their work, this team focused on evaluating the best augmentation scheme for this task. Most transformations focus on adding different patterns of noise to the source recording, thus emulating noisy soundscape recordings. While the authors find that all augmentations methods improve the baseline experiment, Gaussian noise, loudness normalization and tanh distortion appear to be most impactful.”\nMost approaches added noise to the training data to emulate noise in the test soundscape. They did not create synthetic soundscapes entirely like I proposed. Gaussian noise, loudness normalization and tanh distortion appear to be the most useful noise to add.\nAnd in general, the conclusion is useful:\n“Despite being set up as a few-shot learning task, few teams decided to employ techniques other than CNNs. Pre-trained neural networks for image recognition still dominated the task, and participants tried to cope with the lack of training data through intensive data augmentation and transfer learning. Surprisingly, there was only a weak correlation between the number of training samples and overall per-species performance. This indicates that other factors - such as repertoire size and call patterns - might outweigh training data quantity. Automatic detection of endangered and rare species remains challenging. Still, this year’s competition demonstrated that passive acoustic monitoring combined with machine learning could already be a powerful monitoring tool for some endangered species. BirdCLEF continues to engage a large number of data scientists from around the world to develop new and effective acoustic analysis solutions that aid avian conservation.”\nThere’s a lack of training data because BirdCEF doesn’t provide all of the data available on xeno-canto, which isn’t an issue for my project. Somehow more training samples per species didn’t correlate strongly with better species identification, because of other factors. Most teams tried using CNNs, but not with spectrograms.\nThe competition overview does well to motivate my soundscape creation approach:\n“In recent years, research in the domain of bioacoustics shifted towards deep neural networks for sound event recognition [7, 8]. In past editions, we have seen many attempts to utilize convolutional neural network (CNN) classifiers to identify bird calls based on visual representations of these sounds (i.e., spectrograms) [9, 10, 11]. Despite their success for bird sound recognition in focal recordings, the classification performance of CNNs on continuous and omnidirectional soundscape recordings remained low. Passive acoustic monitoring can be a valuable sampling tool for habitat assessments and observations of environmental niches, which often are threatened. However, manual processing of large collections of soundscape data is not desirable, and automated attempts can help to advance this process [12]. Yet, the lack of suitable validation and test data prevented the development of reliable techniques to solve this task.\nBridging the acoustic gap between high-quality training recordings and complex soundscapes with varying ambient noise levels is one of the most challenging tasks in the domain of audio event recognition. This is especially true when the amount of training data is insufficient, as is the case for many rare and endangered bird species around the globe. Despite the vast amounts of data collected on Xeno-canto and other online sound libraries, audio data for endangered birds is still sparse. However, those endangered species are most relevant for conservation, rendering acoustic monitoring of endangered birds particularly difficult.”\nI should reading reference [12] of the paper to get a better idea of the soundscape availability problem.\nSearching for ‘diffusion’ within the paper yields no results, implying further that indeed using diffusion to generate soundscapes is a novel approach."
  },
  {
    "objectID": "posts/MP5.html#found-another-another-thesis",
    "href": "posts/MP5.html#found-another-another-thesis",
    "title": "Masters Project 25/10/2022",
    "section": "8.5 Found another another thesis",
    "text": "8.5 Found another another thesis\nBird Species Classification And Acoustic Features Selection Based on Distributed Neural Network with Two Stage Windowing of Short-Term Features at https://arxiv.org/ftp/arxiv/papers/2201/2201.00124.pdf by Nahian Ibn Hasan like last week’s thesis, describes another good introduction to the problem and approach."
  },
  {
    "objectID": "posts/MP5.html#by-audio-diffusion",
    "href": "posts/MP5.html#by-audio-diffusion",
    "title": "Masters Project 25/10/2022",
    "section": "5.1 By audio diffusion:",
    "text": "5.1 By audio diffusion:\n\n5.1.1 Harmonai\nStability.ai released stable diffusion. They also work in other areas, including AI in biology and AI in audio.\nLast month they released Harmonai, an open source generative audio tool, dance diffusion. Dance diffusion allows you create music. It’s a digital music production tool. More info can be found at a wandb.ai blog post https://wandb.ai/wandb_gen/audio/reports/Harmonai-s-Dance-Diffusion-Open-Source-AI-Audio-Generation-Tool-For-Music-Producers–VmlldzoyNjkwOTM1, at Harmonai’s website https://www.harmonai.org/, and their GitHub https://github.com/Harmonai-org/sample-generator. There’s also a guide to using it at https://drive.google.com/file/d/1nEFEpK27v0nytNXmmYQb06X_RI6kKPve/view.\nI haven’t yet throughly investigated the use of dance diffusion, but:\nThe latter guide detailed an interested model checkpoint (a pretrained model to fine-tune). It is honk-140k, trained on recordings of the Canada Goose from xeno-canto. This implies that it’s possible to generate birdsong with it, once trained.\nBut whether it’s possible to do labeled soundscape creation from individual labeled audio labels is my concern. About the labels, dance diffusion doesn’t seem to use prompts like stable diffusion, it seems to just create new sounds based on the trained data. This isn’t particularly useful because there is already enough birdsong available online.\nRegardless, understanding how dance diffusion generates sound might yield some new ideas about how to use stable diffusion to do. Specifically, how it handles audio data. Perhaps I would find a way to to do labeled soundscape creation with it once I know how it works.\n\n\n5.1.2 The Generative Landscape\nThere is a course online at https://johnowhitaker.github.io/tglcourse/. It covers all types of diffusion generation, including image and audio.\nLesson 15 at https://johnowhitaker.github.io/tglcourse/dm4.html is about Diffusion for Audio on Class conditioned birdcalls. The course is not yet complete yet, but should be soon.\nThe course is created by Jonathan Whitaker, who also is contributing to fast.ai part 2, so should be of great quality."
  }
]