[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my website. It’s currently in its early stages and hopefully will become more detailed with time. My name is Adnan, I’m a 4th year integrated Masters Physics and Computer Science student at Durham University. Currently I use this site to share blog posts about my various technical projects. They are on the main page, and ordered by category via the sidebar."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RvCode",
    "section": "",
    "text": "Masters\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAIS\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastaiCLA\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasters\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2022\n\n\nAdnan Jinnah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-09-15-Lesson1&2Blog.md.html",
    "href": "posts/2022-09-15-Lesson1&2Blog.md.html",
    "title": "fast.ai Lessons 1&2",
    "section": "",
    "text": "2 Lesson Overview\nThe first lesson did well to summarise an introduction to model creation. While it was content I have already covered throughout previous courses, a quick refresher is good. The teaching philosophy behind the course, to learn by doing in conjunction to theory, is excellent in keeping things engaging and enjoyable. The second lesson covered many new software tools and applications very concisely. Although, I had to spend a lot of time troubleshooting various issues with getting things to work on my PC.\n\n\n3 The topics covered, briefly\n\nHow to create and train a deep learning model in FastAI\nHow to better filter and amend training data to get better model performance\nHow to export a ML model as a file\nHow to load ML models and allow websites to use them\nHow to save coding repositories online\nHow to host and create blog posts that include code easily\n\nThis included Kaggle/Google Collab for creating, training and exporting models. Anaconda for Jupyter Notebooks. Visual Studio Code, a programming IDE, to interact with: HuggingFaceSpaces, a website to host models, and GitHub, a website to store one’s code. It took much time and troubleshooting to understand how to install and use these tools, but the result is the understanding of how to quickly produce deep learning models and make them accessible on the internet, as well as produce blogs containing code, with good practices and expandability behind functionality.\n\n\n4 Links\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson2.html, which includes a lecture, a notebook, and a set of questions from the course book. My answers can be found on my GitHub repository below.\nMy machine learning model “NovaOrToast” attempts to classify whether a given cat photo is my older sister’s cat Toast, or my younger sister’s cat Nova. It can be found at https://huggingface.co/spaces/exiomius/NovaOrToast, where it is possible to upload images and get them classified immediately.\nMy repository for this can be found at https://github.com/exiomius/NovaOrToast The code for the model creation and training can be found at https://www.kaggle.com/code/adnanjinnah/nova-or-toast-model-creator/edit. One must use an online GPU service like Kaggle or Google Collab to train models due to their computational intensity. It should be possible to link said services with GitHub, which would be best to keep a centralised codebase.\n\n\n5 The pipeline to upload a machine learning model online\n\nFirstly create, train and export the model using a online IDE such as Google Collab or Kaggle. This is because the training will take up much GPU computation, so it is better to avoid doing it on one’s own machine.\nCreate a Jupyter Notebook and import the fully trained model. Use a Python module called Gradio to create a local webpage of your model to test if it works on your local web adress.\nConvert your Jupyter Notebook into a .py file.\nUsing HuggingFaceSpaces, create a space for your new model.\nUsing Visual Studio Code, clone the repository of your new space, then add in your Jupyter Notebook’s, .py file after conversion.\nCreate a ‘requirements’.txt including the modules used so that HuggingFaceSpaces can install them as required.\nUsing Visual Studio Code, use Git to push your update to HuggingFaceSpaces.\nAfter a few minutes, your model will be avaliable on the HuggingFaceSpaces space you created. The progress behind this will be available to see on your GitHub repository under the ‘actions’ tab.\n\nNot included are many many details of how to specifically do these instructions, as there are too many to throughly convey, especially as much time was spend troubleshooting for my specific machine and operating system (Windows).\n\n\n6 Things to improve:\n\nA better way to convert Jupyter Notebooks to .py files. It seems to work only with a settings.ini file, but I was not able to create one that works myself, so had to copy one on the course director’s GitHub, which works but I cannot figure out how to place the resulting .py file in the correct folder.\nHow to upload Kaggle code to GitHub seamlessly.\n\nA way to automatically update Anaconda’s modules. The default Anaconda update prompts update the app but not always the modules I require. For instance, it took me a while to figure out the Python Pillow module wasn’t updated on Anaconda despite it being a fresh installation."
  },
  {
    "objectID": "posts/2022-09-19-Lesson3Blog.md.html",
    "href": "posts/2022-09-19-Lesson3Blog.md.html",
    "title": "fast.ai Lesson 3",
    "section": "",
    "text": "2 Lesson Overview\nIn this lesson the mechanisms behind deep learning are explored. Unlike the previous lesson, which was focused on applications and many pieces of new software, this lesson was focused on reading through a textbook chapter and understanding how exactly deep learning operates.\nI suppose this knowledge will be required in order to understand how best to create good deep learning models.\n\n\n3 The topics covered, briefly\n\nHow are images represented in a computer.\nHow are files in datasets structured.\nPython functionality: what is list comprehension, and fact that NumPy/PyTorch are much faster than pure python.\nTensors: a list like structure used in deep learning. Ranks indicating dimensions and shapes indicating the length of each axis.\nLoss functions: RMSE and L1 norm. Two loss functions, similar but RMSE penalises larger errors more.\nSGD: stochastic gradient descent. A way to make a model learn by updating its weights automatically.\nThe difference between loss and metrics: Loss is for updating weights, metrics are for human evaluation.\nMini-batches. Each epoch, instead of using one piece of training data to update weights (pure SGD), or using all the training data simultaneously to update the weights (GD), splitting the data into random batches (subsets) of the training data to update the weights.\nThe sigmoid function: a smooth curve between 0 and 1 used in deep learning as a loss function. It is used because it has a meaningful derivative so allows good weight updates.\nDataLoader class: a function to take a dataset and split it into random mini-batches. DataLoaders contains a training a training and validation DataLoader.\nReLU: a rectified linear unit. It’s just a linear function, a line, with negative values = 0.\nActivation functions. Also known as nonlinear functions, these are placed between layers of linear functions in neural networks so that the linear functions do not combine into another linear function.\nThe Universal Approximation Theorem: that two linear layers with an activation function inbetween can approximate any function given the correct weights and enough nodes. In practice however we use more than two linear layers because it works better.\n\nMuch more content was covered in much more detail, including how to program a deep learning model from scratch in Python.\n\n\n4 Links\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson3.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository at https://github.com/exiomius/PDL-Lesson-3."
  },
  {
    "objectID": "posts/2022-09-21-Lesson4Blog.md.html",
    "href": "posts/2022-09-21-Lesson4Blog.md.html",
    "title": "fast.ai Lesson 4",
    "section": "",
    "text": "4: Natural Language (NLP)"
  },
  {
    "objectID": "posts/2022-09-21-Lesson4Blog.md.html#getting-started-with-nlp-for-absolute-beginners-covers",
    "href": "posts/2022-09-21-Lesson4Blog.md.html#getting-started-with-nlp-for-absolute-beginners-covers",
    "title": "fast.ai Lesson 4",
    "section": "3.1 “Getting started with NLP for absolute beginners” covers:",
    "text": "3.1 “Getting started with NLP for absolute beginners” covers:\n\nHow to use Kaggle. Including how to use Kaggle datasets on your own PC and how to submit entries to their competitions.\nHow to use Pandas and Transformers DataFrames to view data, and get it into the correct format for NLP model training.\nHow to use Transformers to train and classify.\nThe difference between training, validation, and testing data. An emphasis has been put on creating good validation data with a quote by Dr Rachel Thomas stating that often a poorly chosen validation dataset results in a disconnect between development and deployment performance. Her article describes this in more detail.\nTraining data is to train. Validation data is get an idea of generalizable performance, but often it is limited in doing so. This is often either because it hasn’t been chosen prudently enough or because one has accidentally overfitted to it. Imagine a model to predict the price of a stock. Randomly selecting points to be validation data is poor because that is not how the model will be used in practice and is a much easier task. Selecting some amount of further price movement to be validation data makes sense, but your model may overfit to the specific movement pattern of that timeframe.\nThe Pearson Correlation Coefficient, r, as a metric is discussed. Emphasis is put on firstly trying metrics on datasets to understand them, rather than delving immediately into the maths. Doing so for example yielding the fact that r is really sensitive to outliers.\nOn outliers. Outliers are important parts of the data and mustn’t be removed without reason. In our housing income and average number of rooms correlation example, there are outliers that could lead to some insights about the data. Perhaps the outliers are from a specific type of house or a specific geographical area. In this case, it may make more sense to use separate models to train on and predict separate clusters of data.\nOn hyperparameters. Learning rate is the most important in this case. The idea is to find the largest value that doesn’t result in failed training. FastAI provides a learning rate finder to help you, but Transformers does not."
  },
  {
    "objectID": "posts/2022-09-21-Lesson4Blog.md.html#iterate-like-a-grandmaster-covers",
    "href": "posts/2022-09-21-Lesson4Blog.md.html#iterate-like-a-grandmaster-covers",
    "title": "fast.ai Lesson 4",
    "section": "3.2 “Iterate like a grandmaster!” covers:",
    "text": "3.2 “Iterate like a grandmaster!” covers:\n\nHow a grandmaster Kaggle competitor works. He focused on creating an effective validation set and iterating rapidly to find changes which improve validation set results. These skills carry over to real projects.\nFor the patent classification, the input is anchor and label is target. In the test data, there are anchors not present in the training data. Thus we should make sure there are anchors in the validation data that are not in the training data.\nPick a learning rate and batch size that fits your GPU. This means picking them so that we can iterate rapidly to test things out.\nPick a reasonable weight decay.\nPick a small number of epochs, like 3, to test with. This is because in practice much of the performance will be made in those. Thus there is no need to run many epochs every time you try a change. If there is not improvement within a few epochs, then your change is likely not very significant. Later on, when you want to more thoroughly evaluate a change, you can use more epochs and cross-validation.\nPick a class to setup your arguments for your trainer. Transformers by default uses one, but FastAI has others.\nYou need a stable validation accuracy from your epochs to know whether your future changes is making improvements. To know whether your predictions are stable, run the model from scratch a few times, say 3, and check how much it varies.\nTo make changes easier, create a function to setup tokenization and a function to setup model creation. Then you can pass parameters quickly to create new models.\nIn this case, previously we tokenised with a special token sep to indicate seperate entities in our input. Simply changing sep to s resulting in a big performance increase.\nInstead of using the same special token to indicate separate entities in our input, using different special tokens for each entity could better inform the model that each entity is different.\nSimply changing all text to lowercase can often help a little too.\nThere’s so many things you could try. In this notebook, most of the iteration was done by changing tokenisation, but also playing around with the other parameters might yield better results. However, instead of trying to optimise the factors already present, there are other ideas you can try. Firstly fine tuning your general language model using just patent data. Or using a model pretrained on legal vocabulary instead of general vocabulary. Using a different type of model, not in terms of architecture but a model created for a different task. One of our columns is ‘context’, which is a code e.g. B7 referring to a patent context. Instead of using the code, we could replace it with a description found online. There’s so many things you can try especially if you think a little creatively.\nRemember for the final submission, to train on your validation data as well."
  },
  {
    "objectID": "posts/2022-09-25-MP1.html",
    "href": "posts/2022-09-25-MP1.html",
    "title": "Masters Project Introductory Post",
    "section": "",
    "text": "2 Title: Data science for biodiversity loss\nSupervisor: Prof Charles Adams. Second Supervisor: Dr Robert Potvliege. Category: General. Type: Computation/Data Analysis/Experiment.\nBiodiversity loss due to human action is increasingly creating an existential threat to all live on Earth. In order to take appropriate action we need better data. However biodiversity data is both more diverse and more difficult to accumulate than say climate data. In this project, we shall look at data analysis on bird song. Although, under ideal conditions it is possible to identify different species [1-3]. In a noisy environment, which is more typical, this becomes more challenging. One approach that we shall pursue is to construct time frequency pattern and then use pattern recognition technique to identify particular events [4].\n[1] scikit‐maad An open‐source and modular toolbox for quantitative soundscape analysis in Python, Ulloa et al, Meth. in Ecology and Evolution, 12, 2334 2021\n[2] Multifractal analysis of birdsong and its correlation structure, R Bishal, GB Mindlin, and N Gupte Phys. Rev. E 105, 014118 2022\n[3] Large-scale analysis of frequency modulation in birdsong data bases, D Stowell, MD Plumbley, Methods Ecol Evol, 5: 901 (2014)\n[4] New aspects in birdsong recognition utilizing the gabor transform, S HEUER , P TAFO, H HOLZMANN, S DAHLKE, Proc. of the 23rd Int. Congress on Acousitics, Aachen, September 9-13, 2019.\n\n\n3 Aim\nIn essence, the (first) aim of this project is to pick apart bird song from a noisy sound recording and identify birds from it.\nIn the future other aims may emerge, such as trying to understand the meaning behind birdsong rather than classify their singers. Perhaps even trying to generate birdsong may be an interesting idea to yield some insights. It would be fascinating seeing if/how birds would respond to generated birdsong.\n\n\n4 Approaches\nThe four above referenced papers are probably the best place to start when looking for initial approaches. While I am comfortable with both Physics and Programming, my preference in comfortability and in interest does lean towards the latter, especially when Machine Learning is involved. Having said that, the first, third and fourth reference all are ML based to a degree.\nI don’t yet have the prerequisite knowledge to understand at a glance the abstracts of the second and third references.\nThe first however is an open-source Python module called scikit-maad, which is instantly recognisable by my familiarity with other popular scikit modules. Furthermore, I feel warmly welcomed by the described online documentation and practical examples around it. Lastly, the module highlights its ability to easily integrate Machine Learning Python packages.\nThe fourth reference details that current approaches convert audio recordings into spectrograms using the Gabor transform, then enter them as images to a CNN for classification. This is a really intuitive ML approach, and one I might try and implement. The paper then details that most approaches focus on finding the best CNN hyperparameters for accuracy, so in contrast the authors attempt to evaluate the parameters for the Gabor transform itself.\nAll in all, I think my first priority is to investigate and implement the first and fourth references. It’s not ideal not being able to easily understand the other papers, but my reasoning is that having gone through the more understandable references first would yield prerequisite knowledge to go back and understand the others.\nCoincidentally, I was talking briefly with my older brother about the project and he commented that the problem is awfully similar to other audio separation problems. Separating out bird song from a noisy forest environment and then identifying them, is similar to separating out instrument sounds from a regular song and identifying them. We both actually have a mutual friend who did his Masters project in the latter. Spotify also appears to have its own development going on for this problem. Looking through how similar these two problems are might prove very useful.\nI will meet with my supervisors next week and discuss our next steps in more detail. The fast.ai part 2 course is releasing soon and I would like to progress through it both for the sake of the project and my own interests."
  },
  {
    "objectID": "posts/2022-09-25-MP2.html",
    "href": "posts/2022-09-25-MP2.html",
    "title": "Masters Project 04/10/2022",
    "section": "",
    "text": "The second post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/2022-09-25-MP2.html#during-the-meeting-we-discussed",
    "href": "posts/2022-09-25-MP2.html#during-the-meeting-we-discussed",
    "title": "Masters Project 04/10/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nWhat I had been learning during the summer, namely the first fast.ai course, the state of exponential growth of machine learning, my website, etc. We are all happy with the progress I have made so far. I will estimate that I spent around 40 hours working this summer, ahead of the 30 hours the department set.\nFor example, I showed how I can upload machine learning models to HuggingFaceSpaces, which lets me send a link easily to Stuart, Robert, the biology dept etc and let them test and play with what I’m doing. This will be very useful as I physically will not need to take my computer over to get feedback on models.\nHow the project will be examined: by extended report (like a dissertation), a seminar, a supervisors mark (on your effort and from a viva), and also a provisional formative 10 page report. An external supervisor will be picked at random from 4 academics Stuart and Robert pick to read your report.\nStuart said to take the formative report seriously, because feedback on your writing style etc is really important. Your supervisors can’t give feedback on your real report later on due to department rules.\nWe will meet with Phil and Steve, from the biology department, next week, meeting at the TLC at 10:55.\nStuart has two audio recording devices that use SD cards to record. He said that he will order SD cards. I could use these to collect sound data if necessary. There is a large amount of data already collected, both by the biology department and available online, but if I wanted data collected in a specific way, then I could do it myself. For example, as we have two devices, I could set both of them up to gather the same audio, and use that to analyse out noise. What I could do, is use the same microphones the biology department did, and use them to remove noise from their whole dataset.\nOn a ambitious note, the biology department has been using community volunteers to look through photos and label animals in them. In a similar way, if this project is really successful, we could get a grant to lend out a specific recording setup to them and get a lot of specific data.\nRobert explained some possible project aims relating to biodiversity. For example, if we could classify birds by audio, we could look for birds that shouldn’t be in the UK or in specific areas. Birds that are migrating in the wrong direction, due to climate change or other environmental issues. This could tell us about biodiversity. If we had data of audio collected over months or years, then about how biodiversity is changing over time. I commented that if the biology department also recorded the GPS location of their recording device, it could be possible to map out how the number of specific birds changes in various areas.\nI asked Stuart whether any of his other supervision students are working on a similar problem to me. While he has many students, we are all doing varied projects which is interesting but hinders collaboration. For now, there should be a PhD student working with the biology team with I could work with.\nI explained that I want my diss to be written in a way it can be understood by anyone with no prerequisite knowledge. This stems from me expressing that the fundamental mechanisms in which machine learning works is actually very simple, it’s high school level maths and can be explained in one or two pages. This approach is good because it will make examination and marking easier.\nI explained that for frequent progress updates, I want to continue writing blogs on this website and link it to the teams channel. Right now one blog a week after the week’s meeting is probably a good pace.\nWe agreed on my plan to spend half my time working directly on the problem, and the other half going through the second fast.ai course starting Tuesday next week."
  },
  {
    "objectID": "posts/2022-09-25-MP2.html#this-week-i-want-to",
    "href": "posts/2022-09-25-MP2.html#this-week-i-want-to",
    "title": "Masters Project 04/10/2022",
    "section": "4.1 This week I want to:",
    "text": "4.1 This week I want to:\n\nFinish fast.ai part 1. Namely the questions and blog posts for the last 2 lessons.\nGo and investigate some of the project’s proposal’s references starting with scikit-maad and a CNN classifier.\nAsk for a reimbursement for fast.ai part 2"
  },
  {
    "objectID": "posts/2022-09-26-Lesson5Blog.md.html",
    "href": "posts/2022-09-26-Lesson5Blog.md.html",
    "title": "fast.ai Lesson 5",
    "section": "",
    "text": "2 Lesson Overview\nThis lesson focused on creating a neural network from scratch using Python and PyTorch. It covered much content present in the textbook chapter for the previous lesson. The lecture goes through how to initialise a neural net with one hidden layer, and then how to add more layers easily in order to create a deep learning model.\n\n\n3 The topics covered, briefly\n\nHow to build a neural network, from scratch.\nData cleaning: how to work with missing data and normalise.\nSigmoid function for classification.\nFeature engineering is important for tabular data.\nWhat is a binary split/OneR.\n\n\n\n4 Lecture Notes:\n\nFor data cleaning, when there is missing data fields, usually just use the mode. Other ways might be better, but the method is usually inconsequential compared to other factors.\nFor a linear model, so when there is only one equation with coefficients linking the inputs and the output, look at the coefficients to understand what inputs most contribute to the output.\nWe normalise the input data so that while training some inputs don’t overpower others. For example, if we wanted to predict lifetime duration with income and number of siblings, we have to make both variables around the same size initially so that income doesn’t just dominate the output mathematically.\nThe way we prepare input data is often similar for different types of models, e.g. classification and regression.\nIn contrast the way we modify the final output is very much dependent on the model. For classification, we apply a sigmoid function in order to get the outputs to between 0 and 1. Fast.ai does this automatically.\nFor tabular data specifically, feature engineering is really important. What the columns mean and how they relate to the prediction, can literally make and break models. If you can think of a creative way the columns relate and base your model around it, you could find a new way to make competition winning predictions. It’s not at all about just trying to teak different factors and hyperparameters to find the best predictions.\nFor quick iteration testing, you can keep the random seed for data creation/sampling the same. But for mature testing, make sure to vary it up because you need to know whether having different initialisations makes a difference, lest you accidentally overfit to them.\nEnsembles, where each network has different initialisations, can be really helpful to avoid overfitting. - Since random forest models (not deep learning) are hard to mess up, they might be a good baseline against deep learning and or more complicated approaches.\nA binary split attempts to find the best number for a variable to split the data into halves and make a prediction based on them. For example, if I wanted to predict whether someone lived in area 1 or area 2 by their income, a binary split would calculate a specific income and make all predictions above that income be area 1, and all predictions below that income, be area 2.\n\nGoing through the fundamentals of neural networks and building one from scratch atleast once I think is important to build a foundational understanding of what you are doing. It clears up that murky feeling that machine learning is some kind of magic. It is not, it really is just a set of logical instructions which actually doesn’t require more than high school maths. It reminds me of when I first had a proper maths lecture detailing integration. Previously I knew what it was and what the results meant, but the way the actual method of it worked felt like some sort of magic. One deep fundamental dive into it did well to make the concept click and clearly that murky feeling up felt amazing.\nIt’s worth just quickly clarifying that a deep learning model is just a neural net with three or more layers.\n\n\n5 Links\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson5.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6."
  },
  {
    "objectID": "posts/2022-09-26-Lesson6Blog.md.html",
    "href": "posts/2022-09-26-Lesson6Blog.md.html",
    "title": "fast.ai Lesson 6",
    "section": "",
    "text": "6: Random Forests"
  },
  {
    "objectID": "posts/2022-09-26-Lesson6Blog.md.html#the-lecture-than-goes-through-a-more-in-depth-notebooks-than-iterate-like-a-grandmaster-called-road-to-the-top-part-1-and-part-2",
    "href": "posts/2022-09-26-Lesson6Blog.md.html#the-lecture-than-goes-through-a-more-in-depth-notebooks-than-iterate-like-a-grandmaster-called-road-to-the-top-part-1-and-part-2",
    "title": "fast.ai Lesson 6",
    "section": "4.1 The lecture than goes through a more in depth notebooks than Iterate like a grandmaster called road to the top part 1 and part 2:",
    "text": "4.1 The lecture than goes through a more in depth notebooks than Iterate like a grandmaster called road to the top part 1 and part 2:\n\nJust squish images, it’s better than cropping. Padding would take more precious CPU power for iteration? (See point below). TTA, augmenting input images, can improve accuracy too.\nThere is another notebook: https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning, showing the best models for vision to use for quick iteration and testing.\nThe fastai learning rate suggestions are a bit conservative so you can pick higher than them.\nCreate different notebooks for different approaches. Duplicate and rename them.\nKaggle’s GPUs aren’t amazing, but for Jeremy his home PC ran training so much faster. The problem wasn’t GPU, it was CPU, the Kaggle CPU indicator showed it as full. This is a image input problem. Loading them up requires CPU power. Simply resizing the images by 1/4 quickened up iteration by 4x but without sacrificing accuracy! Perhaps larger image sizes aren’t so important. But later on, with some data augmentation, using larger image sizes helped.\nSince the GPU was barely used, you might as well switch to a model that’s more demanding. It probably won’t take much longer.\nKeep upto date with new model architecture. ConvNext is a great default speed and performance for now."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html",
    "href": "posts/2022-09-27-Lesson10Blog.md.html",
    "title": "fast.ai Lesson 10",
    "section": "",
    "text": "10: Stable Diffusion 2"
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#for-example-for-handwritten-digits",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#for-example-for-handwritten-digits",
    "title": "fast.ai Lesson 10",
    "section": "4.1 For example, for handwritten digits:",
    "text": "4.1 For example, for handwritten digits:\n\n7 + Noise = Noisy 7.\nGive Noisy 7 to unet, and unet predicts the Noise.\nIt then compares its prediction to the actual noise to get a loss to improve itself.\nTo make the unet work better, we pass in a label: an embedding of 7. For example, we could enter a one-hot encoded vector of it.\nIf we do this, then later on we can get the unet to create 7 of us because it understands what the embeddings of 7 are."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#to-train-these-encoders",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#to-train-these-encoders",
    "title": "fast.ai Lesson 10",
    "section": "5.1 To train these encoders:",
    "text": "5.1 To train these encoders:\n\nFirstly we find images online that are captioned (has a prompt).\nWe input an image into the image encoder, and this produces embeddings (a feature vector) of the image.\nWe input the image’s prompt into the text encoder, and this produces embeddings (a feature vector) of the prompt.\nWe then train both to get these two sets of embeddings to be the same.\nThis is done with contrastive loss: prioritising these two sets of embeddings to be the same for when an image and prompt match, and penalising when they don’t.\n\n\n\n\nimage.png\n\n\nBoth encoders now can produce embeddings, but we only care about the text encoder. It can take prompts of images and create embeddings for them. With this, we can input into our unet both an image and (an embedding of) its prompt!"
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#progressive-distillation-for-fast-sampling-of-diffusion-models",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#progressive-distillation-for-fast-sampling-of-diffusion-models",
    "title": "fast.ai Lesson 10",
    "section": "9.1 Progressive Distillation for Fast Sampling of Diffusion Models:",
    "text": "9.1 Progressive Distillation for Fast Sampling of Diffusion Models:\nLooking at the inference process, we can reduce the steps to denoise an image using distillation. Distillation is a pretty common technique in deep learning.\nLook at step 36 and step 54 of inference below:\n\n\n\nimage.png\n\n\nWhy is step 36 to 54 taking a whole 18 steps when there’s not much left to do comparatively to step 0 to 18 for instance?\nThe reason is, it takes very long because of a side effect of how the original maths of stable diffusion works.\nBut the thing is, after each step, we have an output image to play with! (we only plotted the output images every 6 steps).\nWhat if we used a new model, unet B. unet B can take the output of step 36, and try to create the output of step 54. We can then compare the two to train unet B to learn how to get from step 36 to step 54 directly!\nWe have a teacher network (it already knows how to do something, but is slow and big), and a student network (it tries to do the same as the teacher but faster and with less memory).\nOur teacher model is the original complete stable diffusion model. Our student model is the unet B skipping steps.\nGenerally speaking, we start with a noisy latent and the teacher model denoises it for 2 steps (it doesn’t create an image in 2 steps, just does 2 steps). Our student model then learns how to take the noisy latent and do the teacher’s 2 step denoising in 1 step.  Then we get the starting noisy latent and get the student model to denoise it for 2 steps.  And make another student learn how to do 2 steps of this in 1 step.\nThe original teacher model did some denoising in 2 steps. The first student could do that denoising in 1 step, so 2 steps of it would result in 4 of the original. The second student does the first’s in 1 step, so 2 steps of it results in 8 of the original. And so on, I suppose until you end up with 3-4 steps being enough to generate an image!"
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#on-distillation-of-guided-diffusion-models-paper",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#on-distillation-of-guided-diffusion-models-paper",
    "title": "fast.ai Lesson 10",
    "section": "9.2 On Distillation of Guided Diffusion Models Paper",
    "text": "9.2 On Distillation of Guided Diffusion Models Paper\n\n9.2.1 Classifier-free guided diffusion models (CFGD)\nClassifier-free guided diffusion models (CFGD) is a technique to control how strongly our output image matches our prompt. \nSay we want to create a photo of a cute puppy. We put “a cute puppy” into CLIP (its text_encoder) to get an embedding of the prompt. We then put this embedding into our unet with a pure noise latent to generate our puppy picture for us. This is normally how we do stable diffusion, but CFGD allows us to control how strongly this generated image matches our cute puppy prompt.\nHere’s how it works: Put an empty “” prompt into CLIP too to get another embedding. This embedding is particular because it represents nothing. If we did inference just on this, we would essentially be telling our unet “generate an image without any guidance, no restrictions, as long as it looks good”.\nInstead, we concatenate the ‘a cute puppy’ prompt embeddings with the blank “” prompt embeddings, and put them into our unet with a pure noisy latent. The unet outputs an image representing the real prompt, and another image representing the fake prompt. We combine these two images together. (I don’t understand why the unet produces 2 images, shouldn’t it just produce 1 image of them combined in the first place? That’s how the notebook code did it)\nTo recap, we just did the first step of inference, we took a noisy latent and removed a bit of noise to make it look more the embedding of the prompt “a cute puppy” concatenated with the embeddings of the prompt ““.\nWe then go onto doing inference as normal, removing noise step by step until we get our final image. The point of this is that, we can control how much the final image should rely on the real prompt versus the blank one. The latter is high guidance, the former is less. This allows us to control how strictly the real prompt is followed.\n\n\n9.2.2 The Paper\nThis way of doing CFGD is awkward because the unet has to output two images instead of the usual one, and for other reasons. The paper details a way to skip it. We do teacher student distillation again!\nOur normal stable diffusion with CFGD is the teacher model. It does CFGD with a number of different levels of guidance, 2, 4, 5, 12 etc. It does inference by starting with a noisy latent, then creating an image with guidance introduced. Our student model is another unet, unet B. Very similarly to how it worked in the previous paper, it looks at the different step outputs created by the teacher model, and learns how to do guidance like the teacher, but in a much more less awkward way.\nThere is an extra video on this, walking through the paper. It’s useful to watch this to learn how to read papers properly, as in, what’s important and such, in particular, the most important part is usually the algorithm."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#imagic-text-based-real-image-editing-with-diffusion-models",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#imagic-text-based-real-image-editing-with-diffusion-models",
    "title": "fast.ai Lesson 10",
    "section": "9.3 Imagic: Text-Based Real Image Editing with Diffusion Models",
    "text": "9.3 Imagic: Text-Based Real Image Editing with Diffusion Models\nThis is another paper that just came out 3 hours ago as of writing!\nGive it an input image, and pass in a prompt. It tries to edit the input image to match the prompt.\n\n\n\nimage.png\n\n\nThis paper is for imagen, a different image generation algorithm, but it also works fine for stable diffusion.\nGenerally speaking, it works through fine-tuning and optimising embeddings.\nWe start with a fully trained stable diffusion model.\nOur input image is\n\n\n\nimage.png\n\n\nWe want an image of it spreading its wings.\nUse CLIP to get an embedding of our prompt “a bird spreading its wings”. Use this embedding for inference as usual. This will create a photo of a bird spreading its wings, but not a photo of the bird from out input image spreading its wings.\nTo remedy this, we fine tune the prompt embedding to try and get it to create an image similar to the input bird image. We only do this a little and lock it, it cannot change any more. Now we have an optimised embedding.\nNow we fine tune the entire stable diffusion model to generate images that look like our bird from the input image. Now we have a fine-tuned diffusion model.\nFinally, we combine the optimised embedding and the original embedding for our target prompt, and pass it through the fine-tuned diffusion model to create our output image!\n\n\n\nimage.png\n\n\nMaybe it would only take like an hour to do this process using stable diffusion on GPUs!"
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#our-foundations-are",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#our-foundations-are",
    "title": "fast.ai Lesson 10",
    "section": "11.1 Our foundations are:",
    "text": "11.1 Our foundations are:\n\nPython\nMatplotlib\nThe Python Standard Libary\nJupyter Notebooks and nbdev\n\nTo be clear, after we have created a function, we’re allowed to use a module to do it. For example, we don’t start with Numpy arrays, but after we’ve made our own we can use them.\nThis is also how machine learning models will work. We cannot train full models on our own, so we will create and train small models, and then allow ourselves to use pretrained models online.\nThis is a challenge that will be different, but it’s often the part of the course where people learn the most. This year around it will be the best to date, and is more than worth doing.\nThe notebooks are from the part 2 repo on GitHub."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#useful-shortcuts-and-tools",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#useful-shortcuts-and-tools",
    "title": "fast.ai Lesson 10",
    "section": "11.2 Useful Shortcuts and Tools",
    "text": "11.2 Useful Shortcuts and Tools\n\nShift-m in Jupyter allows you to combine cells together.\nCntrl-shirt-minus allows you to separate them.\nAlt-enter inserts cells.\nRun a function then ? to get a brief description.\nRun a function then ?? to get documentation.\nInside a function, press shift-tab to see parameters available quickly."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#advice",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#advice",
    "title": "fast.ai Lesson 10",
    "section": "11.3 Advice",
    "text": "11.3 Advice\n\nRead the Python documentation a lot, it’s good.\nFor every single method, Jeremy reads the documentation and practices it."
  },
  {
    "objectID": "posts/2022-09-27-Lesson10Blog.md.html#notes",
    "href": "posts/2022-09-27-Lesson10Blog.md.html#notes",
    "title": "fast.ai Lesson 10",
    "section": "11.4 Notes",
    "text": "11.4 Notes\nI made notes on the lecture, then went through the notebook myself and rewrote the notes while running through and testing the cells."
  },
  {
    "objectID": "posts/2022-09-27-Lesson7Blog.md.html",
    "href": "posts/2022-09-27-Lesson7Blog.md.html",
    "title": "fast.ai Lesson 7",
    "section": "",
    "text": "7: Collaborative Filtering"
  },
  {
    "objectID": "posts/2022-09-27-Lesson7Blog.md.html#going-through-road-to-the-top-part-3",
    "href": "posts/2022-09-27-Lesson7Blog.md.html#going-through-road-to-the-top-part-3",
    "title": "fast.ai Lesson 7",
    "section": "4.1 Going through road to the top, part 3:",
    "text": "4.1 Going through road to the top, part 3:\n\nA larger model has more parameters so can find more features, but the problem is that it takes GPU memory that isn’t as flexible as CPU memory.\nHow to use as large a model as you like without worrying about memory. For example, Kaggle has 16Gb GPUs.\nYou can first find out how much memory a model uses. What’s important is that training for longer does not actually require more GPU memory.\nGradient accumulation is how: Run smaller batch sizes, but modify them as to act and train as if we were using the same normal batch size for all the training data.\nGradient accumulation results are identical to using a higher memory GPU for certain models. It is for convText and Transformers (NLP). If a model uses batch normalisation, then it won’t exactly, it will have different results, but probably still good ones.\nPick a batch size that fits your GPU memory, and generally higher and a multiple of 8 is better. Generally (not always) if you double batch size, half your learning rate.\nWe can use ensembles of good models of different architectures, and get even better results. Furthermore, we can add in bagging too to train them on different sets of the training data.\nAt the start it may feel random as to why certain approaches/models are better, but over time as you develop intuition, it will feel be less random and more systematic.\nGenerally, it makes sense to iterate on small models then switch to large models, but there’s a better way of ensuring this performance converts correctly. This is covered in the second course."
  },
  {
    "objectID": "posts/2022-09-27-Lesson7Blog.md.html#going-through-road-to-the-top-part-4",
    "href": "posts/2022-09-27-Lesson7Blog.md.html#going-through-road-to-the-top-part-4",
    "title": "fast.ai Lesson 7",
    "section": "4.2 Going through road to the top, part 4:",
    "text": "4.2 Going through road to the top, part 4:\n\nWe want a model to now predict two things instead of one I.E, two dependent variables instead of one. E.g. from a rice photo, the type of rice (10 types) and the disease it may have (10 types), so there are 20 categories.\nThis requires an understanding of making custom loss functions and a deeper look into how cross entropy loss works.\nMake a learner just for the first dependent variable, disease, and create a specific metric function for it.\nCross entropy loss: Jeremy states it is really important to understand and so goes into the maths using a separate excel sheet. He tries to predict if a image is a cat,dog,plane,fish or building, so there are 5 classification categories. It outputs 5 numbers (relating to probabilities of each category). CEL first finds the softmax value for each of them. Then it compares the actual value (1 for the correct category, 0 for not) to the softmax value for each category. It multiplies the log of the probability prediction for the correct category by the actual value.\nFurther info: https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/\nBinary cross entropy is just cross entropy for 1 category: is a cat or not. Careful here, it’s not for 2 categories e.g. cat or dog.\nThe loss functions in our python environment has two types. The F function type and the nn class type. The latter has more parameters to play with.\nChange the last node outputs to be the number of categories predicted instead of the usual 1 for classification.\nYou encode the loss function for the model to know what/how many categories to predict. You sum loss functions for multiple category types and their sub categories.\nThis new model, that can predict 20 categories, actually is better than a model that just predicts disease type! This is because the training to do other types of predictions helps. Sometimes this approach is better, sometimes not!"
  },
  {
    "objectID": "posts/2022-09-27-Lesson7Blog.md.html#collaborative-filtering-deep-dive",
    "href": "posts/2022-09-27-Lesson7Blog.md.html#collaborative-filtering-deep-dive",
    "title": "fast.ai Lesson 7",
    "section": "4.3 Collaborative Filtering Deep Dive:",
    "text": "4.3 Collaborative Filtering Deep Dive:\n\nCollaborative filtering is a key part of recommender systems.\nWe use the Movielens dataset of movie ratings with 3 columns: UserID, MovieID, and rating.\nImagine a matrix of the users and their movie ratings. There are missing values for unrated/unseen movies. Collaborative filtering is just trying to fill in these missing values to complete the matrix.\nThe problem is predicting how a user will rate an unrated movie for them. We want to match up the user’s movie preferences with the movie’s features to predict this.\nBut we don’t know their preferences and the movie features, these are called latent variables. We only have their ID, their previous ratings, and those of other users. We can however infer a user’s preferences and a movie’s features from this data.\nLet’s assume there are 5 latent factors, say like for a movie, it’s genre, length etc, we don’t set these, we calculate them and then can try and interpret what they are.\nOn choosing the number of latent factors, its hard. Fast.ai has a function to calculate this based on Jeremy’s intuition, but you can play around too.\nUse SGD to optimise these latent factors after we set a loss function.\nWhat is embedding? Just looking something up in an array. An embedding matrix is the array that is looked up. Matrix multiplication in an embedding matrix is the same as looking up index values in a list say as a function in excel. Think about a dot product with a one-hot encoded vector, it just returns the value you’re looking up.\nWe then cover how to create a collaborative filtering model from scratch using python, PyTorch class definition, and features.\nWe create a DotProduct class to define embeddings and looking up values for UserIDs and MovieIDs.\nSome of our user rating predictions can greater than 5, the maximum. Take our predictions and squish them with a sigmoid to fix this.\nWe noticed that some users just relates all movies highly, while some users have a range of ratings. Let’s incorporate this into our model predictions. To do so, we make another inference variable, a movie bias and a user bias, reflecting that for movies, they tend to be especially related well or badly, and that for some users, they can rate all movies generally as good or bad.\nIt’s not covered, but I think we could try and cluster users instead to try and incorporate user and movie types/preferences?\nWe can use L2 regularisation (weight decay), to avoid overfitting. This adds the sum of the square of the weights to the loss function. This also solves the issue of having useless interfered variables, because they won’t contribute. I suppose this makes getting the exact number of inference variables less important.\nIn fast.ai, usually defaults are good, but for tabular data, it’s hard to know good defaults, so it’s good to test yourself."
  },
  {
    "objectID": "posts/2022-09-27-Lesson8Blog.md.html",
    "href": "posts/2022-09-27-Lesson8Blog.md.html",
    "title": "fast.ai Lesson 8",
    "section": "",
    "text": "2 Lesson Overview\nThis is the last lesson of fast.ai part 1. As of writing, fast.ai part 2 starts next Tuesday, and I have already registered for it. I’m very excited for the second half of the course, and really thankful that things fell right into place with my Masters project so that I can dedicate the time and effort towards taking it seriously.\nThis lesson covers embedding matrices like the previous lesson. The emphasis on embedding matrices makes me believe they are quite important. CNNs are also covered, and again like basic neural networks, they end up fundamentally being pleasantly and surprisingly simple. Lastly Jeremy gives some advice about what to do before the second half of the course.\n\n\n3 The topics covered, briefly\n\nMore about embedding matrices: about inference parameters, PCA and embedding distances.\nA combined approach of a dot product model and a neural network is the best for collaborative filtering.\nTransferring the embedding matrix from a neural network to other models can give a nice performance boost.\nHow CNNs work for image classification: how convolutions are made with filter matrices, how these filter parameters are found with SGD, what stride is, max/avg pooling, how dropout can improve generalisation.\nChannels refer to both the input data, but also to the number of activations per grid after a convolution. This second definition is also how features and filters are defined.\nWhat padding is: to avoid losing data on edges.\nWhat stride is: stride 1 is making the conv layer the same grid size as the previous; stride 2 is skipping over.\nWhy we double channels/filters/features after a stride-2 conv layer.\nRefactoring is defining functions for your neural network layers to state its parameters explicitly. E.g. defining a function for convolution layers.\nWhat is a “receptive field”? The area of the image involved in the calculation of (convolutional) layers.\nVarious visualisation plots to understand how the training process is going.\nAbout learning rates: the benefits and drawbacks of both high and low learning rates. 1cycle and cyclical momentum as smart techniques to modify your learning rate dynamically.\nWhy activations near zero are problematic. Most obviously, having a final layer of just 0s has no information and so is useless for classification.\nBatch Normalisation as a technique to lessen the number of activations near zero.\n\n\n\n4 Lecture Notes\n\nJeremy is back to the previous collaborative filtering notebook:\nFor the embedding matrix, PyTorch keeps track of neural network parameters/weights for you.\nmovie_bias is the interference parameter we created in order to add bias for movies, user_bias is the one we created in order to add bias for users.\nHow does our model work? To predict, it trains as normal on the input ratings and output movie ratings, but then we add meaningful interference parameters, movie_bias and user_bias for each movie and user respectively, to adjust with domain specific knowledge. Some movies just everyone likes, so are biased, some users just like every movie, so are biased.\nVisualising embeddings: Plotting a principle component analysis of PCA component 1 and 2, gives a compressed view of how our latent factors affect eachother. It gives us a graph to interpret how the top latent factors are related to eachother. Domain specific knowledge would tell us what there PCA components/inference factors are, and we can try and understand why they relate the way they do.\nEmbedding distance: calculate how far apart each embedding vector is from a specific movie, aka how similar each movie is compared to that movie based on the latent factors.\nUsing deep learning for collaborative filtering instead: We use a neural net to try and enter the missing values in the matrix.\nWe use fast.ai to try and find the embedding matrix side/number of latent variables. Fast.ai does it based on Jeremy’s intuition, there’s not a easy way to know how many we should use, although weight decay might give some leeway in having too many.\nIn practice, a combined approach of a neural net and dot product approach is best for collaborative filtering.\nIn collaborative filtering, often a small number of users and movies overwealm the rest. For, anime some users watch so much anime compared to other users, and this makes predictions for normal users biased towards predicting for these enthusiastic users. It’s a higher level task to try and resolve this.\nEmbedding in NLP: an embedding matrix is just a matrix of latent variables for every word. And as before, the embedding distance calculates the embedding distance between a word and other words, telling us how similar they are based on latent variables.\nFor tabular data, using a tabular model/learner for it, creates and uses an embedding matrix.\nCollaborative filtering, NLP, tabular data neural nets, all also use embedding matrices.\nUsing embedding, latent variables, may be a substitute to doing a lot of feature engineering.\nCreate and train a neural net, take its embeddings and use it with other models like random forests, it can give a nice performance boost. This is akin to letting the neural network find relationships in the data, and using those relationships to boost another model’s performance.\nOne latent variable found to help predict retail store sales was distance in real life. Actually the embedding matrix’s distances for that latent variable matched the distances in real life! It managed to learn that real world distance was important for predicting retail sales and reflected that! Latent variables can find amazing relationships about the data!\nJeremy then goes on to cover how CNNs function:\nConvolutions: A CNN is similar to the neural networks before, but for computer vision they have a particular difference: they can separate out the horizontal and vertical lines of images into convolutions.\nHow? It has a filter matrix, say a 3x3 one, and moves around, dot producting and RELUing each 3x3 subset matrix of the original image, producing a smaller image. This is called a convolution.\nWhy does this work? Because of the values in the filter. E.g. we used a 3x3 matrix with the top elements = 1, middle elements = 0, bottom = -1. The dot product of this will only give the highest value when there’s horizontal image in the top row, anything in the middle, and no image in the bottom, aka whenever there’s a horizontal part of an image! In vertical parts, the bottom row will cancel out the top and produce 0s, aka the convolution layer will have nothing.\nThe 3x3 matrix is called a filter.\nWe them use another filter, but this time with two filter matrices, one for our vertical layer and one for our horizontal layer. We do this to combine their features.\nIn practice, if we wanted to use a CNN to find the best way to make convolutions to classify an image, we wouldn’t know what filter matrix values to use. Previously we just hard coded for a vertical and horizontal convolution, but we want the CNN to find more complex and useful convolutions.\nThe way we find these filter matrix/kernel values is to set them as parameters and use SGD to optimise. In our example, it will find the good kernel parameters and thus convolutions for digit classification.\nNowadays we do a stride convolution, e.g. stride 2, to skip values of the original image. This reduces the grid size by 2x2. The grid size is not reduced by the kernel size, it is reduced determined by the stride we set.\nWhen we’re done with stride convolutions, with about a 7x7 image at the end, we do an average pool: we average the final values (called activations).\nA max pool instead finds the max value of each activation rather than averaging them.\nSay we’re trying to identify a picture of a bear. If the bear is a small part of the picture, max pool is much better than avg pool for classification. By checking each end activation for a bear, rather than having one prediction of bear or not bear for all the activations combined, it can spot the bear’s small presence.\n\nDepending on how you want your model to work, you should pick max or avg pool accordingly. fast.ai does this for you, it actually does an average of max/avg pool and tries to find the best for you.\nAll the kernel multiplies done for a convolution layer is mathematically equivalent to just one big matrix multiplication.\nDropout: we can add a dropout mask which can improve generalisation.\nWe multiply the mask by our filtered image before we do convolutions to delete parts of it. Higher dropout means the image is harder to see as there is less of it.\nThe motivation is as follows: a human is able to look at a image with pieces missing and still classify it. A model should be able to as well. If we use dropout, perhaps it forces the model to learn more fundamental features about our images, more resilient and generalisable ones.\nThere are many more activation functions/activations than ReLU, but in practice it doesn’t make a huge difference so it’s not worth working on it too much.\nJeremy then advices a few things to do before the second half of the course:\nRead the book Meta Learning: How to Learn Deep Learning.\nWatch the videos again and code and experiment as you go.\nSpend time on the forums.\nGet together with others to study together.\nBuild projects.\n\n\n\n5 Questions\nWhat is a “feature”?\nA transformation of the data which is designed to make it easier to model. Feature engineering is just making new transformations. For example, in the titanic dataset, we could make a new feature, the number of family members a person has, and this could help the model’s predictions.\nWrite out the convolutional kernel matrix for a top edge detector.\n-pass-\nWrite out the mathematical operation applied by a 3×3 kernel to a single pixel in an image.\nI’m confused, a kernel doesn’t apply to a single pixel, a 3x3 kernel is applied to 9 pixels, albeit centered on one. The maths for this is just a dot product.\nWhat is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\n0\nWhat is “padding”?\nThe sides of the image will have the kernel try and use pixels out of bounds. We don’t want to simply lose these sides, so we apply padding, usually just 0 pixels so the kernel has values to use.\nWhat is “stride”?\nStride-1 is just simply applying the kernel to a centered pixel, then moving one pixel away and doing it again. Stride-2 is moving the centered pixel two pixels away every time. Stride-1 is useful to add convolutional layers without changing input size, noting that it’s the stride, not the kernel size, that affects the convolution layer’s dimensions. Stride-2 is useful to decrease the size of our outputs.\nCreate a nested list comprehension to complete any task that you choose.\nNested loops follow the syntax: print(((i for j in range(1,5)) for i in range(1,5))) which is just for j in range(1,5): for i in range (1,5): print(i) print((i for i in list1 for j in list2))\nWhat are the shapes of the input and weight parameters to PyTorch’s 2D convolution?\ninput:: input tensor of shape (minibatch, in_channels, iH, iW) weight:: filters of shape (out_channels, in_channels, kH, kW) iH and iW are just the height and width of the image. kH,kW are just the height and width of the kernel. in_channels and out_channels are just the number of input and output channels.\nWhat is a “channel”?\nA channel is a single basic colour in an image. RGB images have 3 channels, red green and blue. I imagine a 3 channel image like 3 sets of pixel maps, each for a channel, or one pixel map with 3 separate values we can operate on. However channels don’t just refer to the input data as described, but also to the number of activations per grid after a convolution. This second definition is also how features are defined.\nWhat is the relationship between a convolution and a matrix -multiplication?\nA convolution mathematically is just a special matrix of the kernel’s values, multiplied by the pixels, with a bias matrix term added. I.E. kM + b\nWhat is a “convolutional neural network”?\nFor a neural network, when we use convolutions instead or in addition to linear layers.\nWhat is the benefit of refactoring parts of your neural network definition?\nRefactoring is defining functions for your neural network layers to state its parameters explicitly. E.g. defining a function for convolution layers. Refactoring makes it much less likely you’ll accidentally make errors in your architecture and also makes it easier for your user to understand how your layers are constructed.\nWhat is Flatten? Where does it need to be included in the MNIST CNN? Why?\nThe final Conv2d layer has a output tensor shape of 64x2x1x1, but we need to remove the extra 1x1 layer to get 64x2x1, so we use flatten to do so. This is similar to squeeze in PyTorch.\nWhat does “NCHW” mean?\nOur input’ shape is 64x1x28x28: batch,channel,height,width. Meaning a batch of 64 one channel (colour) images of 28x28. NCHW refers to this.\nWhy does the third layer of the MNIST CNN have 77(1168-16) multiplications?\nThe third layer output has the shape 64x16x4x4, so has 16 channels, and so has one bias for each channel. The input shape from the previous layer is 64x8x7x7. For each input pixel (7x7), we multiply by the number of parameters minus the bias weights (1168-16).\nWhat is a “receptive field”?\nThe area of the image involved in the calculation of (convolutional) layers. For example, if we select a pixel on the second convolutional layer, we can see the pixels used to calculate it in the first layer, and then further still the pixels used to calculate those in the original image.\nWhat is the size of the receptive field of an activation after two stride 2 convolutions? Why?\nMathematically, if you apply a 3x3 kernel with 2 stride to a 7x7 area, then do it again, you get one pixel left.\nRun conv-example.xlsx yourself and experiment with trace precedents.\n-pass-\nHave a look at Jeremy or Sylvain’s list of recent Twitter “like”s, and see if you find any interesting resources or ideas there.\nJeremy retweeted a kaggle competition where the top teams and winner used fast.ai for their image classification!\nHow is a color image represented as a tensor?\nA rank 3 tensor. e.g. (3, 1000, 846), 3 colour channels, 1000x846 image size. You then can go into the 3 colour channels and print their individual pixel maps.\nHow does a convolution work with a color input?\nA convolution takes an image with a certain number of channels, and outputs an image with a different number of channels. For a channel 3 image, we have 3 different kernels, and apply them, then add the result together.\nWhat method can we use to see that data in DataLoaders?\ndls.show_batch\nWhy do we double the number of filters after each stride-2 conv?\nA stride-2 conv halves the grid size from 14x14 to 7x7. Say the original image was 14x14, then the convolution would be only 7x7. We double the number of filters to avoid this. Filters are just channels, also called features. So say we apply stride-2 to a 14x14 3 channel image, we then end up with a 7x7 grid size with 6 channels.\nWhy do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\nNeural networks can only create useful features if the number of outputs from them is lower than the number of inputs. It has to condense information to create useful features. That’s why we use a large kernel, because it keeps the number of outputs meaningfully lower than the number of inputs, particularly because if we double the number of filters each time we have a stride-2 layer, we’d be using nine pixels to calculate eight outputs.\nWhat information does ActivationStats save for each layer?\nIt records the mean, standard deviation, and history of the activations for each layer, so that we can look into our model’s training process and improve it.\nHow can we access a learner’s callback after training?\nlearn.activation_stats.plot_layer_stats(0) prints out useful plots of the first (0) layer for us.\nWhat are the three statistics plotted by plot_layer_stats? What does the x-axis represent?\nThe mean, std, and % of activations with a value near 0. x-axis is just the frequency (number) of activations.\nWhy are activations near zero problematic?\nWe don’t want activations to be 0 or near it. Multiplying by 0 gives 0, which means if a early layer has some 0 activations then latter values will too. Multiplying by 0 means we have computation occuring that doesn’t do anything. If our final layer is just 0s, then it’s not very useful to classification since it’s just empty.\nWhat are the upsides and downsides of training with a larger batch size?\nLarger batches have a more accurate gradient to update the loss with, although they also incur fewer batches per epoch, with means the model weights are updated less frequently. The latter can result in slower training, but it depends on your GPU and other factors.\nWhy should we avoid using a high learning rate at the start of training?\nBecause the initial weights were initialised randomly, a high starting learning rate could result in the training being ruined at the start very quickly by going in a very wrong direction. Also: a lower learning rate throughout would result in ending training at a local minima, while a high learning rate would skip over them.\nWhat is 1cycle training?\nTo avoid the previous problem, we want to start with a low learning rate, but when the training has settled some, we want to increase the learning rate to speed up the training, but near the end we want a low learning rate again to avoid accidentily skipping over loss minima.\nThis is where 1cycle training comes in: it splits the learning rate into a warmup and annealing phase. The former starts low and increases to a maximum, and the latter decreases from the maximum back to the lowest.\nWhat are the benefits of training with a high learning rate?\nA high learning rate trains quickly and can avoid being stuck in local loss minima.\nWhy do we want to use a low learning rate at the end of training?\nBecause we want to converge on a final loss minima, but a high learning rate would skip over it.\nWhat is “cyclical momentum”?\nMomentum is when the optimiser goes in the direction of the gradients as usual, but follows the direction of the previous loss updates.\nCyclical momentum is varying momentum in the opposite direction of the learning rate. When we are at high learning rates, use less momentum, when at low learning rates, use more momentum.\nWhat callback tracks hyperparameter values during training (along with other information)?\nplot_sched\nWhat does one column of pixels in the color_dim plot represent?\nEach vertical slice of the color_dim plot, the plot with the colours changing, represents the histogram of activations for a single batch.\nSo looking at colour_dim shows us how the activations in the CNN layers shows as training progresses. More specifically, as each vertical line represents the activation histogram for each batch, we see how the activations change after each batch.\nWhat does “bad training” look like in color_dim? Why?\nDark blue is bad training, at the start all activations are zero. Yellow is better, as there are near-zero activations instead of just zero. But we see over training it gets better as the colour changes so we have less zero activations! But our training is bad because it oscillates in colour, it doesn’t just get better.\nWhat trainable parameters does a batch normalization layer contain?\nFor our model to work, we need to fix the initial large zero/near-zero activations and maintain not so many during training. Batch normalisation is a solution to this, it has two parameters, gamma and beta to do so.\nWhat statistics are used to normalize in batch normalization during training? How about during validation?\nDuring training we normalise the training data with the mean and standard deviation. During validation we instead use the running mean of the stats calculated during training.\nWhy do models with batch normalization layers generalize better?\nWe’re not entirely sure yet but probably because each batch adds some more randomness to the training process. Each mini-batch will have a somewhat different mean and std from the others, so the activations will be normalised differently each time. Thus we force the model to cope with with variations and it improves generalisability.\n\n\n6 Links\nThe course page for this sessions is https://course.fast.ai/Lessons/lesson8.html, which includes a lecture, notebooks, and a set of questions from the course book."
  },
  {
    "objectID": "posts/2022-10-06-MP3.html",
    "href": "posts/2022-10-06-MP3.html",
    "title": "Masters Project 11/10/2022",
    "section": "",
    "text": "The third post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/2022-10-06-MP3.html#during-the-meeting-we-discussed",
    "href": "posts/2022-10-06-MP3.html#during-the-meeting-we-discussed",
    "title": "Masters Project 11/10/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nHow to introduce more Physics into the project. This would be done namely using stereo audio, which is audio recorded in 3 dimensions. The idea is that using the Doppler effect and other Physics concepts, we could come up with a better audio collection technique that would make the model classification better. However, it may take a lot of time and effort to understand how to setup multiple microphones and analyse their data, given that there are features such as their synchronisation, batteries, and calibration. In total we have around 90 microphones? Because stereo is 3D, we could use it to classify where a bird singing is, and multiple birds singing. Would also need to account for audio reflecting off surfaces.\nA previous masters student at Durham, Stuart, worked with the biology department to create his thesis which may contain useful domain knowledge for this project. It can be found at http://etheses.dur.ac.uk/11481/. He is now doing a PhD in Scotland and his further work may be worth looking into, or he may be worth directly getting in contact with to collaborate.\nThe biology department have published a paper 3 years ago about using a ensemble models to classify the cocktail problem. While the models and approach may be outdated, the data preprocessing steps might be similar to what I would have to do now.\nI would have to have a look at the collected datasets to get an idea about their properties and train accordingly. Some of their data was painstakenly hand labeled.\nFor humans, some bird song is easier classified by image, and some by sound. I suppose this is why ensembles are a good approach.\nFor the aims of this project, I would need to define a custom metric for the biology department’s needs. For example, if we really want a certain species of bird, we could penalise false positives of it very harshly.\nAbout volunteer interaction: some birds are really hard to classify that even professors struggle. If you were to have a volunteer system, you should have a delegation system to assign hard tasks to experienced users.\nBecause humans have to listen to the same audio file multiple times to recognise birds, I wonder if we can train or apply a model this way.\nTraining on bad quality data, which xeno-canto labels, might be a good idea to improve generalisability.\nSome bird species have different times at which they sing in the day, so it could be worthwhile including this in the training data somehow.\nAn interesting problem is say, looking at how the frequency of a rare species changes over time in a given area. Since we have a dataset collected over 3 years, this in theory would be possible."
  },
  {
    "objectID": "posts/2022-10-06-MP3.html#there-are-two-separate-and-related-problems-for-classifying-birdsong",
    "href": "posts/2022-10-06-MP3.html#there-are-two-separate-and-related-problems-for-classifying-birdsong",
    "title": "Masters Project 11/10/2022",
    "section": "3.1 There are two separate and related problems for classifying birdsong:",
    "text": "3.1 There are two separate and related problems for classifying birdsong:\n\nThe easier problem, trying to classify birds from an audio recording of their song in a vacuum.\nThe harder problem, trying to classify birds from an audio recording of their song in a noisy environment.\n\nMy Masters project is attempting the latter problem. However some testing and knowledge of the former problem is worthwhile regardless."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html",
    "href": "posts/2022-10-11-L9Blog.md.html",
    "title": "fast.ai Lesson 9",
    "section": "",
    "text": "9: Introduction to Stable Diffusion"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#part-1-overview",
    "href": "posts/2022-10-11-L9Blog.md.html#part-1-overview",
    "title": "fast.ai Lesson 9",
    "section": "4.1 Part 1, Overview:",
    "text": "4.1 Part 1, Overview:\n\nGPU needs have increased for this course. Funnily enough, Collab prices have skyrocked as everyone is using it for SD. There are four options: Colab, Paperspace Gradient, Lambda Labs, Jarvis Labs. Lambda GPU is the cheapest with an offer:$1.10/hour. Lambda is cheaper than everything else but it nor collab allow you to pause.\nAn issue is, if we use paperspace or lambda, the pipeline we downloaded will be saved, and sometimes it can take hours to download, but if we use collab then it won’t.\nTraining GPUs need 16/24 gb memory. My laptop is only 12gb, and it’s probably not simple to optimise it for ML. I might be able to do everything that isn’t training on my own PC, but I need a web service.\nStable diffusion is moving so quickly, this lecture is already outdated. Literally in the last 24hrs before the lecture, 2 crazy new papers came out.\nThe first paper reduced the number of steps required for SD to 250 from 1000!\nThe second paper changed SD to make it 10x-20x faster!\nI wonder if this is a result directly from SD’s open source nature. The research community may well be able to improve SD much faster then say OpenAI can improve DALL-E.\nBut after this first lesson, we go into the foundations which won’t change often, and we’d be able to understand the updated research’s details ourselves.\nWe can fine-tune SD using DreamBooth, to put any object or person into an image! strmr.com is a service to do so, which costs $3.\nJeremy links the Diffusion-nbs notebook and repo. It contains many things to play with. There’s notebooks with tons of parameters we don’t yet understand. I think this is the philosophy that, to understand a black box we should first get intuition for it’s inputs and outputs, then kneed into the details.\nIt’s not actually easy to know what prompts to give SD. The best way to learn is to look at other people’s prompts and outputs. lexica.art gives plenty examples."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#part-2-notebook",
    "href": "posts/2022-10-11-L9Blog.md.html#part-2-notebook",
    "title": "fast.ai Lesson 9",
    "section": "4.2 Part 2, Notebook:",
    "text": "4.2 Part 2, Notebook:\n\nFirst start by cloning the Stable Diffusion notebook.\nDiffusers is the HuggingFace module for this. HuggingFace has had really good packages for a while now, so using them is a good idea generally.\nWe use the SD pipeline. A pipeline is similar to a fast.ai learner. It does many things automatically for us. We can also save pipelines like we save learners.\nUnlike leaners, we can save pipelines and upload them into HuggingFace’s cloud server. Like this, we can upload our own piplines and share them, or browse others’ and download them.\nIf we use the same pipeline with the same random seed, we get the same result. This is just like MineCraft world seeds.\nGenerally, diffusion works as follows: we start with random noise, and each step we get slightly less noisy and towards the result we want. Through many steps, E.g, 50, we get our image. Although after yesterday’s research it’s now only 3-4 steps!\nBut why don’t we just do it in one step? Our models aren’t smart enough, but considering we started with 1000 steps and now it’s 3-4, maybe at some point they will be.\nWe took the exact same prompt, four times, and pass them into our pipeline.\nThe pipeline has a variable, guidance_scale, g, meaning intuitively: ‘to what degree should we focus on the caption versus just creating the image’.\nToo low guidance won’t make images of the prompt, too high could be strange depending on the implementation of SD.\nIt works by making an image without guidance, then image with guidance, and finding the average of them. ?\nNegative prompts work similarly. Make two images, and subtract one from another. Say we want to remove blue from the image. Subtract an image from the prompt ‘blue’ from it.\nInstead of passing prompts in SD, we can pass images!\nTo do so, we use image to image pipelines. Instead of starting diffusion with random noise from scratch, it starts with a noisy version of the input image, using it as a guiding point. It uses a parameter, strength, which is similar to guidance I think, to exert how strongly SD should follow the input image image. This creates a variant of our original image as we’d like.\nWhat if we did this multiple times? Let’s take an image and pass it into SD, then pass the resulting image into SD again! We can use this to change styles of images for example!\nFine-tuning: We take a pretrained model (from a pipeline), cut off the head, and fine tune it with our own images and captions. This will let us create images of things the pretrained model hasn’t encountered, like pictures of our friends. Lambdalabs have a blog post guide for it.\nTextual inversion is a special type of fine-tuning. Create a new model embedding for some concept in some images we have. E.g. ‘watercolour’, and add that token to the text model, and train embeddings for it using our images. Then we can have use the prompt “Woman in the style of ” to create it!\nDreambooth is similar to textual inversion. Instead of making a new embedding, it finds a existing token in the embeddings that is barely used, and fine tunes it.\nNot sure why/if Textual inversion or Dreambooth is better.\nTextual inversion example. Say we have images of a teddy bear and we want to create a novel image of it riding a horse. Our model couldn’t end up generating it. It was however able to generate a picture of our teddy sitting on a carpet though. The point is, sometimes textual inversion can fail."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#part-3-conceptual-overview",
    "href": "posts/2022-10-11-L9Blog.md.html#part-3-conceptual-overview",
    "title": "fast.ai Lesson 9",
    "section": "4.3 Part 3, Conceptual Overview:",
    "text": "4.3 Part 3, Conceptual Overview:\n\n4.3.1 SD Model:\n\nImagine that we wanted to generate hand written digits using SD. Start by assuming there’s some function,f, that takes an image,X1, and returns the probability that it’s a handwritten digit, P(X1).\nWe get various probabilities returned from f. Say 0.98, 0.4, 0.02.\nWe can use f to generate handwritten digits! If you have a function that can classify/match images to labels/vectors, you can create a model to do the inverse, create a new image to match the labels/vectors.\nImagine we had a mess of an image that is supposed to be an handwritten digit. It’s 28x28, 784 px. We pick one of these pixels, and make it a little bit lighter or darker. We then pass it into f and see how the probability changes. If it increases, then we’ve gotten something that’s more like a handwritten digit. If we do this enough times for every pixel, then we could create an image of a digit, even from starting from pure noise!\nWe should optimize this process using gradients.\nStart by finding the gradient of the probability that our input image, X, is a digit, w.r.t the gradient of X’s pixels. The gradient represents, ‘how much does the probability X is a digit increase as we increase the pixels’ values’. Our gradient will have 784 values, one for each pixel.\nThen we change the image pixels using this gradient. Take every pixel and subtract it by it’s own gradient w.r.t the probability, multiplied by a constant (like a learning rate), C. In maths: X + C*(Gradient)\nThis method, finding the derivative of every pixel, so finding 784 derivatives, is called the finite differencing method of differencing.\nBut we need not do this, it’s slow and computationally expense. Instead, simply use f.backward() in Python to use analytics derivatives to do it super quick. X.grad from f.backward() gives us all 784 gradients.\nThe we simply repeat X + C*(Gradient) enough times to get our result: turning a noise image into a handwritten digit.\n\n\n\n4.3.2 The Probability Function, f\n\nBut how do we get our probability function f?\nUsing a neural network, of course!\nWe first get training data, that is, images of handwritten digits with a random amount of noise added to them.\nWe then train a neural network to try and predict how much noise is in a training image, I.E, what part of the image is noise and what part is not.\nA way to think about it is sketched below, we input the first image, and our network tries to predict the third. For the first example, the digit 9 has no noise. 7 has some noise. 3 has much noise, 6 further so.\n\n\n\n\nimage.png\n\n\n\nWe use can use MSE as our loss function for this network.\nWe can calculate it by adding the LHS together (remember that we have the non-noisy input images), and seeing how it compares to the input noisy image.\nBut wait, this process can actually just be used to generate an image for us instead of using a probability function f. \n\n\n\n4.3.3 Unets\n\nWe’ll call use a unet for this network from now on and call it that.\nIf we pass an image of pure noise into our unet, it returns info about what parts of an image it thinks is the noise for a handwritten digit image. Like, ‘if we left behind these pixels that aren’t noise, it would look a little bit more than a handwritten digit’.\nIf we did this multiple times, we could turn pure noise into a handwritten digit!\nNoise itself is defined such that, if we subtract the noisy input image by the noise, we will get an image without/with less noise.\n\n\n\n4.3.4 Compression\n\nThe issue is that we have too many pixels to do this practically.\nFor example, a standard 512x512x3 image is 786432 px! And if we had millions of images, this is impossible unless you’re Google!\nBut we can do this more efficiently. Storing the exact pixel values isn’t the most efficient way to store them. For instance, if we had a line of just blue pixels, why not store just info saying that. Jpg files are an example of compressing images.\nLet’s compress our image in a particular way. Put the image through a convolution layer of stride 2, and it’ll result in an 256x256x6 image. Then again, 128x128x12, then again, 64x64x24.\nNow a few resnet blocks to get 64x64x4. The number of pixels left is 16384, which is 48x less than the original!\nThis is pointless if we can’t reconstruct our full sized image well.\nWe can go backwards! Take our 64x64x4 image into a inverse convolution, giving us 128x128x12, then again and again back to 512x512x3, our original image.\nThis entire thing, both the compress (encoder) and decompress (decoder) is one neural network called an autoencoder, or in this case, a VAE.\nThe reconstruction part, the decoder, will give us random noise back, so we need a loss function to optimise it.\nOur loss function is just a comparison between the input image and output image using MSE. If reconstruction is done 100% correctly, then the MSE is just 0.\nWe can save decoder separately, and share it with others. This lets us share full images of things using only the compressed image. They don’t need the encoder.\nBut wait, if the compressed image had all the information needed to reconstruct the original, then there’s no point using the original massive image for training! The compressed version has all the information we need!"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#so-to-train-our-unet",
    "href": "posts/2022-10-11-L9Blog.md.html#so-to-train-our-unet",
    "title": "fast.ai Lesson 9",
    "section": "4.4 So, to train our unet:",
    "text": "4.4 So, to train our unet:\n\nWe put all the training images (which we’ve added noise to) through the autoencoder to compress them,\nWe then train the unet on these compressed noisy images.\n\nThe compressed images are called latents. The output of the unet is now the noise present in the latents. To get back to our original images, we subtract the noise from the latents, then use the decoder to get them back into full size.\n\n4.4.1 Guidance\nWith this new unet, VAE combination in mind, we can produce handwritten digits sure, but how do we get it to produce images based on prompts?"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#firstly-lets-understand-how-to-add-guidance-extra-information-to-the-unet-to-improve-it-using-our-handwritten-digit-example.",
    "href": "posts/2022-10-11-L9Blog.md.html#firstly-lets-understand-how-to-add-guidance-extra-information-to-the-unet-to-improve-it-using-our-handwritten-digit-example.",
    "title": "fast.ai Lesson 9",
    "section": "4.5 Firstly let’s understand how to add guidance (extra information) to the unet to improve it using our handwritten digit example.",
    "text": "4.5 Firstly let’s understand how to add guidance (extra information) to the unet to improve it using our handwritten digit example.\n\nPreviously we just inputted noisy handwritten digits as inputs, and made the unet find the noise.\nBut now, in addition to giving just noisy handwritten digits as inputs, we can also give something like labels to each input image. E.g, inputting a noisy 3 digit and a label of 3.\nIt’s not exactly a label we give, we give a onehot encoded vector that specifies what digit it is. I think the way we create this vector is arbitrary. If we kept the labels consistent for all the training data then it shouldn’t matter? E.g. the number eight label is just a vector thats (0,0,0,0,0,0,0,1,0,0).\nNow it will learn both how to predict the noise in an image, but what the image is supposed to be. It should be better at predicting noise now it has this extra info (called guidance).\nFor example, input a noisy 3 digit and a label of 3, the unet will ‘think’, ‘the noise is everything that doesn’t represent the number 3’."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#with-that-in-mind-we-can-understand-how-to-produce-a-picture-based-on-a-prompt.",
    "href": "posts/2022-10-11-L9Blog.md.html#with-that-in-mind-we-can-understand-how-to-produce-a-picture-based-on-a-prompt.",
    "title": "fast.ai Lesson 9",
    "section": "4.6 With that in mind, we can understand how to produce a picture based on a prompt.",
    "text": "4.6 With that in mind, we can understand how to produce a picture based on a prompt.\n\nSay we wanted to create a picture of ‘a cute teddy bear’.\nUnlike handwritten digits, it’s not easy to know how to construct the guidance/labels to add to each noisy input image.\nThis is because, unlike handwritten digits where there’s a small finite amount, we couldn’t possibly encode every possible input for every possible word in English. You could technically make ‘a cute teddy bear’ into a one-hot vector, but it would be impossible to make and store all possible prompts into one-hot vectors.\nSo instead, we have to create yet another model to assist us!\n\n\n4.6.1 CLIP\n\nOur new model takes a prompt, e.g. ‘a cute teddy bear’, and outputs some vector that represents that prompt.\nIf we had a good vectors to represent our prompts, it’s like generating handwritten digits, we have a good noisy image label so our unet can generate images for us.\nTo create our new model, we can look Online. For accessibility, people have already captioned (created prompts) of images.\nSay we find an image online captioned ‘a graceful swan’.\nWe have two encoders, a textencoder, that takes a caption as input and generates a string (a UTF-8 encoding) of it, and a imageencoder, which takes an image as input and generates a string.\nThe details behind what these strings are will be passed for now.\nFor ‘a graceful swan’:\nTextencoder takes ‘a graceful swan’ as input and creates it’s own embeddings/features to create its string.\nImageencoder takes the image of it as input and creates it’s own embeddings/features to create its string.\nWe want the embeddings created for ‘a graceful swan’ from these two encoders to be the same. This signifies that both encoders are producing and using similar features/embeddings to encode image and text.\nBecause the embeddings are vectors, we can dot product them. We want their dot product to be really big, because that signifies that they are similar.\nWith this, we can create an embedding matrix, for all the training images, 4 in this example:\n\n\n\n\nimage.png\n\n\n\nWe not only want matching prompts and images to have a high dot product, but non matching images to have a low dot product.\nThe latter is because, say for our ‘a graceful swan’ prompt, our textencoder will have features for it, and a non-matching image say the one of Jeremy, will have the imageencoder have features of his image. These features shouldn’t match, because they are representing different things.\nIn short, the diagonals should be high and non diagonal should be low.\nWe can then create a loss function for this! It’s simply: add up all the diagonal and subtract all the non diagonal.\nIf we do this, we will end up with a really good textencoder, that can take prompts, and produce good labels/vectors to represent it.\nOur setup is called multimodel because we have >1 model.\nThis pair in particular is called CLIP.\nThe loss function described is called in the embedding matrix is called contrastive loss.\nFinally, we can now take our ‘a cute teddy’ prompt, put it into textencoder, get features representing it, and use them to act as a label/vector!"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#to-summarise",
    "href": "posts/2022-10-11-L9Blog.md.html#to-summarise",
    "title": "fast.ai Lesson 9",
    "section": "4.7 To summarise:",
    "text": "4.7 To summarise:\n\nCLIP takes prompts as input and produces embeddings for them. Prompts with similar meanings gives us similar embeddings. We use Clip’s embeddings as guidance, a sort of label to noisy input images into our unet, which is pivotal for it to be able to generate images.\n\n\n4.7.1 Overview of the 4 models:\n\nUnets take noisy latents (compressed images) in and outputs their noise. We can subtract the two to get the latents back.\nVAE’s decoder takes latents and decompresses them back into full original image size.\nCLIP Text Encoder takes prompts and produces embeddings. We use these embeddings as guidance to train the unet.\nSD is the process of starting with noise and producing a full image. It works by starting with pure noise, then using a unet to identify noise (defined as pixels that aren’t our target) and subtracting it repeatedly.\n\n\n\n4.7.2 Noise in Training Data\n\nRecall, that there must be noise in our unet input images.\nThe language involved is weird, but just ignore it.\nThere’s a concept called “time steps”. It has nothing to do with time. It’s about have varying levels of noise in the unet input images.\nWe can create a noising ‘schedule’. E.g, We randomly pick a number of 1 to 1000, Say 4, and we lookup how much noise to use if we happen to get it following this graph:\n\n\n\n\nimage.png\n\n\n\nThe timestep 1000 has no noise.\nThis entire concept is just a way to pick how much random noise to add to images to use them for unet training.\nThis t plotted above is called a timestep, and again, it has nothing to do with time.\nThe y axis is sigma, or B, and all it means is the amount of noise.\nFor every minibatch image, we randomly pick a timestep to pick an random amount of noise, and then train with it."
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#steps",
    "href": "posts/2022-10-11-L9Blog.md.html#steps",
    "title": "fast.ai Lesson 9",
    "section": "4.8 Steps",
    "text": "4.8 Steps\nRecall in the lecture, when we tried to create an image from a prompt in one step, it didn’t look good:\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2022-10-11-L9Blog.md.html#steps-1",
    "href": "posts/2022-10-11-L9Blog.md.html#steps-1",
    "title": "fast.ai Lesson 9",
    "section": "4.9 Steps",
    "text": "4.9 Steps\n\nThis occurred because having only one step means we subtracted all the noise in one go.\nDoing it in multiple steps resolves the issue.\nBut then we run into the issue: what constant should we use to dictate how much noise we should subtract per step? How should we add noise? These are research questions and the properties of a diffusion sampler.\nBut think about the noise subtraction equation, P = P - C * N, the picture is the picture - a constant multiplied by the noise.\nIt looks a lot like a deep learning optimiser! The constant is just a learning rate. Why not use LR concepts like momentum! Or Adam!\nThe thing is, diffusion models’ unets’ don’t just train on an input image and guidance, they take timesteps t as a variable too too.\nThe idea is that the unet will get better at removing noise if it has the extra info of how much noise we’re adding (t).\nJeremy doubts this is needed because neural nets can easily solve the noise problem.\nIf we remove t, then the differential equations approach is much simpler and becomes an optimiser problem we’re already famimilar with!\nFor unets also, MSE is usually used in ML because it’s easy. But what if we use more sophisticated loss functions. Like perceptual loss.\nThere’s a lot of novel research in this area, particularly in abandoning t and thinking of everything as an optimiser."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html",
    "href": "posts/2022-10-13-MP4.html",
    "title": "Masters Project 18/10/2022",
    "section": "",
    "text": "The fourth post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#during-the-meeting-we-discussed",
    "href": "posts/2022-10-13-MP4.html#during-the-meeting-we-discussed",
    "title": "Masters Project 18/10/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nTrying to compete directly with Google or other big research teams isn’t a good direction for a one year project. What would be better is a novel direction of something else.\nOne such approach would be trying to use stero data instead of mono data to solve the cocktail problem.\nMost audio data online is mono, for example on xenocanto, but we could try and get in contact with research groups like the biology team and ask for stero data.\nOne of the reasons why stero data is rarer, is because it requires a special microphone to collect it with.\nDifferent stero microphones have different properties, such as the width between the the two microphones, and this needs to be accounted for when gathering and analysing data.\nWe could look at stero data, and then calculate the difference between the first channel and second channel to make a third channel. Then try classifying with one channel, and with all three, and seeing if it helps.\nWhat would be interesting only possible with stero data is trying to find the direction of the birds singing. But this might be impossible, because getting labeled data of that is difficult.\nThe differences between mono and stero data. Besides having 2 channels, there are differences in time delay and attenuation. I need to look into this more.\nInvestigating whether it’s possible to convert between mono and stero data. It might be impossible to do exactly because there is information missing within the mono data, particularly because intuitively, stero data you can find direction from but mono you cannot. This is an information problem.\nStuart might order a stero microphone to play around with; Robert is asking whether it is possible to borrow one.\nAnother novel approach would be trying to use stable diffusion to generate spectrograms. The idea being, if there is a lack of stero data we could synthesise our own. There could even be another model added to correct synthesised audio data to be more like real audio data.\nA motivation behind this could be the prevalence of an image classification approach in classifying birdsong. Much research uses CNNs for example.\nAbout the Physics content of the project. There needs to be some Physics for the sake of the external marker and external questions at the viva.\nPhysics content can be added by investigating how to do the image to audio conversion (or vise versa in the case of stable diffusion), because of the transformations and information problem involved, or the prevalence of linear algebra/maths being involved, and even just in the Physics way of thinking of testing hypothesis and different approaches."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#machine-learning-skills",
    "href": "posts/2022-10-13-MP4.html#machine-learning-skills",
    "title": "Masters Project 18/10/2022",
    "section": "4.1 Machine Learning Skills",
    "text": "4.1 Machine Learning Skills\nUsing frameworks like fast.ai and transformers isn’t as simple as just using their predefined functions and models to do everything. Learning how to find the best hyperparameters, and good validation sets, among many other things, takes a combination of theory and practice to gain intuition. Jeremy from fast.ai said there is no substitute for practice, and provides a lot of guidance on how to do so."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#machine-learning-theory",
    "href": "posts/2022-10-13-MP4.html#machine-learning-theory",
    "title": "Masters Project 18/10/2022",
    "section": "4.2 Machine Learning Theory",
    "text": "4.2 Machine Learning Theory\nAs well as using frameworks and models, you have to spend time learning the theory behind how they work. For instance, how the components in a CNN work the way they do."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#framework-skills",
    "href": "posts/2022-10-13-MP4.html#framework-skills",
    "title": "Masters Project 18/10/2022",
    "section": "4.3 Framework Skills",
    "text": "4.3 Framework Skills\nUnderstanding the theory, and then having novel ideas to approach the cocktail problem, I need to then implement these ideas by knowing how to create the new code to do so.  This involves learning how to edit frameworks and create your own, covered in fast.ai part 2. \nThere’s also learning about https://nbdev.fast.ai/ to create frameworks and their documentation."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#data-handlingpreprocessingphysics",
    "href": "posts/2022-10-13-MP4.html#data-handlingpreprocessingphysics",
    "title": "Masters Project 18/10/2022",
    "section": "4.4 Data handling/preprocessing/Physics",
    "text": "4.4 Data handling/preprocessing/Physics\nLearning how to store data, access it, transform it into the right size and format, edit it, add noise to it, interpret it (bird domain information) etc.  There could be much work to be done on transforming the audio data. Fourier and Gabor transformers etc. I found a YouTube playlist of guides on this at https://www.youtube.com/watch?v=RMfeYitdO-c. The fourth initial project reference, “New aspects in birdsong recognition utilizing the gabor transform”, focuses on the gabor transform and likely much Physics too."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#custom-metrics-creation-and-evaluation-for-models",
    "href": "posts/2022-10-13-MP4.html#custom-metrics-creation-and-evaluation-for-models",
    "title": "Masters Project 18/10/2022",
    "section": "4.5 Custom metrics, creation, and evaluation for models",
    "text": "4.5 Custom metrics, creation, and evaluation for models\nThe biology department have their own interests and goals of what they want from a model. I would need to talk in detail with them about their priorities, e.g. preferences in confusion matrix metrics, in bird species etc. They might want a model to work with data over a few years to spot trends too."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#machine-learning-explainability-and-communication",
    "href": "posts/2022-10-13-MP4.html#machine-learning-explainability-and-communication",
    "title": "Masters Project 18/10/2022",
    "section": "4.6 Machine learning explainability and communication",
    "text": "4.6 Machine learning explainability and communication\nLearning how to implement and create methods and visualisations to communicate why the models are predicting as they do. This is especially important for marking in the final report."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#machine-learning-maths.",
    "href": "posts/2022-10-13-MP4.html#machine-learning-maths.",
    "title": "Masters Project 18/10/2022",
    "section": "4.7 Machine learning maths.",
    "text": "4.7 Machine learning maths.\nTo read and implement the latest machine learning papers, some mathematical knowledge is needed. I am contemplating doing yet another free fast.ai course, Computational Linear Algebra, explained here https://www.fast.ai/posts/2017-07-17-num-lin-alg.html, to help with the maths side of things. \nAlternatively or in addition, the book Deep Learning by Ian Goodfellow provides a mathematical backing and Jeremy recommended reading the first 6 chapters of it to help with understanding and implementing maths in papers."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#practiced-transformers",
    "href": "posts/2022-10-13-MP4.html#practiced-transformers",
    "title": "Masters Project 18/10/2022",
    "section": "5.1 Practiced Transformers",
    "text": "5.1 Practiced Transformers\nA list of transformer tasks is at https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb which is quite useful. \n\n5.1.1 In particular for audio classification it details the process: \n\nInstantiate a feature extractor and a model from the checkpoint name.\nProcess the audio signal to be classified with a feature extractor.\nPass the input through the model and take the argmax to retrieve the most likely class.\nConvert the class id to a class name with id2label to return an interpretable result.\n\nI went through the HuggingFace transformers documentation and did some of the notebooks to understand them. - https://www.kaggle.com/adnanjinnah/audio-classification-hf-1/ - https://www.kaggle.com/adnanjinnah/audio-classification-hf-2/ - https://www.kaggle.com/adnanjinnah/audio-classification-hf-3/ and they covered the 4 step process detailed above."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#practiced-trying-to-attempt-birdclef-2022",
    "href": "posts/2022-10-13-MP4.html#practiced-trying-to-attempt-birdclef-2022",
    "title": "Masters Project 18/10/2022",
    "section": "5.2 Practiced Trying to attempt BirdCLEF 2022",
    "text": "5.2 Practiced Trying to attempt BirdCLEF 2022\nIt’s well worth practicing attempts for a competition with the goal exactly as my own. After trying fast.ai’s audio module last week, and thinking it is outdated (the GitHub repo hasn’t been updated in roughly 6 months), I decided to use HuggingFace instead. This is mainly due to Jeremy recommending it as an up to date framework, but also because it is used in fast.ai part 2.\nWith that in mind, I attempted it at https://www.kaggle.com/adnanjinnah/birdclef-first-attempt/. This attempt was writhe with problems. While it was my first time using HuggingFace audio, the number of problems I encountered and issues involved were too much. I did not manage to get any model to work. I spent the entire time just trying to get the data loaded properly for usage.\n\n5.2.1 To summarise:\n\nHuggingFace’s load_model has several different methods to load audio. They all require the data to be formatted in a particular way. I tried all them with no success.\nKaggle’s competition dataset is set to read only for some reason. This makes it so I cannot directly just edit the files to get them right.\nI tired simply downloading the dataset and reuploading it to Kaggle but A. this is inefficient and B. won’t work for the unseen test data.\nI tried copying over the dataset from the read-only input folder to the editable output folder, but this is also inefficient and even so:\nI couldn’t load the copied data using load_dataset’s audiofolder function. I’m not sure why. I have it formatted in the exact way the documentation shows. The issue may be I need to upload the dataset to HuggingFace’s website first, but this has the same issues as the first attempt.\nA way to get around having to copy the data, with is also inefficient but would atleast work with the unseen test data is to tell load_dataset the URLs of the audio files. This didn’t work either, because some of the URLs don’t work in the instant load_dataset wants to access them. I couldn’t find a way to tell load_dataset to ignore or look later at these URLs.\nI tried using a different method of load_dataset, this one however seems to require the main .csv file to contain the audio files in array format. Because the .csv file contains a path to the audio files instead of their content, I tried using another module, librosa, to create a column in the .csv file containing the audio. This didn’t work, because of an excess memory error. And also, this is very inefficient.\n\nAfter extensively trying all methods I could find in the documentation with little success, this entire process took around 10 hours. I found tutorials to help with no luck. For now, I’ve given up on trying to get it to work myself. I need to find some resource online or in person to help. In hindsight, I probably should have done this earlier.\n\n\n5.2.2 On the bright side, atleast I learn’t a few things from the struggle:\n\nFirst, how transformers requires a dataset to be formatted in a specific way, and that HuggingFace has a website dedicated to storing datasets in an already formatted way.\nExperience in reading through documentation and troubleshooting.\nThe fact that sometimes URLs don’t work, and that last week’s code had a solution, but I couldn’t implement it into HuggingFace’s load_model.\nThat different loading methods require paths to audio files or them on the .csv file.\nThat audio files are stored as a file such as .ogg or as an array.\nHow librosa is a module to convert audio files into audio files into said arrays.\nThat memory errors will occur from trying to do too much at once. I could get my last method to work if I figured out a way to split up the data, but regardless this approach is inefficient considering we already have the files so it’s better to find a different method.\nHow to use os to copy files and folders over, or search and retrieve their file paths.\nThe fact that, for some datasets like BirdCLEF, there is a metadata.csv file with a column for the paths of the audio files.\nThat for advanced dataset formatting, for HuggingFace, you can create a .py script to do things exactly as you want."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#finished-fast.ai-lesson-9",
    "href": "posts/2022-10-13-MP4.html#finished-fast.ai-lesson-9",
    "title": "Masters Project 18/10/2022",
    "section": "5.3 Finished fast.ai lesson 9:",
    "text": "5.3 Finished fast.ai lesson 9:\nThis lesson was the first of fast.ai part 2 and a very well taught one. In it, Jeremy described conceptually how stable diffusion, an crazy new image generation model, works. Due to it’s difficulty, the lesson took me a full day to complete, but it was well worth it. The ideas and skills I’m being introduced to and learning will prove really helpful for the project going forwards. Next week, the lesson will focus on programming stable diffusion from scratch, and building on that, how to programme your own custom Python machine learning libraries. This is vital because it would allow me not just to copy other people’s code to solve the cocktail problem, but implement my own ideas and test things, perhaps even at a research level.\nMy post for lesson 9 can be found at: https://exiomius.quarto.pub/blog/posts/2022-10-11-L9Blog.md.html"
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#finished-cla-lesson-1",
    "href": "posts/2022-10-13-MP4.html#finished-cla-lesson-1",
    "title": "Masters Project 18/10/2022",
    "section": "5.4 Finished CLA lesson 1:",
    "text": "5.4 Finished CLA lesson 1:\nComputational Linear Algebra is a fast.ai course covering linear algebra to be centered around practical applications and algorithms.  More info and lesson 1 blog can be found here: https://exiomius.quarto.pub/blog/posts/2022-10-17-CLA1.md.html"
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#useful-datasets-found",
    "href": "posts/2022-10-13-MP4.html#useful-datasets-found",
    "title": "Masters Project 18/10/2022",
    "section": "5.5 Useful Datasets Found",
    "text": "5.5 Useful Datasets Found\n\nBirdCLEF 2022 uses data from xeno-carto, implying that last week’s approach to downloading them is a good idea.\nI found ESC-50, a dataset of labeled environmental audio recordings at https://dagshub.com/kinkusuma/esc50-dataset, also at https://huggingface.co/datasets/ashraq/esc50. These include sounds like rain, sea waves, animals.\nI found that Machine Listening Lab at Queen Mary’s University run a birdsong competition and have many datasets that I could possibly use at http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#useful-research-tools",
    "href": "posts/2022-10-13-MP4.html#useful-research-tools",
    "title": "Masters Project 18/10/2022",
    "section": "5.6 Useful Research Tools",
    "text": "5.6 Useful Research Tools\n\nScholarcy summarises research articles.\nhttps://inciteful.xyz/ is good for finding papers.\nI was told that Prostudy is useful for keeping resources stored for a dissertation."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#a-similar-thesis",
    "href": "posts/2022-10-13-MP4.html#a-similar-thesis",
    "title": "Masters Project 18/10/2022",
    "section": "5.7 A Similar Thesis",
    "text": "5.7 A Similar Thesis\nMy friend’s friend wrote a thesis similar in aim to mine last year. \nTitle: Using mel-frequency cepstral coefficients and principal components analysis to classify bird vocalisations based on citizen science recordings.  Student Name: Alex Dyfrig Swainston. \nI messaged Alex and got a copy, and he said he’s happy to help if I have any questions."
  },
  {
    "objectID": "posts/2022-10-13-MP4.html#new-ideas",
    "href": "posts/2022-10-13-MP4.html#new-ideas",
    "title": "Masters Project 18/10/2022",
    "section": "5.8 New Ideas:",
    "text": "5.8 New Ideas:\nHere are a few new ideas I had about tackling the cocktail problem.\nA big issue is the lack of properly labeled data for soundscapes. The biology department painstakingly handlabeled some soundscapes, but it is a difficult and time consuming task that even great ecologists struggle with. What if there was a way to create our own soundscapes that are already labeled? For instance, we have plenty of data from xeno-canto of individual bird songs with varying amounts of noise. What if I also found some audio files of forest environments, and I created a model to combine xeno-canto bird songs with these to imitate a real soundscape? This way, I could create an endless amount of soundscapes to train on, and the birds within them would be labeled!\n\n5.8.1 To create a soundscape:\n\nI could download bird song(s),\nCut out various parts of them, e.g. if it’s 3 minutes long, I cut out random intervals of 20-30 seconds to imitate the bird moving or other sounds overpowering their song,\nRandomly vary how loud the bird songs are,\nAdd in environmental sounds like a forest soundscape (but being careful there are no birds present!),\nUse a noise function, (which is used in stable diffusion), to randomly add noise. Alternatively, find a way to make a model that can generate real noise that is recorded by microphones and use that.\n\nI could put multiple birdsongs in the same artificial soundscape, and even make them overlap, but I also need to be careful that perhaps I should make the birds singing be realistically in the same environment. I mean I shouldn’t put two birds together that geographically would never meet, or two birds that never sing at the same time of day, or in general environmental sounds that don’t match the birds present.\nAnother idea is to add geographical data somehow to the dataset. Perhaps with another input for a satellite image to help."
  },
  {
    "objectID": "posts/2022-10-17-CLA1.md.html",
    "href": "posts/2022-10-17-CLA1.md.html",
    "title": "fast.ai CLA Lesson 1",
    "section": "",
    "text": "2 Course Introduction\nAn introductory blog post for this course can be found here https://www.fast.ai/posts/2017-07-17-num-lin-alg.html. \nBecause machine learning is largely about manipulating data, and almost all data can be represented as a matrix, understanding linear algebra is often cited as a prerequisite to reading and understanding formal mathematical descriptions of machine learning methods, as well as creating or editing existing methods. \nComputational Linear Algebra is a fast.ai course covering linear algebra to be centered around practical applications and algorithms. \nThere are four main areas for machine learning in which some linear algebra knowledge can help: - Speed (how fast matrix multiplication occurs) - Accuracy (how accurately can computers represent numbers) - Memory Usage (how to efficiently store matrices) - Scalability (how to use more data than you have the memory to store)\nThe reason why we are interested in these things is because often the bottleneck to a machine learning algorithm is within these four areas. In other words, knowledge in these areas can be the difference between a great ML approach and an unusable one. One example is in the case of how CNNs create their convolutional layers. While there are many mathematically equivalently orders in which to create these layers, some are evidently significantly faster. When these are applied in bulk, the optimisation makes all the difference. So in order to design or edit algorithms for usage in ML, knowledge in computational linear algebra is essential, particularly in research contexts as new approaches have not yet been optimised or implemented in existing frameworks.\n\n\n3 Lesson Overview\nThis lesson covers the basics for our four main optimisation areas: Speed, Accuracy, Memory Usage and Scalability.\n\n\n4 The topics covered, briefly\n\nAccuracy: Number representation, Machine Epsilon, Conditioning and Stability. Approximation Accuracy.\nMemory Use: Sparse vs Dense Matrices.\nSpeed: Computational Complexity, Vectorisation, Locality (Memory Usage), Scaling.\n\n\n\n5 Lecture/Notebook Notes\nThere are two key types of matrix computation: Matrix and tensor products (combining matrices), and matrix decompositions (pulling them apart).\nConvolutions are a special kind of matrix product, but can also be represented as a neural network where the image pixels are the start nodes, the kernel elements are the weights, and the convolution pixels end nodes.\n\n5.0.1 Accuracy\nThe representation of numbers:\nOn paper, fractions are infinitely written. Computers however cannot store fractions 100% precisely because they are using discrete memory to store infinite precision. We ran iterations of a function that inputs and outputs fractions. Every iteration a very small error is added, harmless for the first few. But over time, these errors result in an entirely wrong answer.\nFor IEEE Double precision (an agreed standard): \nThe continuous interval between [1,2] in a computer is represented as \\(1, 1+2^{-52},1+2x2^{-52}...,2\\)  So in this case, we see clearly it doesn’t represent infinite precision. The smallest increment, in this case \\(2^{-52}\\), depends on the size of the interval. For a bigger interval, [2,4], it’s \\(2^{-51}\\), bigger by a magnitude. \nMachine Epsilon:\nMachine Epsilon is defined as half the distance between 1 and the next larger number. \nI believe this means in the case of our [1,2] interval, \\(\\varepsilon_{machine}=2^{-52}/2\\). \nBut the notes state: “IEEE standards for double precision specify \\(\\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}\\)”, implying that Machine Epsilon is a constant value for a machine, rather than dependent on the interval or calculation involved?\nRegardless, we often describe error in terms of \\(\\varepsilon\\). For instance, say we represent a real number \\(x\\) in a computer, so have a approximation \\(fl(x)\\). The difference between \\(x\\) and \\(fl(x)\\) is always smaller than \\(x*\\varepsilon\\).\nAs an equation: \\(fl(x)=x \\cdot (1 + \\varepsilon)\\),  the error is from the \\(x*\\varepsilon\\) term.\nFor operations in a computer, +,-,x,/:  $ x y = (x * y)(1 + )$, the error is from all the terms containing \\(\\varepsilon\\).\nConditioning and Stability:\nBecause we can’t represent numbers exactly, we need to know the errors that occur as a result. There are two defined terms to help with this:\nConditioning, about how accurately we can represent the problem.  Conditioning: perturbation behavior of a mathematical problem (e.g. least squares)\nStability, about how accurately we can compute the answer to said problem.  Stability: perturbation behavior of an algorithm used to solve that problem on a computer (e.g. least squares algorithms, householder, back substitution, gaussian elimination)\n“A stable algorithm gives nearly the right answer to nearly the right question.” –Trefethen\nAn an example for how small problems in accuracy can cause problems, consider how a small difference in matrix values results in very different eigenvalues.\nimport numpy as np import scipy.linalg as la\nA = np.array([[1., 1000], [0, 1]]) B = np.array([[1, 1000], [0.001, 1]])\nprint(A) print(B)\nwA, vrA = la.eig(A) wB, vrB = la.eig(B)\nprint() print(wA, wB)\nHaving 0.001 instead of 0 resulted in the first eigenvalue to be 2 instead of 1!\n\n\n5.0.2 Approximation Accuracy\nAccepting some decreases in accuracy can speed up computations by orders of magnitude. So often using approximate algorithms is better.\nIn ML, some errors in training data representation are actually good because they force generalisability. \nAnd sometimes we need not be super concerned about having 100% precise training data representation because the data collected isn’t 100% precise in the first place.\nBloom filters can tell you a definite no, but not a definite yes, more like a probably yes. To remedy this, we can make a second more precise method to evaluate the items that are probably yes, while just ignoring the ones that are already known to be definitely no.\n\n\n5.0.3 Memory Use\nSparse vs Dense matrices.\nSparse storage is just storing the non-zero elements of your matrix because you know the others are just 0. There are special ways of doing sparse storage.\nDense storage is the normal way we do it when we code, we just store everything explicitly.\n\n\n5.0.4 Speed\nThe difference in speed between algorithms come from a number of areas by in particular: - Computational Complexity - Vectorisation - Scaling - Locality\nComputational complexity and big \\(\\mathcal{O}\\) notation is about approximating the number of operations you need to do for a particular algorithm. More info: on Interview Cake and practice on Codecademy.\nVectorisation is about applying an operation is multiple elements at once. Numpy replies on vectorized low level linear algebra APIs (BLAS and LAPACK) to do it’s matrix operations.\nLocality is about how data in use is stored. Computers are usually slow because of the way we access data. Generally speaking, the faster the memory (so the faster we access the data), the less of it we have/the more expensive it is. Computers have many varying memory storage types, and each step down to slower memory you go, that memory is atleast an order of magnitude slower than the one before it.\nWe want to minimise the time we take to retrieve data in a computation. For example, by keeping items we are going to use multiple times in a computation in fast memory, and keepings items we use rarely in slow memory.\nA video to illustrate locality is then shared.\nfrom IPython.display import YouTubeVideo YouTubeVideo(“3uiEyEKji0M”)\nCode optimisation is really important. Even for a simple task, finding the average of 3 pixels, the code would normally be simple, but writing complex code would speed it up by 11x! It’s faster because it distributes work across threads (parallelism). Locality is making sure the pixels being used successively is in fast memory (cashe). W/O locality, paralleism can’t be great.\nWe change the order in that CNN is done and as a result the way we store the pixel data, and get a much faster computational as a result. Removing redundancy in computation also speeds up the computation. Each computation technique has potential trade offs. For example, having redundant computation to improve locality.\nTemporaries is data stored in a temporary variable in RAM. For example in Numpy, when we compute an equation, Numpy stores each equation variable as temporaries and then retrieves it. This is slow because there’s no point storing each variable in the RAM and then immediately having to use it. Simply storing these variables in the cache would be so much faster.\nScaling:\nWe we want to scale our computation across multiple cores in a computer. This is called parallesiation. Scalable algorithm are algorithms where the input can be broken into smaller pieces where each can be handled by a different core and the end result found by piecing together these pieces.\n\n\n\n6 Links\nThe lecture for this sessions is https://www.youtube.com/watch?v=8iGzBMboA0I&list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&index=1.  The lesson resources including the notebook(s): https://github.com/fastai/numerical-linear-algebra."
  },
  {
    "objectID": "posts/2022-10-25-L11.md.html",
    "href": "posts/2022-10-25-L11.md.html",
    "title": "fast.ai Lesson 11",
    "section": "",
    "text": "The eleventh lesson of fast.ai."
  },
  {
    "objectID": "posts/2022-10-25-L11.md.html#diffedit-paper",
    "href": "posts/2022-10-25-L11.md.html#diffedit-paper",
    "title": "fast.ai Lesson 11",
    "section": "3.1 DiffEdit paper:",
    "text": "3.1 DiffEdit paper:\nArxiv is a preprint paper. It contains papers before they gave been peer reviewed. Regardless, they are useful because you can test the concepts and code yourself and look on discussions online to see if the paper is good. Waiting until peer review takes too long in such a fast moving field.\nJeremy recommends Zotero as free software to store, sort, and annotate papers.\nReading papers is hard. The goal isn’t to understand the entire paper at first. Focus on the basic ideas so that we can look at the code and diagrams to see how it works and then we can work backwards. Writing your own implementation of a paper is the best way to understand it.\nStart by reading the abstract.\nIt uses text-condition diffusion, so what we’ve being doing.\nSemantic image editing is image generation where the generated image is as similar as possible to a given input image.\nI didn’t know this, but abstracts can contain diagrams, this one is really useful!\n\n\n\nimage.png\n\n\nCurrently techniques require you to draw a mask around the subject you want to change in the input image. This paper has an apporach to generate the mask for you!\nPapers are full of citations. Don’t read all of them, it’s impossible because every paper will have even more citations to read, and you’ll end up spending too much time on it. Focus on only the very important/relevant papers.\nIf we want to turn a dog into a cat, we want to keep the dog’s pose. Older papers used a mask to delete the area and regenerate, but that can’t keep the pose. This paper’s method attempts to keep the pose!\nIn a paper, the first thing we need to understand is the aim of the paper. This context makes everything else make much more sense. To do this, we start with the abstract, then look at the references, experiments and examples. Often we skip a lot of sections and text.\nThe related work section is important to study if you want to do a deep dive. There’s a lot of repetition in the paper of the papers cited and that’s fine to skip. The last lines of related work are normally the most interesting. If you wanted to do the best possible to get the best ideas, spending ages on related work is important.\nThe background section. This section is often the scariest. It’s the maths behind how the model works. No one in the world looks at the background and gets it immediately.\nIn this case, first you need to really know DDPM well. This could take a very long time of reading previous papers, testing it yourself, and talking to others. After that, then it makes sense quickly. The background is meant to be a reminder of something you already know, not a place to learn it from scratch.\nEvery diffusion paper has these equations, and lesson 9B covers them properly. The background is often to look cool for reviewers. The main goal of reading it is to know the symbol’s definitions so that you can understand them later on in the paper.\nHow should you approach reading the background section?\nFirst learn the greek alphabet properly. We can use a program to take images of maths and tell us their LaTex representation. This allows us to google the terms. Instead, we can just download the paper in LaTex format.\nThe fancy L is just the loss function.\nAfter some googling we find that:  The L2 norm has the subscript 2. L2 norm is just denoted with a subscript 2, and a top 2 for squared.\nThis means that the entire thing in the modulus is just Sigma(x^2)!\nTrying to find what E is!\nLooking at the latex paper file:\n\n\n\nimage.png\n\n\nIt’s mathbb{E}, now let’s google it! \nIt’s the expected value operator. Expected value is a weighted average. It’s Sum \\((probability*value)\\) and gives the average result.\n\n\n\nimage.png\n\n\nEpsilon is the noise. It is distributed normally with a mean of 0 and variance of I.  I is the identity matrix, covered more in lesson 9b.\nEpsilon subscript 0 is a estimate for the noise.\nLooking at the loss equation:\n\n\n\nLoss.png\n\n\nWe have an expression for the noise, Epsilon, and for a prediction of it, Epsilon_0. We subtract them from eachother, square it, and find the expected value.\nDoes it look familiar to something we know?\n\n\n\nFrom https://vedexcel.com/how-to-calculate-mean-squared-error-mse-in-python/.png\n\n\nThe loss function is just MSE but with some tweaks! There’s no 1/n and there’s L2 normalisation.\nThe point is, this scary looking loss equation boils down to something we’re already well familiar with!\nOnce you understand that we’re working with a concept we already know, the equations make sense. Remember that this is a background section. It tells us about prior concepts and how they changed them.\nIn practice, often none of the further background matters. There’s only a subsection of the background we need to understand for our purposes. You don’t need to read the whole thing.\nThe diagram in the paper is often the best part in understanding it.\n\n\n\nimage.png\n\n\nThe strange O means nothing: unconditional diffusion. In my terms, with a prompt of ““.\nStep 1: We make a mask based on the differences between the denoising of the original prompt (zebra) and the denoising on a new prompt (horse). Step 2: Take the Horse, add noise to it. Step 3: During inference, while we turn the noised horse, we make the background stay the same.\nJeremy skips the Theoretical analysis. It’s needed to get past peer review. It proves more concretely why this method is better than others. This is needed from a science pov, but as appliers we can clearly see it works well and test it ourselves so we don’t need to throughly know the proof.\nThe Experimentation setup shows us what datasets they used and so on. They use metrics like CSFID and FID. We don’t usually care for out interests either. This is needed because good metrics are proof to explain why the method is better for reviews.\nBut here we find some examples. It’s very cool to see, and also lets you understand how good and bad the technique is in certain areas.\nLooking at these examples, we thought of a realisation:  The generated masks cover the core subject in the image, but what if we wanted to change both the core subject and the rest of the image? It can’t have its single mask cover both properly to change both. For example, changing a bowl of apples to a bowl of oranges in a purple tinged photo.\nThe conclusion section almost never adds anything on top of what we’ve already read.\nHowever, the appendices afterwards are useful for more examples. Often some of the most interesting examples are there.\nIt’s important to remember that this wasn’t a carefully chosen paper. It was just the most interesting paper this week. This walkthrough was the typical paper reading experience.\nChallenging homework given is to try and implement some of this paper. To do it, lesson 9 contains what you need."
  },
  {
    "objectID": "posts/2022-11-01-L12.html",
    "href": "posts/2022-11-01-L12.html",
    "title": "fast.ai Lesson 12",
    "section": "",
    "text": "The twelfth lesson of fast.ai."
  },
  {
    "objectID": "posts/2022-11-01-L12.html#clip-interrogator.",
    "href": "posts/2022-11-01-L12.html#clip-interrogator.",
    "title": "fast.ai Lesson 12",
    "section": "3.1 CLIP Interrogator.",
    "text": "3.1 CLIP Interrogator.\nIt’s a model where, you put a generated image in, and it (tries to) return the prompt that generated it. In theory, if it returned the correct prompt and you (somehow) had it’s seed, you could regenerate it.\nBut generally speaking, it doesn’t actually return the right prompt at all! Why not?\nImagine a friend took a photo of themselves.  Using CLIP encoder to compress it, he then send the resulting embedding to Jeremy.  Compressing it makes sense, the embedding is very small, just a vector of a few floats.  The idea is, if Jeremy had the embedding, he could just decode it to turn it back into the photo.  But can Jeremy decode it? No.\nThe clip encoder takes an image and encodes it. It’s a function f(x).  Does that imply an inverse function f(x)^-1 exists to do the opposite, to decode it? \nNo. Not every function has an inverse. If the function destroys all the information, it’s not possible to reproduce it.\nThe CLIP encoder will lose some of the photo’s information, so can’t be reconstructed.\nInstead of trying to reconstruct the full original photo via a decoder, which we can’t, we can create something that is vaguely like the original photo, using diffusion!\nTake noise of an image embedding and remove the noise. It won’t return the OG picture, but something like it, using conditional diffusion.\nStart our diffusion with an image of pure noise, and use the image embedding to guide it to the image output.  Essentially, the model tries to remove the noise that doesn’t match/look like the embedding.\nIn general, diffusion takes an embedding/prompt and generates an image using it. Ideally, you could use CLIP’s encoder on the resulting image to get that embedding/prompt back.\nIn other words, diffusion is the (approximation) of the inverse of the CLIP encoder.\nLooking back, we said to guide our diffusion with an image embedding, but remember we don’t use image embeddings, we use text embeddings?\nOpen.ai trained CLIP on pictures and their captions. For each image, we optimised so that the embedding created from the picture by the image encoder is similar to the embedding created from the caption by the text encoder.\nSo our text and image embeddings should be really similar in CLIP, therefore, we can guide diffusion inference using text or image embeddings.\nRight now, we don’t have a perfect way to invert an encoder.\n\n3.1.1 In conclusion:\n\nWe are given an image embedding/prompt and want to invert the encoder to get back to the image.\nThis requires perfectly inverting the encoder.\nWe don’t have a way to do this yet.\nDiffusion is an approximate inverse of the encoder: it takes an image embedding/prompt, does inference on it starting with pure noise, and returns an image that somewhat resembles the original.\nWhen we previously described diffusion, we stated with do inference with a text embedding, not an image embedding, but recall in CLIP they should be similar."
  },
  {
    "objectID": "posts/2022-11-01-L12.html#implement-new-algorithm",
    "href": "posts/2022-11-01-L12.html#implement-new-algorithm",
    "title": "fast.ai Lesson 12",
    "section": "6.1 Implement new algorithm",
    "text": "6.1 Implement new algorithm\nChallenging (important) homework: implement an different algorithm yourself and GPU optimise it. For example, dbsscan or LSH. You will learn much from having to do it yourself."
  },
  {
    "objectID": "posts/2022-11-01-L12.html#torch.einsum-practice",
    "href": "posts/2022-11-01-L12.html#torch.einsum-practice",
    "title": "fast.ai Lesson 12",
    "section": "6.2 Torch.einsum practice",
    "text": "6.2 Torch.einsum practice\nRewrite the dist function using torch.einsum. Rewrite dist = torch.sqrt(((x-X) ** 2).sum(1))  Using einsum instead. x-X can’t be changed, but the multiply (** 2) and summation can be done in one einsum!"
  },
  {
    "objectID": "posts/2022-11-01-L12.html#animationvisualisation-practice",
    "href": "posts/2022-11-01-L12.html#animationvisualisation-practice",
    "title": "fast.ai Lesson 12",
    "section": "6.3 Animation/Visualisation practice",
    "text": "6.3 Animation/Visualisation practice\nCreate your own animation/visualisation of your implementation, of something else, or of your stable diffusion."
  },
  {
    "objectID": "posts/AIS1.html",
    "href": "posts/AIS1.html",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "",
    "text": "This is the first post in a series about AI safety (AIS). I am apart of the effective altruism AI discussion group at Durham. These posts will contain my rewritten notes of and comments about the core readings."
  },
  {
    "objectID": "posts/AIS1.html#core-reading-1-four-background-claims",
    "href": "posts/AIS1.html#core-reading-1-four-background-claims",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.1 Core Reading 1: Four Background Claims",
    "text": "3.1 Core Reading 1: Four Background Claims\n\n3.1.1 Claim 1: Humans have a very general ability to solve problems and achieve goals across diverse domains.\nWe generally call this intelligence/general intelligence but there’s isn’t a former definition for intelligence. This makes sense because unlike a definite property like height, it’s impossible to define how intelligent a human is let alone make a mathematical definition. Whatever intelligence may be, we might be able to replicate it in code, or maybe not. Do we even need to know what intelligence is to replicate it?\nAlternative view: General intelligence doesn’t exist. Actually humans just have a collection of separate specific modules/functions that add up together to seem as if we have a general problem solving ability. Computers can’t get general intelligence, they can only get better at specific tasks. Even if this view was true, what’s theoretically stopping a computer from getting good at enough separate specific tasks until, like a human, it seems to have general intelligence?\nShort response: The author finds this “disparate modules” hypothesis implausible because humans can gain skill in areas that early humans (I presume evolved for specific tasks) have no experience in (I presume tasks unevolved for).. He thinks general intelligence probably comprises from a number of different cognitive modules and interactions, isn’t not just one module called the general intelligence module that humans have. As a result of these modules and interactions, humans have cognitive adaptability more so than chimpanzees for example.\nI agree that general intelligence probably comprises from a number of different cognitive modules and their interactions. It can’t be a specific module that is just general intelligence that is well defined and can be replicated easily. However, his rebuttle of the disparate modules hypothesis doesn’t exactly make sense to me. Even if we’ve not been evolved to do these new tasks, if we were evolved a large enough amount of task specific modules, then we can learn to do them anyway?\nWhy this claim matters: because humans became dominant because they are more intelligent. We share chimpanzees as ancestors, then a few million years later gained this ‘general intelligence’. A few million years by evolutionary standards isn’t very long. This implies there a few key ideas/changes between chimpanzees and the first humans that result in our intelligence. If we knew these, then perhaps we could make very intelligent AI systems.\n\n\n3.1.2 Claim 2: AI systems could become much more intelligent than humans.\nAt MIRI, they don’t have conviction about when smarter-than-human AI will be developed, but expect that a) they will eventually be developed (probably within a century) and b) they can become significantly more intelligent than any human.\nAlternative view 1: The human brain does something special that can’t be replicated by a computer.\nShort response: Brains are physical systems. If certain versions of the Church-Turning thesis hold, then computers theoretically can replicate the functional input/output behaviour of any physical system, including the brain. I think this means that the brain is a physical system, a collection of physical objects that take an input and form and output, a computer can replicate the process: it can take the same input(s) and produce the same output(s). Even if there was something like qualia, the qualitative component of consciousness, that can’t be replicated, it doesn’t matter, unless it’s important for our brain’s problem-solving computation process of taking an input and giving an output. Computers can replicate the brain’s problem-solving computation process and problem solve/gain intelligence regardless.\n(The Church-Turning thesis states that a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine. A turning machine can implement any computer algorithm.)\nAlternative view 2: That the algorithms at the root of general intelligence are too complex for human beings to be able to program for many centuries.\nShort response: Looking at the previous claim’s conclusion, the cognitive advantage/general intelligence in humans took a extremely short evolutionary time frame. The general intelligence part of humans that sets us apart from less intelligent species mustn’t be extremely complicated because of this. The building blocks of general intelligence must be present in chimpanzees too, but they didn’t have the shift we did. I’m not sure I agree because I question the assumption that evolutionary time reflects complexity.\nAlternative view 3: Humans are already at or near peak physically possible intelligence. Thus, although we may be able to build human-equivalent intelligent machines, we won’t be able to build superintelligent (smarter-than-human) machines.\nShort response: It seems very possible within the boundaries of physics to run a computer simulation of the human brain thousands of times faster than a real one. I think it really depends what we mean by speed. Does speed matter to reasoning? If there a ideal speed for the human brain that results in the best reasoning? Can a computer reach that speed reliably and so outreason a human?\nComputers can probably use computational resources more effectively than humans do, even at the same speed, if we program them correctly. Overall I think that humans are very suspect to issues with reasoning such as stress or bias, so if we can build human-equivalent intelligent machines, surely we can optimise them against these to make them smarter-than-humans.\nWhy this claim matters: Already we know human-designed machines are orders of magnitudes better than any biological creature we know of at certain tasks. For example, cars and horses. We can build machines better than animals for tasks we care about, and cut out biological mechanisms irrelevant for it such as reproduction. We could build machines with the task of (narrow, even if only for specific tasks) intelligence that are better than humans without biological mechanisms like stress or bias tainting reason. This can solve the world’s biggest problems by scientific and technological innovation, improving the world at unprecedented pace.\n\n\n3.1.3 Claim 3: If we create highly intelligent AI systems, their decisions will shape the future.\nSmarter-than-human AI will shape the environment like humans have, as the most intelligent beings.\nAlternative view: Our environment is too competitive for AI to outcompete us.\nShort Response: Smart AI will be able to outcompete humans by having the knowledge and capability to do things we would never. In the past, the deciding factor between human conflict has been intelligence. For smarter-than-human AI, that intelligence would lead to outcompeting humans.\nWhy this claim matters: If think AI decisions will shape the future, then obviously we need to think about this.\n\n\n3.1.4 Claim 4: Highly intelligent AI systems won’t be beneficial by default.\nIn order to build AI systems that will benefit humanity, we need to prioritise solve a number of technical challenges over building more powerful and general AI.\nAlternative view: As humans became smarter, we’ve become more peaceful to eachother. Less war etc. AI will do the same as it figures out our values, and do them better than we do.\nShort response: Smart enough AI can figure out our intentions and values, but that doesn’t mean they will automatically carry them out for us.\nIf we want values of compassion and peace, we have to program them. Setting a goal like ‘cure cancer’ could easily go wrong.\nWhy this claim matters: We could solve some of the world’s largest problems with AI. But to do so, we need more than just computational power, we need AI that takes human values into account and faithfully executes them, or else things can and will go wrong.\n\n\n3.1.5 Conclusion\nCurrently I learn towards the four claims, or at least their sentiment. For example, maybe general intelligence doesn’t exist, or can’t be replicated, but AI still can outperform humans, have an immense impact, and won’t be beneficial by default."
  },
  {
    "objectID": "posts/AIS1.html#core-reading-2-agi-safety-from-first-principles-ngo-2020",
    "href": "posts/AIS1.html#core-reading-2-agi-safety-from-first-principles-ngo-2020",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.2 Core Reading 2: AGI safety from first principles (Ngo, 2020)",
    "text": "3.2 Core Reading 2: AGI safety from first principles (Ngo, 2020)\n(from section 1 to end of 2.1)\nThis report is from 2020, so I wonder how dated it is, especially regarding image generation advances.\n\n3.2.1 1. Introduction\nAIs will eventually become more capable than us. If they don’t want to obey us, we’ll become Earth’s second most powerful ‘species’, and lose control over the future.\n\n\n3.2.2 2. Superintelligence\nLet’s define intelligence as the ability to achieve goals in a wide range of environments.\nTask-based approach: agents that can do well at many tasks because they have been optimised for each individual task.\nGeneralisation-based approach: agents that can understand unseen tasks with little no task-specific training, the understanding comes from experience from previous tasks.\n\n\n3.2.3 2.1 Narrow and general intelligence\nTask-based approach:\nThe task-based approach is based on how computers are powerful and flexible, but we need to create detailed instructions (code) for them to do great things.\nSimilarly, ML model currently can do great tasks (e.g. Starcraft, Go), but need detailed experience in it first.\nGeneralisation approaches:\nNLP models like GPT-3 can perform great results on language tasks that it has not been trained on. Transfer learning (with little/no finetuning) is an example of generalisation. The model can do unseen tasks with little/no experience from learning general concepts like an understanding of the syntax and semantics of a language!\nHumans can make great progress on the scale of years or decades because of how strong generalisation is. The ability to learn new tasks based on experience, and even share information about new tasks to other humans, has made us very powerful.\nBoth: They are more like a spectrum than discrete approaches.\nAlphaZero trained on data of itself playing against itself, and we evaluate how well it did (task-based because it has been trained/optimised/evaluated on same data). But we made it play against humans (generalisation-based because it has been trained/optimised not on human data and now it is evaluated on unseen human strategies). This example lies on a spectrum between the two approaches.\nThe task-based approach might outperform the generalisation based approach, atleast in the short term, on tasks we care about. For example, driving cars. But on other too complex abstract tasks, like being a CEO in a social human interconnected world, might be better with generalisation based approaches.\nThis is true because normally we need a lot of training data to get an AI to do a task well, and training data for something as abstract as a CEO isn’t easy to come by.\nDoing the entire thing as just ‘CEO training data’ to output ‘CEO actions’ is no good, but splitting up the CEO’s tasks (writing speech, choosing who to hire, etc) could be done with task-based AI, as an aid to a human CEO. Combining all these tasks together for a full task based AI is no easy feat however.\nWe could get superhuman CEOs with generalisation based AI by training it to develop a range of useful and abstract cognitive skills. For example, by living in a simulated world. Even if it’s very different to our own world, the planning and learning concepts developed will transfer over. This is similar to smart human scientists who transfer fields.\nA potential obstacle to generalisation-based AI becoming successful is about the features of the environment being correct for development. For the case of humans, we needed a ‘social arms race’ to give us enough social intelligence to develop large-scale cultural transmission and communities. However, the state of the environment for AI could be purposely modified to be correct for development.\nAnother obstacle could be about the human brain being too specific for an AI to replicate well enough. For example, the quantum properties of neurons. However, the human brain operates under conditions that are too messy for it to be plausible for our intelligence to rely on these specific complex conditions. In other words, the too-hard-to-replicate features of the human brain probably don’t matter to our intelligence, so AI can develop regardless. Consciousness might be too-hard-to-replicate, but does it matter for intelligence?\nWith these obstacles out the way, the author believes it is very likely that eventually we will create AIs that can generalise well enough to perform as well as humans on a wide and varied range of tasks, including abstract ones like being a CEO.\nThese AIs are called artificial general intelligences, or AGIs."
  },
  {
    "objectID": "posts/AIS1.html#core-reading-3-more-is-different-for-ai-steinhardt-2022",
    "href": "posts/AIS1.html#core-reading-3-more-is-different-for-ai-steinhardt-2022",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.3 Core Reading 3: More is different for AI (Steinhardt, 2022)",
    "text": "3.3 Core Reading 3: More is different for AI (Steinhardt, 2022)\n(only introduction, second post, third post)\n\n3.3.1 More Is Different for AI\nTwo approaches about ML safety risks: the Engineering approach and the Philosophy approach.\nEngineering tends to be bottom-up and in touch with current state-of-the-art systems. It looks at issues that are already major problems or are expected to become them.\nPhilosophy tends to be more sci-fi movie like and abstracted and ambitious. It’s about the limits of advanced systems, and not about current ones.\nThey both agree misaligned objects are an important problem, for differing reasons. Philosophy is more based on theoretical problems that could or could not exist in the future.\nFrom the author’s experience, people from the two camps disagree with eachother strongly. Coming in from Engineering, they believe philosophy is significantly underrated by most ML researchers. Simultaneously, the engineering worldview actually assigns/implies a lot of weight onto thought experiments. However, philosophy undervalues empirical data.\nOverall, we don’t have a single good approach to thinking about AI risk. Both have their issues.\n\n\n3.3.2 Future ML Systems Will Be Qualitatively Different\nPhilip Anderson wrote “More is Different”, in it, he argues quantitative changes can lead to qualitatively different and unexpected phenomena.\nFor example, individual water molecules aren’t wet. Enough water molecules are.\nIsn’t this just the same as emergent behaviour?\nSome changes occur very sharply: after a certain point there is a sharp phenomena. This are called phase transitions.\nThere are a few ML areas where “More is Different” or emergent behaviour apply:\nCheap enough hardware allowed the creation of previously impossible ML architectures like large neural networks.\nSometimes accuracy changes with training strangely. For example, the model doesn’t improve for 90,000 steps but suddenly explores at 100,000! In other words, the phase transition is at 100,000 steps.\nThe existence of phase transitions damages the engineering worldview for predictions because it goes completely against extrapolating previous experience. If not after 90,000 steps, why after 100,000 steps? Emergence requires some adoption of the philosophy worldviews. Regardless, the engineering worldview gets surprisingly far.\n\n\n3.3.3 Thought Experiments Provide a Third Anchor\nIt’s hard to predict the future of AI because of emergent behaviour. To remedy this, we think in term of “anchors” reference classes that are broadly analogous to future ML systems. We then use them to make predictions.\nAnchors are reference classes. Let’s all the reference class/anchor for current ML systems as the current ML anchor. It’s good, but fails to account for emergent behaviour.\nOther anchors? Intuitive ones like things humans can do but ML can’t do easily. Liker reading a textbook to learn a new subject, or long-term planning (achieving long term goals). We’ll call this the human anchor.\nThe issue with this is that it risks thinking about model acting too much like humans would.\nAnother anchor, the optimisation anchor, which is associated with the philosophy worldview. Think of ML as a ideal optimiser and ask what such a thing would do in a given scenario. The issue is it ignores practicalities and the fact that the optimisation process can work in a more ‘natural’ way than an ideal one.\nOther anchors to think about how ML acts include evolution and economic behaviour. There’s also biological systems like the immune system, the brain, ecosystems etc. Emergence often appears in these complex systems, so studying them might be a good way to understand emergence in ML.\nThought experiments and anchors are good at predicting problems, but solving them requires an engineering worldview.\nIn-context learning refers to learning done in the field. Like how humans learn on the job, not at uni. E.g. GPT-3’s ability to do few-shot learning."
  },
  {
    "objectID": "posts/AIS1.html#core-reading-4-most-important-century-series-summary-karnofsky-2021a",
    "href": "posts/AIS1.html#core-reading-4-most-important-century-series-summary-karnofsky-2021a",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.4 Core Reading 4: “Most important century” series summary (Karnofsky, 2021a)",
    "text": "3.4 Core Reading 4: “Most important century” series summary (Karnofsky, 2021a)\nThe “Most import century” series of Blog posts argues the 21st century could be the most important for humanity because of the development of advanced AI.\n\n3.4.1 Why?\n\nBecause advances in technology could result anywhere between utopia and dystopia.\nBecause these advances could be sooner than we think. The relevant kind of AI looks like it will be developed in this century.\n\nRight now, humanity is at a turning point. Our actions now could very well dictate the livelihoods of the many more to follow. But we aren’t ready to carry out the actions we should take.\nExponential growth means that this may be the most important century without us realising easily. We went from more growth in millions of years than billions. Then more growth in hundreds of years than millions of years. Perhaps with AI, we could get more growth in hundreds of years in decades.\nWe have no idea what will happen in the ‘long-run’ future. Maybe a few mega-corps own everything and the population are radically exploited in ways previously impossible. Maybe instead we live in a world where nobody has to work and is free to pursue their own passions. We think of this as long off, so don’t bother discussing it.\nHowever, it could come sooner than we think. Standard economic growth models imply that technology that can improve innovation improves growth. Having more economic growth leads to more resources, which leads to more innovative technology, which leads to more growth.\nAI could be the key innovation that makes this cycle break out of orbit. If it could fully automate innovation, then an ‘economic singularity’ would occur with productivity reaching infinity.\nIn history, more growth lead to more innovation (from more people existing, more resources available etc), which lead to more growth. However, recently this cycle has been broken because both the number of people created from growth leveled off, and the resources created from growth went to a select few (rich people) instead of more people. With a similar number of people and worse resource distribution, the pace of ideas slows down and so does growth.\nBut what if AI could do this cycle. An AI that gets resources, improves itself, gets more ideas, and gets more resources as a result.\nSuch an AI is called PASTA: Process for Automating Scientific and Technological Advancement. It would have to be something akin to AGI.\nOne forecasting method states:\nBut no AI model to date has even had 1% of the number of computations as the human brain (but how many of the brain’s computations are necessary for general intelligence?) Also, just by looking at how quickly computational power is increasing, by the end of the century, will it be possible to train human-brain-sized models?\nSo PASTA-like AI is more likely than not this century, atleast nearing the end of it.\nThere is no scientific field of “AI forecasting” nor expert consensus on the issue. This means it’s hard to be confident that the most important century hypothesis is true, but we should assume it is to be prudent, atleast until a mature AI forecasting field develops.\nExamples of things this field cost include are:\n\n\n\nhttps://www.cold-takes.com/making-the-best-of-the-most-important-century/#robustly-helpful-actions.png"
  },
  {
    "objectID": "posts/AIS1.html#core-reading-5-forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell-karnofsky-2021b",
    "href": "posts/AIS1.html#core-reading-5-forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell-karnofsky-2021b",
    "title": "AIS 1: Artificial General Intelligence",
    "section": "3.5 Core Reading 5: Forecasting transformative AI: the “biological anchors” method in a nutshell (Karnofsky, 2021b)",
    "text": "3.5 Core Reading 5: Forecasting transformative AI: the “biological anchors” method in a nutshell (Karnofsky, 2021b)\nWhen will PASTA, capable of doing scientific research automatically, develop? The quicker the more important this decade is.\nBio Anchors is an approach to this to predicting this. It’s a complex approach with a lot of assumptions and estimates. It’s also not as trustworthy as other models.\nNonetheless, it’s the ‘best guess estimate’ we have. This century will likely see us hit some the more extreme milestones in the report.\nModern AI learns by training. By trial and error. Generally, the training process is more costly for larger AI models and more complex tasks.\nBased on the usual patterns on how expensive training is, how much would an AI model as ‘big’ as the human brain cost? And when? From all of Bio Anchors approaches, there is a high level of PASTA/transformative AI occurring: a 70-80% weighted average of transformative AI occurring by 2100.\nBio Anchors’ different approaches are called anchors. Neural net anchors are the most used for prediction.\nThere are two ways to train models:\nProgram in extremely specific step-by-steps instructions. Hardcode them in.\nTrain by trail and error. This is just ML. Just training to update weights. Meta learning is training how to train.\nTrail and error is a brute-force approach and expensive. It requires a lot of data, and a lot of processing power.\nThe cost how train this way depends on the model size, the training data, and the task. For example, a single model now in 2021 has already taken millions of dollars.\nBio Anchors asks when it can be possible to train a model using trial and error to do the hardest tasks humans can do. Like doing research and science.\nIt estimates how expense training costs based on just the model size and task type.\nIn general, a larger model with more parameters can learn more complex tasks, but is more expensive. We estimate today’s AI systems are sometimes as big as insect brains, but never as big as mouse brains. Right now, not even 1% as big as human brains. But it is up to debate how to estimate and compare brain size to AI model size.\nBio Anchors assumes a transformative AI must be atleast 10x times as big as human brains to do complex tasks, because AI brains will be less efficient than human brains.\nThe most contentious part of Bio Anchors is its approach to finding the cost for tasks. There are some tasks a human can do in a second, like image classification. Other tasks like logic puzzles take minutes. Some others take years, like writing science papers. It assumes that the longer tasks will be more vastly costly to train an AI to do.\nHowever, this isn’t as simple. Many of long tasks like writing a paper can be broken into shorter tasks like writing sentences. If an AI can do all short subtasks quickly, can’t it do the long task quickly? And in this case, the AI doesn’t need even need any training of the longer task to do it!.\nThe question is, are the hardest human tasks able to be decomposed to shorter easy tasks?\nHow expensive will an AI model size 10x the human brain, trained on a task where each ‘try’ took days, weeks, or months of ‘thinking’ cost?\nToday, the estimate is roughly a million trillion dollars.\nBut advances in hardware and software, and the growing role of AI in the economy, might make this lower and easier attained.\nBio Anchors states that transformative AI will be developed when this funding amount is met.\nBio Anchors simplifications can be too aggressive (too soon) and too conservative (too late).\nToo aggressive:\nThere’s no reason to think that DL can learn everything a human can do with just trail and error training.  We need fundamental new approaches to AI to imitate human reasoning. Humans have ‘true understanding’. AI does ‘pattern recognition’.  But is there actually a difference between the two? If there isn’t, then humans aren’t special, AI can. We don’t have a good definition of ‘true understanding/reasoning’ to base this argument on.\nThe breakthroughs needed for Bio Anchors to be wrong have to be more than what AI scientists can do. In the scale of multiple decades, researchers might be able to find whatever we’re missing in today’s DL techniques.\nBio Anchors assumes enough computing power will result in transformative AI. But in fact, money alone isn’t enough. It requires human intelligence, will, innovation, and a way to setup the trail and error process correctly.\nConservative:\nPerhaps we don’t need to get it to do full trial and error. We can hardcode some things in and result it in much higher efficiency.\nWe could find better ways of doing trial and error training, like developing meta-learning. Over time, AI becomes bigger in the economy and different AI gets better at different real-world tasks. At some point, we can integrate these different task-specific AI with their own training together. AI which isn’t nearly as good as PASTA, could be so good at doing tasks economically, resulting in funding increasing exponentially.\nThe most important tasks AI needs training to do could be actually easier compared to what Bio Anchors assumes.\nBio Anchors Conclusions. That there’s a <10% chance of transformative AI by 2036. 50% by 2055, 80% by 2100.\nAI systems might be able to be trained soon to be anything humans can do within a second of thought. Like humans who speak with less than 1 second of thought in speaking. Only very recently, AI models have increased in size so much.\nBio Anchors is consistent with what we observe from AI today. We’re getting close to the time where AI model is as big as the human brain. Mainly, the funding argument is simple to believe why it will explode.\nIn order to create transformative AI the computation done in all of evolution must be recreated. This is very conservative. The idea being that whatever is unique to human brains should be rediscoverable if we can rerun natural selection.\nPros and Cons of biological anchors for predictions on transformative AI:\nCons:\nThere are some extremely uncertain estimates and assumptions.\nCan AI learn key tasks just using trial and error? How to compare AI models size with animals and humans. How to estimate task time for to predict expensive. How to estimate future advances in hardware/software. How to estimate funding for AI labs.\nWe shouldn’t consider the forecasts to be reliable given this.\nPros:\nEvery assumption and estimate can be explained discussed and over time, tested. This is very valuable.\nThe framework can give a way of thinking about that simultaneously covers:  a) the fact that AI systems a decade ago aren’t impressive,  b) the fact that AI systems now can do impressive things but not what humans can do,  c) the fact that the next few decades could result in transformative AI being developed. \nBio Anchors concludes that, in the coming decades, AI models will be developed with the ‘size’ of the human brain.\nWe can compare Bio Anchors framework and compare it to what we see and learn better predictions of timelines as a result.\nE.g. How much can trial and error learn?  How the ‘size’ of the AI models completes tasks compared to similar animal sized brains.  Are AI models getting better at the rate the report currently projects?\nThere is no robust expert consensus on this topic."
  },
  {
    "objectID": "posts/MP5.html",
    "href": "posts/MP5.html",
    "title": "Masters Project 25/10/2022",
    "section": "",
    "text": "The fifth post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/MP5.html#during-the-meeting-we-discussed",
    "href": "posts/MP5.html#during-the-meeting-we-discussed",
    "title": "Masters Project 25/10/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nStuart has a stereo microphone to play around with. It can connect to a device and you could test it with a ping pong ball to see differences in phase, amplitude etc in the two channels.\nWe zoomed in on audacity to see the waves of the two channels, looking at the phase difference specifically, but noticing a large different in amplitude.\nThe biology data is only 16khz. The rule is that you need double the sampling rate of the maximum frequency present in an audio file to record it accuracy. This means sounds, like birdsongs, which are likely above 8khz in frequency will not have been recorded with high precision.\nThe idea behind stereo data is about this extra information a model could discern such as directionality. This information could be used both to improve bird classification, but also to discern, in the case of multiple birds of the same species singing, that there are indeed multiple birds.\nInstead of training a model with (specific) stereo data for our purposes, we could take a different approach and instead use an already good classification model to classify birds from certain segments of soundscapes. We would then look into these segments and try to analyse them with concepts such as directionality and so on.\nI expressed my concerns about having a lack of data to train a model from scratch. The concern being that the specifics of a recording setup, I.E, the type of microphone(s), the distance between them, the positive of the device relative to other objects such as trees and the ground, etc, matter too much in order to extract sensitive data such as directionality if we used data from different setups.\nStuart suggested that if a model trained on different setups, these setup specifics might not matter, as the model would be able to be generalised to work regardless, but I expressed that this would require not just a lot of data, but a lot of data where we know the setup specifics in order to understand what the model is training on.\nI have a underlying concern that there might be some specific we would overlook since recording setups don’t appear to be a simple field in practice (but are initially in theory).\nInstead of getting the aim of this project to be useful immediately, Stuart said that instead we could make it more of a first step for further research to build on. For example, “If we setup stereo microphones in this way and do this analysis with this model, we find X, in the future, more people can copy this setup and improve our results to possibly find Y”.\nDirectionality however might be okay because instead of absolute directionality, we could find relative directionality, which doesn’t matter depending on the distances between the microphones.\nI said that I found it hard to find papers about using stereo data for classification, but Robert has found some. Specifically, he said he would send me a literature review about it, which should be helpful.\nWe then move on to talk about the possibility of using stable diffusion to generate soundscapes.\nRobert found the example prompt I gave very funny because that combination of birds and environment doesn’t make sense. This is entertaining, but also important, because generating real possible prompt combinations might be really important so that models can train properly on them. Generating purposely really fake prompt combinations somehow might also help. In either case, some domain knowledge from Robert or the biology team would be needed.\nAnother reason why generating synthetic data is a good idea is because some models might struggle with the lack of training data for rare/exotic species. This approach might help to aid that significantly.\nA spectrogram shows frequency and time, with amplitude (power) as shaded. A 3D stable diffusion model might be better because instead of shades to show amplitude, it has another dimension for it.\nIn theory, we agree that it is a good idea, and regardless of whether it is too ambitious, masters projects are allowed to be exploratory.\nIt’s very ambitious, but it would be nice to publish a paper even if this doesn’t work out because the idea and exploration might be very helpful.\nOne really interesting observation Stuart made was that a mask of a subject in a normal image would be really similar to a mask of a birdsong in a synthetic spectrogram. In fact, isn’t the latter just then a birdsong classification model?? I need to throughly read the recent DiffEdit paper which generates masks automatically for images. Their mask generation model might be able to be turned into a soundscape birdsong classification model!\nHowever, by Christmas holiday (6 weeks) I need to have some real work done rather than just learning. Masters projects will be accessed not necessarily by learning, but by exploration of ideas that needs to be written out.\nBy Christmas, I would like to at least have a stable diffusion model that can generate individual bird song (not soundscapes)."
  },
  {
    "objectID": "posts/MP5.html#stable-diffusion-mechanics",
    "href": "posts/MP5.html#stable-diffusion-mechanics",
    "title": "Masters Project 25/10/2022",
    "section": "4.1 Stable Diffusion mechanics",
    "text": "4.1 Stable Diffusion mechanics\nStable diffusion heavily relies on two things: the data it has been trained on, and the prompt it is given to generate a given image.\n\n4.1.1 Training\nCurrently as far as I can see, stable diffusion model online have been trained on ‘normal’ images in order to generate ‘normal’ artwork. It’s hard to define what normal is, but what it isn’t, is spectrograms of birdsongs and environments. My approach would likely require training a stable diffusion model from scratch on spectograms, of either relevant spectrograms or other audio, because intuitively, fine-tuning pretrained wouldn’t work well enough. Regardless it’s worth trying just too see results however.\nTo train a model yourself takes two things: enough data and enough computation. There should be enough data online given the size of xeno-canto. Computation is the harder demon. Being at a university fortunately might resolve this for me. There are computing clusters available for research use, so in theory it should be possible to request time for one. Alternatively, it is possible to rent GPU time from companies, but this would likely be very expensive, so would require research funding.\nHowever, there might be a way to decrease computational needs: \nTo lower the computation needed to train a diffusion model, we use latent (compressed) representations of images. In fact, stable diffusion itself is a latent diffusion model rather than a general diffusion model specifically because of this. The autoencoder (vae) in stable diffusion is what does this image compression, and it makes a significant difference, reducing memory requirements for a 512x512x3 image by 48 times, speeding up training and inference (image creation) significantly.\nStable Diffusion is based on latent diffusion. It was proposed in a paper High-Resolution Image Synthesis with Latent Diffusion Models at https://arxiv.org/abs/2112.10752.\nWhat if there is some specific autoencoder approach for spectrogram generation that could decrease computational needs significantly?\n\n\n4.1.2 Prompts (and labels)\nThe entire point of this soundscape approach is to create labeled soundscapes because there are not enough available. If stable diffusion could create soundscapes, but not labeled ones, the whole approach falls out. Fortunately, the way stable diffusion creates images, using prompts, might also conveniently be the answer the answer to this issue.\nAn example I found on https://lexica.art/ at https://lexica.art/prompt/ea5b8646-6e6e-4a0e-b618-bae8c796f8cc of a generated image is as follows:\nPrompt: “Scifi art by greg rutkowski, a man wearing futuristic riot control gear, claustrophobic and futuristic environment, detailed and intricate environment, high technology, highly detailed portrait, digital painting, artstation, concept art, smooth, sharp foccus ilustration, artstation hq”\nImage:\n\n\n\nimage.png\n\n\nNot all prompts are like this, in fact, thinking about what prompts to use in itself is another entire process since it’s surprisingly not intuitive or easy to write good prompts.\nRegardless, looking at our prompt, it also has words that can be related to labels.  “Scifi art”, “man”, “futuristic riot control gear”, “claustrophobic and futuristic environment”, “high technology”, “highly detailed portrait”, etc. Some words aren’t as useful, like “detailed and intricate environment” as it doesn’t give much information. Some words like “by greg rutkowski” are telling of the artist this was based on.\nIf we had a soundscape generation model, we could use a prompt I’m making up to illustrate like:\nPrompt: “Forest enviroment, a Barn owl singing lightly at the start, lightly noisy environment, near a river, a Black grouse singing throughout, highly detailed, soundscape”\nThis prompt would include information about the labels we want, namely, “a Barn owl singing lightly at the start”,“near a river”, “a Black grouse singing throughout”.\nThis does mean however, that a model trying to use these soundscapes for training needs to be able to know how to use these labels properly, which is different from using the labels in BirdCLEF 2022 to train for instance. There is also the natural concern of whether the labels from the prompts are correct (enough) to make soundscape generation method good enough to be useful training for real soundscapes.\n\n\n4.1.3 An ambitious addition\nA spectrogram is a 2D (or 3D) visualisation of a sound, it is an image.  In the example given I used a 2D visualisation, but what about doing all of this in 3D? \nRecently, Google released a paper about 3D image generation! That is creating 3D images from a written prompt. 2 minute papers has a brilliant video on it https://www.youtube.com/watch?v=L3G0dx1Q0R8. They call it “DreamFusion”.\nUnofficial open source DreamFusion using stable diffusion is already becoming available. https://github.com/ashawkey/stable-dreamfusion. I really do wonder if using a 3D spectrogram to generate soundscapes would be better than a 2D one.\n\n\n4.1.4 Sound to Spectrogram and Spectrogram to Sound conversion.\nWhether or not it’s possible to turn a spectrogram back into audio might not actually matter too much.  What I mean is, to classify the birds in a given test spectrogram, we simply convert it into a spectrogram and then use a image model trained on only spectrograms to classify it. I think there are CNN based papers that do this.\nThere is also the idea that we can modify the test soundscape/spectrogram to be more similar to the training ones on purpose. Maybe we can use another model to do this."
  },
  {
    "objectID": "posts/MP5.html#by-audio-diffusion",
    "href": "posts/MP5.html#by-audio-diffusion",
    "title": "Masters Project 25/10/2022",
    "section": "5.1 By audio diffusion:",
    "text": "5.1 By audio diffusion:\n\n5.1.1 Harmonai\nStability.ai released stable diffusion. They also work in other areas, including AI in biology and AI in audio.\nLast month they released Harmonai, an open source generative audio tool, dance diffusion. Dance diffusion allows you create music. It’s a digital music production tool. More info can be found at a wandb.ai blog post https://wandb.ai/wandb_gen/audio/reports/Harmonai-s-Dance-Diffusion-Open-Source-AI-Audio-Generation-Tool-For-Music-Producers–VmlldzoyNjkwOTM1, at Harmonai’s website https://www.harmonai.org/, and their GitHub https://github.com/Harmonai-org/sample-generator. There’s also a guide to using it at https://drive.google.com/file/d/1nEFEpK27v0nytNXmmYQb06X_RI6kKPve/view.\nI haven’t yet throughly investigated the use of dance diffusion, but:\nThe latter guide detailed an interested model checkpoint (a pretrained model to fine-tune). It is honk-140k, trained on recordings of the Canada Goose from xeno-canto. This implies that it’s possible to generate birdsong with it, once trained.\nBut whether it’s possible to do labeled soundscape creation from individual labeled audio labels is my concern. About the labels, dance diffusion doesn’t seem to use prompts like stable diffusion, it seems to just create new sounds based on the trained data. This isn’t particularly useful because there is already enough birdsong available online.\nRegardless, understanding how dance diffusion generates sound might yield some new ideas about how to use stable diffusion to do. Specifically, how it handles audio data. Perhaps I would find a way to to do labeled soundscape creation with it once I know how it works.\n\n\n5.1.2 The Generative Landscape\nThere is a course online at https://johnowhitaker.github.io/tglcourse/. It covers all types of diffusion generation, including image and audio.\nLesson 15 at https://johnowhitaker.github.io/tglcourse/dm4.html is about Diffusion for Audio on Class conditioned birdcalls. The course is not yet complete yet, but should be soon.\nThe course is created by Jonathan Whitaker, who also is contributing to fast.ai part 2, so should be of great quality."
  },
  {
    "objectID": "posts/MP5.html#by-other-generation-methods",
    "href": "posts/MP5.html#by-other-generation-methods",
    "title": "Masters Project 25/10/2022",
    "section": "5.2 By other generation methods:",
    "text": "5.2 By other generation methods:\nThis would be doing the process by combining previous audio data together to create soundscapes.  Unlike diffusion methods, this isn’t as new as an idea, and has likely been tried and tested before. Because diffusion is so new, I’m more attracted to it as a novel idea.\nHowever there are some things I could learn from these approaches.\nThe Earth Species Project (https://www.earthspecies.org/) released a paper about BioCPPNet at https://www.nature.com/articles/s41598-021-02790-2. This is about solving the cocktail problem to tell apart sounds from a group of animals of the same species. For example, to tell which individual is speaking from a group of macaques monkeys.\nThey have a video explaining BioCPPNet at https://www.youtube.com/watch?v=TGWFr-6JCDk. In particular at the 1 minute mark, they state “We implement a supervised training scheme: we construct a synthetic mixture dataset by additively overlapping signals”.\nThis is synthetic mixture dataset creation, which could be similar to soundscape creation, so could be very useful to understand."
  },
  {
    "objectID": "posts/MP5.html#motivation",
    "href": "posts/MP5.html#motivation",
    "title": "Masters Project 25/10/2022",
    "section": "6.1 Motivation",
    "text": "6.1 Motivation\nThe paper (likely outdated, 2017) Multi-band Approach to Deep Learning-Based Artificial Stereo Extension, attempts to use machine learning to turn mono audio into stereo audio.\nIt motivates that: “It is well known that stereophonic sound provides a more pleasant and natural experience than monaural (monophonic) sound on account of the presence of spatial information containing both ambience and/or the distinguished relative positions of objects and events”.\nThe idea behind using stereo data for birdsong classification is that this extra presence of spatial information, the information about relative positions of objects and events, would help."
  },
  {
    "objectID": "posts/MP5.html#stereo-data-availability",
    "href": "posts/MP5.html#stereo-data-availability",
    "title": "Masters Project 25/10/2022",
    "section": "6.2 Stereo Data Availability",
    "text": "6.2 Stereo Data Availability\nBirdCLEF 2022, and other datasets, use data from xenocanto, so it is worth looking through it. Ideally, there would be a search tag to find stereo data.\nOn xenocanto records, under Technical details, it states various details. Here is an example from https://xeno-canto.org/757580. ## Xenocanto Technical Details - File type mp3 - Length 24.7 (s) - Sampling rate 44100 (Hz) - Bitrate of mp3 258189 (bps) - Channels 2 (stereo) - Device not specified - Microphone not specified - Automatic recording yes\nIt does tell us the number of channels and so whether it is stereo or mono. This example is missing the device and microphone, but I found another with a iphone using a Echo Meter Touch 2 Pro. There are also useful properties on xeno-canto like the recording quality and environment, as well as the type of bird sound. Whether it is a flight call or dawn song etc.\nXeno-canto doesn’t just have individual bird recordings, but also soundscapes that I suspect will be mostly unlabeled.\nBut how about searching for stereo data in bulk?\nhttps://xeno-canto.org/help/search states how to do an advanced search. Entries are tagged, and you search through tags with tag:searchterm. Available tags include the country, the geographic coordinates, whether there are other species in the background, the recording quality. A limitation is that I cannot see how to search for specifically mono or stereo data. There is a tag for the device, the microphone, and the sampling rate, but not the number of channels. I can instead search using the remarks tag (the comments from the uploader) and the mic tag for stereo microphones.\nLooking at the API at https://xeno-canto.org/explore/api, it has dvc: recording device used, mic: microphone used, smp: sample rate, but not explicitly the number of channels. Perhaps if I contact xeno-canto they will know of a way to look for just stereo data.\nAnother website Robert linked to me, freesound.org, explicitly has a stereo tag! https://freesound.org/browse/tags/stereo/. However, it only has 5093 recordings, and if I add ‘bird’ as a tag, only 241. There might however be other datasets for stereo data.\nSince there are ways to record stereo data, I will assume the type of microphone used really matters to process delicate information like the relative positions of objects and events. This means that data might be very limited in supply. Either I can train a model on just general stereo audio and fine tune to see if it works for a specific microphone, or train on mono audio and fine tune, or train on some combination and fine tune."
  },
  {
    "objectID": "posts/MP5.html#papers",
    "href": "posts/MP5.html#papers",
    "title": "Masters Project 25/10/2022",
    "section": "6.3 Papers",
    "text": "6.3 Papers\nThe first paper in this section was about turning mono data into stereo data. There is also another paper at https://www.researchgate.net/publication/352807819_Identification_of_Fake_Stereo_Audio_Using_SVM_and_CNN, which tries to identify stereo audio data created from mono audio data. In theory you could use the two to improve a model’s ability to generate stereo data from mono data.\nClassification of Bird Species using Audio processing and Deep Neural Network, at https://ieeexplore.ieee.org/abstract/document/9917735:\nIt discusses audio feature extractors like the spectrogram and Inverse Short Time Fourier Transform, as well as different ML approaches as related work.\nThey (frustratingly) don’t state their dataset’s name, only that it’s on Kaggle and that it’s 23.5gb. I searched through Kaggle and couldn’t easily find it based on that.\nRegardless, in their dataset, 11472 audio files were mono, and 9903 stereo. Not a big difference. THere might be enough stereo data available, but not enough from a specific microphone if that is what is needed.\nBirdCLEF 2022’s dataset doesn’t even include whether the files are stereo or mono in its metadata.csv file."
  },
  {
    "objectID": "posts/MP5.html#conclusion",
    "href": "posts/MP5.html#conclusion",
    "title": "Masters Project 25/10/2022",
    "section": "6.4 Conclusion",
    "text": "6.4 Conclusion\nUsing stereo data might a good approach to the cocktail problem, but I’m unsure whether it would be novel because stereo data has been available for some time. Looking online for “Stereo vs Mono data for audio classification” is surprisingly dry for papers on Google and Google Scholar?\nBecause of its delicacy, stereo data might need to be collected from a specific microphone setup to extract it’s delicate information. Xeno-canto doesn’t seem to provide an easy way to search for stereo data, but does allow microphone searches. There is a concern that limiting this project to a specific stereo microphone will limit it’s usefulness for others."
  },
  {
    "objectID": "posts/MP5.html#biosciences-recordings",
    "href": "posts/MP5.html#biosciences-recordings",
    "title": "Masters Project 25/10/2022",
    "section": "6.5 Biosciences recordings",
    "text": "6.5 Biosciences recordings\nFrom the biology team, I have received some data of audio recordings.\nThe data in total 3.54 gb in size. There are 8 files, all .wav. Each file is 16000hz with two channels (stereo).  I used audacity to have a look at some of the files.\nThe recordings are from UK woodlands and from a couple of scrub (the wildlife habitat) sites. They are examples of the kind of typical data the biology team has collected. They’ve collected this data for 3-4 years daily during Jan-Jun for about 3-4 years from about 20 different locations.\nWhat stands out from this dataset to me is the amount of time it has been collected over. If there was a good model to classify birds, it would be really interesting to see how the number of birds changes over the years in these varied locations. This could yield insights into changes in biodiversity due to climate change for example. An alternative project direction would be rather than trying to solve the cocktail problem, to understand and use someone else’s solution to do analysis on it’s results over time.\nThe audio files are not labeled. However I recall the biology team had a PhD student who handlabeled some data for them. This would be useful to look at, however due to how much time and effort it takes to handlabel soundscapes, there is most likely not enough data to train a model from scratch on.\nSteve commented on the dataset:  “Attached are a selection of woodland audio files to have an initial play with, all from 2017, one from each site in mid-May. They are 2hrs 15mins long and start approx. 45mins before sunrise. So, the first 30 mins is often quiet, and things get gradually noisier and more complex thereafter. We have these data for 3-4 years, daily from about Jan-Jun for about 20 sites. Sites here include Abernethy Forest RSPB , Durham Uni Woodland, Minsmere RSPB, The Lodge RSPB, RSPB Wood of Cree, RSPB Ynis Hir, all from around mid-May. Also included, for slight contrast are a couple of scrub habitat sites (Green Farm nr Durham, Pinnock Hill near Durham)”"
  },
  {
    "objectID": "posts/MP5.html#papers-about-birdclef-competitions",
    "href": "posts/MP5.html#papers-about-birdclef-competitions",
    "title": "Masters Project 25/10/2022",
    "section": "8.1 Papers about BirdCLEF competitions",
    "text": "8.1 Papers about BirdCLEF competitions\nAn immensely useful find this week are papers describing the different approaches the competition teams for BirdCLEF used.\nThe most recent: Overview of BirdCLEF 2022: Endangered bird species recognition in soundscape recordings, at http://ceur-ws.org/Vol-3180/paper-154.pdf.\nFor my soundscape generation approach, perhaps the most useful paragraph is found searching for ‘data augmentation’:\n“Sampathkumar & Kowerko [15]: Data augmentation is an important processing step in bird sound recognition because of the domain shift between training and test recordings. In their work, this team focused on evaluating the best augmentation scheme for this task. Most transformations focus on adding different patterns of noise to the source recording, thus emulating noisy soundscape recordings. While the authors find that all augmentations methods improve the baseline experiment, Gaussian noise, loudness normalization and tanh distortion appear to be most impactful.”\nMost approaches added noise to the training data to emulate noise in the test soundscape. They did not create synthetic soundscapes entirely like I proposed. Gaussian noise, loudness normalization and tanh distortion appear to be the most useful noise to add.\nAnd in general, the conclusion is useful:\n“Despite being set up as a few-shot learning task, few teams decided to employ techniques other than CNNs. Pre-trained neural networks for image recognition still dominated the task, and participants tried to cope with the lack of training data through intensive data augmentation and transfer learning. Surprisingly, there was only a weak correlation between the number of training samples and overall per-species performance. This indicates that other factors - such as repertoire size and call patterns - might outweigh training data quantity. Automatic detection of endangered and rare species remains challenging. Still, this year’s competition demonstrated that passive acoustic monitoring combined with machine learning could already be a powerful monitoring tool for some endangered species. BirdCLEF continues to engage a large number of data scientists from around the world to develop new and effective acoustic analysis solutions that aid avian conservation.”\nThere’s a lack of training data because BirdCEF doesn’t provide all of the data available on xeno-canto, which isn’t an issue for my project. Somehow more training samples per species didn’t correlate strongly with better species identification, because of other factors. Most teams tried using CNNs, but not with spectrograms.\nThe competition overview does well to motivate my soundscape creation approach:\n“In recent years, research in the domain of bioacoustics shifted towards deep neural networks for sound event recognition [7, 8]. In past editions, we have seen many attempts to utilize convolutional neural network (CNN) classifiers to identify bird calls based on visual representations of these sounds (i.e., spectrograms) [9, 10, 11]. Despite their success for bird sound recognition in focal recordings, the classification performance of CNNs on continuous and omnidirectional soundscape recordings remained low. Passive acoustic monitoring can be a valuable sampling tool for habitat assessments and observations of environmental niches, which often are threatened. However, manual processing of large collections of soundscape data is not desirable, and automated attempts can help to advance this process [12]. Yet, the lack of suitable validation and test data prevented the development of reliable techniques to solve this task.\nBridging the acoustic gap between high-quality training recordings and complex soundscapes with varying ambient noise levels is one of the most challenging tasks in the domain of audio event recognition. This is especially true when the amount of training data is insufficient, as is the case for many rare and endangered bird species around the globe. Despite the vast amounts of data collected on Xeno-canto and other online sound libraries, audio data for endangered birds is still sparse. However, those endangered species are most relevant for conservation, rendering acoustic monitoring of endangered birds particularly difficult.”\nI should reading reference [12] of the paper to get a better idea of the soundscape availability problem.\nSearching for ‘diffusion’ within the paper yields no results, implying further that indeed using diffusion to generate soundscapes is a novel approach."
  },
  {
    "objectID": "posts/MP5.html#finished-fast.ai-lesson-10",
    "href": "posts/MP5.html#finished-fast.ai-lesson-10",
    "title": "Masters Project 25/10/2022",
    "section": "8.2 Finished fast.ai lesson 10",
    "text": "8.2 Finished fast.ai lesson 10\nThis lesson was really useful in consolidating understanding about how stable diffusion works. It also starts the hard but rewarding journey of programming entire Python modules/frameworks from scratch, a skill likely vital further on in this project. The fast ai lessons for this advanced course are very demanding. They include a 2-2.30 hour lecture and plenty of homework, with the course creator Jeremy stating that he expects each lesson to take around 10 hours.\nMy blog for this lesson can be found at https://exiomius.quarto.pub/blog/posts/2022-09-27-Lesson10Blog.md.html."
  },
  {
    "objectID": "posts/MP5.html#updated-and-fixed-blog",
    "href": "posts/MP5.html#updated-and-fixed-blog",
    "title": "Masters Project 25/10/2022",
    "section": "8.3 Updated and Fixed blog",
    "text": "8.3 Updated and Fixed blog\nI was having trouble with my old fastpages based blog not displaying posts with maths and images correctly. To remedy this, I created a blog using Quarto instead. This post is on the new blog, and I transferred all the old posts here too."
  },
  {
    "objectID": "posts/MP5.html#investigated-durham-uni-supercomputers",
    "href": "posts/MP5.html#investigated-durham-uni-supercomputers",
    "title": "Masters Project 25/10/2022",
    "section": "8.4 Investigated Durham Uni Supercomputers",
    "text": "8.4 Investigated Durham Uni Supercomputers\nThe computer science department has a list of machines at https://www.durham.ac.uk/departments/academic/computer-science/about-us/facilities/. I met a student who used one for some machine learning research. As I’m a student at both the physics and computer science department, I could possibly use the physics department’s supercomputers too, if they are suited towards ML.\nFor my purposes, Bede is GPU based and the description states it is ideally suited towards ML. Bede is shared between multiple universities. At Durham, the main contact Dmitry Nikolaenko at durham-bede-support@n8cir.org.uk.\nLearning how to use Bede, as its Linux based, and how/where to store the training data etc is going to be a task within itself."
  },
  {
    "objectID": "posts/MP5.html#found-yet-more-datasets",
    "href": "posts/MP5.html#found-yet-more-datasets",
    "title": "Masters Project 25/10/2022",
    "section": "8.5 Found yet more datasets",
    "text": "8.5 Found yet more datasets\nhttps://github.com/AgaMiko/bird-recognition-review has useful resources for birdsong classification and yet more datasets.\nThis blog post, https://towardsdatascience.com/sound-based-bird-classification-965d0ecacb2b, also contains an approach, but also a nice introduction to the problem."
  },
  {
    "objectID": "posts/MP5.html#found-another-another-thesis",
    "href": "posts/MP5.html#found-another-another-thesis",
    "title": "Masters Project 25/10/2022",
    "section": "8.6 Found another another thesis",
    "text": "8.6 Found another another thesis\nBird Species Classification And Acoustic Features Selection Based on Distributed Neural Network with Two Stage Windowing of Short-Term Features at https://arxiv.org/ftp/arxiv/papers/2201/2201.00124.pdf by Nahian Ibn Hasan like last week’s thesis, describes another good introduction to the problem and approach."
  },
  {
    "objectID": "posts/MP6.html",
    "href": "posts/MP6.html",
    "title": "Masters Project 1/11/2022",
    "section": "",
    "text": "The sixth post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/MP6.html#during-the-meeting-we-discussed",
    "href": "posts/MP6.html#during-the-meeting-we-discussed",
    "title": "Masters Project 1/11/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nThere’s a webinar “Statistical Methods Seminar Series” by the Ecological Forecasting Initiative on 7th November. It’s mainly in R, but might be worth attending regardless.\nIn the Stable Diffusion prompts, instead of just the bird name, it could be better to state the type of sound its creating, e.g. “dawn song”. Actually this further motivates my approach, because having the flexibility to create custom training data with these niches included could greatly improve model training. Normally training just ignores the type of song and just separates by species. If a model could the training data to incorporate the type of song too, it could be better. For a soundscape, a model could first try and predict the type of song, and use that to better inform the type of bird.\nFound a blog about the difference between spectrograms and sonograms. Essentially they’re similar, but sonograms refer often more to Medicine, both are used interchangeably, I.E, you could refer to a spectrogram by saying sonogram in scientific literature. https://bioacousticsprocrastinator.blogspot.com/2014/10/spectrogram-vs-sonogram.html\nQuestions about the information lost or possible to deduce from spectrograms.\nFor a stereo recording, if an event occurs, say a tree falling and making a sound, can you find the phase difference between the two signals?\nFor a stereo recording, if an event occurs, say two signals passing through the same point at a different time, can you find the time difference? A brief estimate gave a 10ms time difference, which should be possible to deduce from a spectrogram, even at low sampling rates.\nTo be able to deduce this ms time difference, the spectrogram window size(s) must be small enough.\nFor a spectrogram, as the sampling rate decreases, the frequency band ends up being smeared out. It’s like an uncertainty principle. Increased time resolution decreases the frequency resolution.\nStuart advised making your own Python code to do spectrograms instead of using others’. However, not recreating certain things like .fft because they are too difficult.\nFourier transforms in Python are easy. Say you have an audio file A(t). A(t).fft() Fourier transforms it, but then you need to shift it back to get it in the middle because .fft() also shifts it.\nAs practice, try to .fft a sign wave and make a spectrogram.\nNow use two sign waves, and look how their frequencies look on the spectrogram.\nAlso, there is an app we demoed, Spectroid. It makes spectrograms in real time. Stuart whistled and we could see it clearly. Try and recreate it’s function.\nWe talked about the assessment:\nWell made and explained figures are really important. Never go below font 10/9 on figures. They shouldn’t be much smaller than the main text. A lot of Physics department specific criteria like figure creation and reference style you need to learn.\nMake the 10 page report in a way to get the most useful feedback from it. Make it contain a little bit of each section so you can get critic on them. The department says that you might write and use your 10 page report for the start of your diss but Robert says this most often isn’t the case.\nAbout the project:\nBefore I can train a Stable Diffusion Model on spectrograms, first I need to properly learn about and decide how to create the spectrograms.\nUntil the term ends, try and create deliverables that we can store so there’s less pressure later and I can be more exploratory."
  },
  {
    "objectID": "posts/MP6.html#that-leaves-four-possible-combinations",
    "href": "posts/MP6.html#that-leaves-four-possible-combinations",
    "title": "Masters Project 1/11/2022",
    "section": "4.1 That leaves four possible combinations:",
    "text": "4.1 That leaves four possible combinations:\n\nFinetuning a regular pretrained model on only birdsong to generate only birdsong.\nFinetuning a regular pretrained model on birdsong and environmental noise to generate environmentally noisy birdsong.\nTraining a model from scratch on only birdsong to generate birdsong.\nTraining a model from scratch on birdsong and environmental noise to generate environmentally noisy birdsong.\n\nThe idea of my entire approach is that the second and fourth combinations can be used/adapted to create entire soundscapes.\nAs an example, if a prompt was “A American Robin singing in a forest environment” to generate a spectrogram, the model would also be able to generate a soundscape spectrogram from the prompt “A American Robin singing in a loud forest environment followed by a American Goldfinch singing in a quiet forest environment”.\nThe first thing I will attempt is the first combination. It is very questionable whether a normal pretrained model can be finetuned to generate spectrograms as spectrograms are radically different to normal images. To do so, we first must consider what method to finetune a pretrained model with."
  },
  {
    "objectID": "posts/MP6.html#finetuning",
    "href": "posts/MP6.html#finetuning",
    "title": "Masters Project 1/11/2022",
    "section": "5.1 Finetuning",
    "text": "5.1 Finetuning\nWe take a pretrained model, cut off its head (the last node: the output node), and create a new head. We then finetune the whole model with new data: our own images and captions. This makes the new head produce outputs deliberate for our interests."
  },
  {
    "objectID": "posts/MP6.html#textual-inversion",
    "href": "posts/MP6.html#textual-inversion",
    "title": "Masters Project 1/11/2022",
    "section": "5.2 Textual inversion",
    "text": "5.2 Textual inversion\nTextual inversion (TI) is a special type of finetuning. The process is to create a new model embedding for some concept we want in some new data we have. For example, we have watercolour-styled images and we want the model to learn how to generate styled images.\nWe start by adding a new token, ‘watercolour’, with a corresponding new embedding to our text model. Then we train the new embedding on our watercolour images. Once the model has been trained, it ‘knows’ what watercolour means. Now we can write prompts with it as we please. For example: “Woman in the style of watercolour”.\nAn embedding is a store of relations between images and words in the CLIP subsection of our stable diffusion model. Simply put, it’s how the model stores knowledge to ‘know’ what words are associated to what images. For textual inversion, we create a new embedding and train the model so that it ‘knows’ what watercolour ‘means’.\nA token is easiest understood to be a written word that corresponds to a embedding. In the prompt “Woman in the style of watercolour”, watercolour is a token and we trained a corresponding embedding for it."
  },
  {
    "objectID": "posts/MP6.html#dreambooth",
    "href": "posts/MP6.html#dreambooth",
    "title": "Masters Project 1/11/2022",
    "section": "5.3 Dreambooth",
    "text": "5.3 Dreambooth\nDreambooth is similar to textual inversion. Instead of making a new token/embedding set, it finds an existing token/embedding set that is barely used, then finetunes it with our new data. This works because some token/embedding sets are rarely used in prompts as an artifact of how they are created."
  },
  {
    "objectID": "posts/MP6.html#aesthetic-gradients",
    "href": "posts/MP6.html#aesthetic-gradients",
    "title": "Masters Project 1/11/2022",
    "section": "5.4 Aesthetic Gradients",
    "text": "5.4 Aesthetic Gradients\nAesthetic Gradients (AG) is different to the previous techniques because it allows you to generate your own style without finetuning or needing large computation. Both textual inversion and Dreambooth are intended to add the ability to generate new objects. In contrast, AG is designed to implement the ability to create new styles, not new objects.\n\n5.4.1 AG Example\nThe AG paper at https://arxiv.org/pdf/2209.12330.pdf gives some examples of the difference between original Stable Diffusion (SD) and Stable Diffusion with AG. The examples showcase AG’s ability to generate desired new styles of images.\n\n\n\nIvan Aivazovsky Imitation\n\n\nWe want our model to create a painting of a tree that looks like it was drawn by the artist Ivan Aivazovsky.\nThe first image is original SD creating an image just with the prompt “A painting of a tree, oil on canvas”. There is no link to Aivazovsky in it.\nThe second image is original SD creating an image with the modified prompt “A painting of a tree, oil on canvas, by Ivan Aivazovsky”. We have trained original SD on 5 paintings by Aivazovsky using the SD’s default training mechanisms. Notice how it is only slightly changed.\nThe third image is AG implemented to SG. With only 5 paintings it had dramatically changed the style of the image. Notice that the prompt is unchanged from the first image’s: “A painting of a tree, oil on canvas”.\nThe paper also shows other examples where instead of 5 paintings (images) of the new style, it uses 100. The trend is still the same: using SD to append the style keyword to the prompt has little effect compared to using AG to imitate a given style.\n\n\n5.4.2 AG Discussion\nWe could use AG to implement a ‘spectrogram’ style to our images. This would simplify prompt creation because we would no longer have to specify the style using a keyword. Furthermore, it appears that AG is better at changing a model to be able to implement dramatic shifts in style, and creating images of spectrograms is indeed a very dramatic shift.\nOne prudent concern about AG as a finetuning method is it’s prompt reduction method. In our example, all prompts we enter into our model now will be in the style of Ivan Aivazovsky. This could be an issue because say we want the same model to produce images in the style of another artist. We would have to start over and use a new model to do so. This is unlike the usual stable diffusion modified prompt method in which adding “by Ivan Aivazovsky” to any given prompt modifies the style of the generated image. With this method, we could use the same model to generate images in the style of many artists, not just one.\nInstead of having one spectrogram style for all generated prompts. Perhaps we could make a style for each spectrogram of each species of bird, and each spectrogram of each type of environmental sound. AG might be incompatible for this approach.\nTo use AG, a user recently implemented it into his stable diffusion repository at https://github.com/AUTOMATIC1111/stable-diffusion-webui with discussion at https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2585. This user in particular, AUTOMATIC1111, appears to implement new papers very quickly, so is worth following to see updates in the field. His repository has a plethora of papers and techniques implemented which are worth investigation.\nThe AG preprint paper was released 25th September. It is unlikely that mature research to use it to generate spectrograms has been done or even thought about.\nAesthetic Gradients can also be used not as an alternative but as an addition to Dreambooth or TI. A example is given here https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/3350.\nMore information about AG can be found at a blog post https://metaphysic.ai/custom-styles-in-stable-diffusion-without-retraining-or-high-computing-resources/ and at the official author’s Github https://github.com/jmoraes7/stable-diffusion-aesthetic-gradients."
  },
  {
    "objectID": "posts/MP6.html#limitations-of-finetuning",
    "href": "posts/MP6.html#limitations-of-finetuning",
    "title": "Masters Project 1/11/2022",
    "section": "5.5 Limitations of finetuning",
    "text": "5.5 Limitations of finetuning\nNaturally, the limitations are dependent on the approach specified. I will however write some general concerns:\n\n5.5.1 Limitations:\n\nIf a model has only been trained on some type of data there is no guarantee that finetuning it with new data will result in good results.\nFinetuning methods tends to be reliant on the specifics of this new data. For example, to finetune an image of myself it is best to give images of me in different poses and environments. Furthermore, the number of images given matters too.\nFinetuning can be computationally expensive. It requires too much computation for my laptop. To remedy this, I have been using an online GPU service called Paperspace. Regardless it still can takes hours to finetune a model even using a 30Gb GPU."
  },
  {
    "objectID": "posts/MP6.html#paperspace",
    "href": "posts/MP6.html#paperspace",
    "title": "Masters Project 1/11/2022",
    "section": "7.1 Paperspace",
    "text": "7.1 Paperspace\nPaperspace is an online service for a programming environment and GPU usage. This costs money depending the service: there are free limited options available, but paid options too. For testing, I paid to use the cheapest GPU available, a 30Gb P4000 for about 0.4$/hr. Thankfully I have some free credit from a fast.ai offer. It is worth noting however that Paperspace advises using a more expensive GPU is more cost effective for larger projects.\nThe first thing I tried was AUTOMATIC1111’s popular stable diffusion implementation. Paperspace themselves have a guide to do so at Paperspace have a guide at https://blog.paperspace.com/stable-diffusion-webui-deployment/.\nThe guide’s method of deployment is extremely limited. You cannot modify the files or implementation, making it impossible for me to install Aesthetic Gradients. However, it comes with Textual Inversion. It is also unable to use Dreambooth.\nA reminder to myself is when I’m done with my session, to end it by changing the number of replicas from ‘1’ to ‘0’, to avoid wasting money.\nA separate Paperspace implementation would be to use a ‘notebook’ instead of a ‘deployment’. This allows you control over all the files. I tried both the Windows installation instructions and the Linux ones for AUTOMATIC1111. The former didn’t won’t because of a permission error (I assume Paperspace doesn’t allow installation of third party programs for safety) and the latter didn’t work easily either.\nFortunately, I found a guide specifically for use on Paperspace notebooks. The installation this guide uses appears to be flexible. You can modify files so it should be possible to use Textual Inversion, Dreambooth, and Aesthetic Gradients. However, using this method could avoid spending money renting a GPU.\nOne such method of modifying the installation to include Dreambooth is detailed here: https://github.com/TheLastBen/fast-stable-diffusion.\n\n7.1.1 Paperspace conclusion\nTheir official implementation guide of AUTOMATIC1111’s stable diffusion costs money, cannot be edited, cannot use Aesthetic Gradients, cannot use Dreambooth. I believe it can use textual inversion.\nHowever, an unofficial guide appears to have the flexibility to use all three finetuning methods and can use free Paperspace GPUs so could have money.\nUnsurprisingly, I prefer the latter."
  },
  {
    "objectID": "posts/MP6.html#locally",
    "href": "posts/MP6.html#locally",
    "title": "Masters Project 1/11/2022",
    "section": "7.2 Locally",
    "text": "7.2 Locally\nMy laptop is designed to run graphically intensive videos games so it was worth testing if I could do stable diffusion on it.\nStable Diffusion is recommended only for use with GPUs that have =>10gb of memory. My graphics card, an NVIDIA RTX 3050 Ti, only has 4gb of dedicated GPU memory. My PC has 12gb (total) GPU memory.\nTotal GPU memory (in windows) = shared GPU memory + dedicated GPU memory.  Shared GPU memory is half of your computer’s RAM.  Thus, for me, GPU memory is 16/2 + 4 = 12 gb.\nUnfortunately, it appears that the memory from shared GPU memory is significantly slower than the dedicated memory. This results in a significant performance loss when the dedicated memory is all allocated and shared memory starts being used up.\nAs a result, it does not seem that I can run stable diffusion on my own machine. My dedicated memory of only 4gb (albeit also with plenty of RAM) is not enough pending some more thorough investigation of optimisation techniques.\nWhen I tried generating images and/or using Aesthetic Gradients, I got a memory error like: \n“RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.43 GiB already allocated; 0 bytes free; 3.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF”\nI tried a number of things to help this. I updated my drivers, moved my default browsing habits off Google Chrome, disabling hardware acceleration on my new browser, webui-user.bat to use –medvram, –lowvram, –always-batch-cond-uncond at command line.\nNone of these things allowed me to create an Aesthetic Gradient and generate an image with it."
  },
  {
    "objectID": "posts/MP6.html#deployment-conclusion",
    "href": "posts/MP6.html#deployment-conclusion",
    "title": "Masters Project 1/11/2022",
    "section": "7.3 Deployment Conclusion",
    "text": "7.3 Deployment Conclusion\nI cannot easily use my own machine to do Stable Diffusion with Aesthetic Gradients, even just for small testing. It might be possible with enough VRAM optimisation, but regardless with be very slow. I don’t believe it’s worth the time and or effort to figure it out.\nInstead, I’ll focus on using Paperspace or a service like it. My next step will be to properly implement AUTOMATIC1111’s unofficial paperspace guide which appears to create a implementation that has flexibility to use all three finetuning methods as well as free Paperspace GPUs."
  },
  {
    "objectID": "posts/MP6.html#soundscape_ir-module",
    "href": "posts/MP6.html#soundscape_ir-module",
    "title": "Masters Project 1/11/2022",
    "section": "10.1 soundscape_IR Module",
    "text": "10.1 soundscape_IR Module\nhttps://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13960\nsoundscape_IR is (the first) open-source Python toolbox designed for improving results in ML models by using unsupervised source separation (SS) in soundscape information retrieval. It contains a supervised algorithm too. To do so, it uses nonnegative matrix factorization.\nThis paper is useful because understanding soundscape separation techniques may lead to some insights into how to generate stable diffusion soundscape spectrograms. Papers like this indicate to me that I need to spend more time learning about computational linear algebra and the signal processing/the properties of sound. Thankfully, it being based in Python and open source means I can play around with it and learn these topics practically."
  },
  {
    "objectID": "posts/MP6.html#computational-bioacoustics-with-deep-learning-a-review-and-roadmap",
    "href": "posts/MP6.html#computational-bioacoustics-with-deep-learning-a-review-and-roadmap",
    "title": "Masters Project 1/11/2022",
    "section": "10.2 Computational bioacoustics with deep learning: a review and roadmap",
    "text": "10.2 Computational bioacoustics with deep learning: a review and roadmap\nhttps://peerj.com/articles/13152/\nThe paper is a recent (March 2022) overview about the growth of computational bioacoustics. This is exactly what I need to read: it covers how recent approaches are done bearing in mind all of the advances in various fields and methods (big data, signal processing, machine/deep learning, speech and image processing). As it’s a literature review, it will help a lot with writing my diss, as well as my motivation for using such a novel approach. Also, because deep learning is new to the field, it does well to explain it to people with a biology background, so is easy to understand.\nIt outlines the typical bioacoustic classification approach, which I had an implicit understanding of, but is nice to see outlined properly."
  },
  {
    "objectID": "posts/MP6.html#typical-approach",
    "href": "posts/MP6.html#typical-approach",
    "title": "Masters Project 1/11/2022",
    "section": "10.3 Typical Approach:",
    "text": "10.3 Typical Approach:\n\nUses a common CNN architecture like ResNet, VGGish, Inception and MobileNet. This could be a pretrained model from Google’s AudioSet, (a dataset of manually annotated audio events of humans and animal).\nUses spectrograms.\nThey are usually divided into fixed lengths ranging from 1-10 seconds.\nSpectrograms as three types: standard (linear-frequency), mel, or log-frequency. There are more details about transformations.\nThere is no strong consensus on the ‘best’ spectrogram format. Best being relative because it depends on the problem. Usually researchers decide empirically.\nThe ML tasks are usually binary classification (e.g. is this sound in the spectrogram? Yes/No) or multi label classification (are any of these sounds from a list in the spectrogram?).\nUses data augmentation to improve diversity of a small training dataset to be more diverse. Done using noise mixing, time shifting, mixup.\nOther than standard CNNs, a modification called CRNNs are relatively popular. You add a recurrent layer (LSTM or GRU) after the convolution layers.\nTrain the model using the standard good practices in deep learning. Model variables varied include Adam, dropout, early stopping, hyperparameters. Reference paper given (Goodfellow, Bengio & Courville, 2016).\nDataset is split as it usually is in machine learning: by training, validation and testing.\nPerformance metrics used are accuracy, precision, recall, F-score, and/or area under the curve (AUC or AUROC).\nBioacoustic datasets tend to be very unbalanced because some categories/classes have many more entries than another. This is why techniques such as macro-averaging are used. Macro-averaging calculates the model performance for each class and then takes the average of those to give equal weight to each class (Mesaros, Heittola & Virtanen, 2016).\nAs a field, computational bioacoustics with deep learning is immature. There are few reference works. There’s a lot of interesting work to be done to adapt the typical (and fast progressing!) deep learning techniques to the specific requirements of bioacoustic analysis.\n\nThe author then comments that the standard recipe works well for many bioacoustic classification tasks, including noisy outdoor sound scenes, but heavy rain and wind remains a problem across all analysis/ML methods, including DL. Also, that while data augmentation of spectrograms is specific to the audio domain and computational bioacoustics, the field’s use of CNNs is a good idea because it follows the advances, practices, and research in CNNs used for standard DL tasks (images, audio, video).\nThe typical approach outlined makes a lot of intuitive sense to me; most of the process I have encountered before. I find it interesting that the field mainly uses a image approach for audio classification, not a direct audio approach. I’m not suprised that the field also doesn’t have a strong consensus on the ‘best’ spectrogram format and individuals decide empirically. This is unfortunately often the case in ML, especially as the field progresses too fast. Finally, it’s exciting that the field is immature because there’s some interesting work to be done!"
  },
  {
    "objectID": "posts/MP6.html#dataset-synthesissimulation-sim2real",
    "href": "posts/MP6.html#dataset-synthesissimulation-sim2real",
    "title": "Masters Project 1/11/2022",
    "section": "10.4 Dataset synthesis/simulation (sim2real)",
    "text": "10.4 Dataset synthesis/simulation (sim2real)\nThere’s a section on this. It’s very useful to see other approaches than mine!  sim2real in DL is the creation of synthetic datasets. Data augmentation is editing existing training data, sim2real is creating new training data. Because of this, if there are problematic biases in the training data, sim2real could have the ability to significantly reduce them as creating new data entirely rather than changing existing data is more flexible.\nThe issue is whether the synthetic data is as realistic as the real data. The author states three papers using sim2real, that their “results imply that wider use in bioacoustic DL may be productive, even when simulation of the sound types in question is not perfect”.\nTwo of these papers are very relevant to my end goal:\n“Simulation is also especially relevant for spatial sound scenes, since the spatial details of natural sound scenes are hard to annotate (Gao et al., 2020; Simon et al., 2021).\nSimulation, often involving composing soundscapes from a library of sound clips, has been found useful in urban and domestic sound analysis (Salamon et al., 2017b; Turpault et al., 2021). Such results imply that wider use in bioacoustic DL may be productive, even when simulation of the sound types in question is not perfect.”"
  },
  {
    "objectID": "posts/MP6.html#conclusion-for-now",
    "href": "posts/MP6.html#conclusion-for-now",
    "title": "Masters Project 1/11/2022",
    "section": "10.5 Conclusion (for now)",
    "text": "10.5 Conclusion (for now)\nThis paper is extremely long as expected of a thorough lit review. It’s most definitely worth reading in depth.\nIt’s conclusion is that developments in DL, data availability, audio hardware, processing power, and demands of national and international biodiversity accounting will benefit Bioacoustics. However, the field must align these developments (especially in DL) towards their specific problems and needs. An example earlier being that CNN research has advanced in DL, but that data augmentation of spectrograms is specific to the audio domain and computational bioacoustics, so the latter must be investigated.\nReading this paper A: gives me an overview of the field. B: gives me ideas and knowledge on how to do my approach. C: gives me flexibility to find another approach in case my own doesn’t work out.\nThere’s many sections of this section indirectly important for me to properly understand. Some of it covers knowledge I’m missing, such as, “Acoustic features: spectrograms, waveforms, and more”, but others cover content I’m familiar with such as data augmentation and pretraining, just not in the context of bioacoustics.\nLastly, it also contains a section on Spatial acoustics, relevant to our previous approach using stereo data. This fits in case C above. Similarly, here is another paper about using an acoustic vector sensor (similar to stereo data) approach for sound separation. https://asa.scitation.org/doi/full/10.1121/10.0013505"
  },
  {
    "objectID": "posts/MP6.html#papers-to-motivate-bioacoustic-classification",
    "href": "posts/MP6.html#papers-to-motivate-bioacoustic-classification",
    "title": "Masters Project 1/11/2022",
    "section": "10.6 Papers to motivate bioacoustic classification:",
    "text": "10.6 Papers to motivate bioacoustic classification:\nThese two papers would be useful to motivate my approach. Better classification models means better bird density and migration information, means better biodiversity tracking.\nOn monitoring of flight calls based on artificial light at night (bird migration): https://onlinelibrary.wiley.com/doi/epdf/10.1111/ibi.12955#\nOn estimating the number of birds from audio data, a literature review: https://onlinelibrary.wiley.com/doi/epdf/10.1111/ibi.12944"
  },
  {
    "objectID": "posts/MP7.html",
    "href": "posts/MP7.html",
    "title": "Masters Project 8/11/2022",
    "section": "",
    "text": "The seventh post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/MP7.html#during-the-meeting-we-discussed",
    "href": "posts/MP7.html#during-the-meeting-we-discussed",
    "title": "Masters Project 8/11/2022",
    "section": "2.1 During the meeting we discussed:",
    "text": "2.1 During the meeting we discussed:\n\nIf you have two frequencies with a small difference, say 220hz and 224hz, you can hear a beat. Stuart played around with an app where you can enter upto three frequencies and hear their superposition.\nWhere is the ‘noise’ coming from in my simple spectrogram? There are only two sine waves in seconds 1-3 with literally nothing else. Why are the lines blurry? Perhaps trying librosa or matlabplot to generate the same spectrogram might give a different result.\nWhat (max) sampling rate do we need in order to differentiate/resolve two similar frequencies superimposed together in a spectrogram? How does sampling rate affect resolving? Try low and high sampling rates.\nWe will try different data synthesis approaches, for example, in the mono case, two superimposed waves with a range of frequencies and lengths.\nInformation like the frequencies of the signals, their lengths, their amplitudes. For stereo data, the phase (or angle) of the signal, the distance from the sound source to the microphones.\nA machine learning model that could look at these different synthesised audio spectrograms and extract information.\nI need to doublecheck the literature review paper to see which spectrogram parameters are well known or set, and which ones are up for variation. The former I could leave (mostly) alone, and the latter like window size I should test with.\nIn stereo data, often one channel has a higher amplitude than another. This difference leads to important information, but for a machine learning model, it could lead to the model assigning more importance to the larger amplitude channel for the prediction, so perhaps there is a way to equalise the importance of each channel if the model struggles to do it itself through training. For example, making both channels’ data have equal importance to the prediction.\nBinaural Audio is audio that is very natural to humans to listen to using headphones. For example, hearing that you are in NYC with BA makes you feel immersed like you are there. This is so much more immersive than mono audio.\nHumans can use audio to hear and feel the difference with BA, but when myself and Stuart looked at a BA spectrogram for the left and right channel, we couldn’t easily visually deduce the difference. Perhaps this is because information is lost in a spectrogram. Perhaps this is because humans simply have better biological capacity for audio than visuals. Perhaps its because we simply have more training on audio than specifically looking at spectrograms. Or even it could be because the spectrogram information is not plotted in a useful way to extract information. Maybe we could better deduce difference in a difference between channels plot.\nWhere is information lost in a spectrogram? It’s lost from the modulus squared destroying the phase information. A mono spectrogram loses information because it loses this phase info. Fundamentally, a mono spectrogram destroys information because there area an infinite number of audio signals that can construct the same spectrogram, so we cannot inverse it to reconstruct the original audio. Using stereo instead, could lead to spectrograms with less information lost like phase and less possibilities of audio signals leading to the same spectrogram.\nInstead of trying to deduce phase, it could try and deduce the angle at which a signal is heard from relative to the microphone. This could be more intuitive and useful for us to understand. Say a model states it thinks the audio came from 30 +- 10 degrees.\nFor distance as an example, if a signal is 3 lambda away, or if it is 5 lambda away, it has the same phase. The info about the number of phases travelled could be lost.\nThe aim this week is to play around with data synthesis, sampling rate, and whether we can resolve signals.\nResolving is, imagine two waveforms superimposed together. How well can we draw the separate curves around these two waveforms.\nTo resolve, we’ll first use a simple approach. For these two waveforms, we use curve fit in Python to return two waveforms with amplitude A1 and A2. If fully resolved, and these waveforms have equal amplitude and are directly ontop of eachother, A1 should be the same as A2. If being resolved is worst, then A1 would equal the amplitude of the superimposed wave and A2 would be 0.\nFor example, trying two similar frequency signals, 600Hz and 650Hz, then seeing if we can resolve them. More complex, setting a signal fixed at 500Hz, then varying another signal to start at 0Hz and vary in steps to 1000Hz over time, to see how resolve ability changes as frequencies get closer. Could generate an animation for the latter.\nThe main aim is to figure out which spectrogram transform parameters result in the best spectrogram training data for a stable diffusion model. The spectrogram transform(s) that produce spectrograms that are best for a separate model to deduce information will probably be the best for a stable diffusion model to generate spectrograms with.\nIn order to do this, we need to know more about what information can be deduced from a spectrogram and why. So we start with a simple case, two sine waves with a big frequency difference, then we add more complexity, making it more like an (artificial) soundscape.\nStereo data might be essential to (stable diffusion) soundscape generation. Imagine creating a soundscape, it would be better if we could specify in the prompt directionality of which a bird song is coming from. For example “X bird singing dawnsong at 30 degrees”.\nHowever, the easier and more obvious approach is just to use different spectrogram transforms to generate spectrogram training data then just use that directly to train different stable diffusion models and see which one is the best at generating spectrograms."
  },
  {
    "objectID": "posts/MP7.html#test-data",
    "href": "posts/MP7.html#test-data",
    "title": "Masters Project 8/11/2022",
    "section": "4.1 Test Data",
    "text": "4.1 Test Data\nI generated data of an analog phone dialing numbers. This is because it’s simple data to play with: pressing a digit on an analog phone generated two sine waves simultaneously.\nFor example, when ‘1’ is pressed, a sine wave at 697hz is created along with one at 1209hz. We will observe the superposition of these two waves in our data.\n697hz means that the sine wave completes it’s full cycle 697 times within one second. This means if we want to see one cycle, we have to zoom into a 1 second interval 697x!\nOur generated audio will be 7 seconds in total. The first 3 being the sound made when ‘1’ is pressed, the next 1 second being silence, and the last 3 seconds being ‘2’ pressed.\nThat means first there will be 3 seconds of sine waves at 697hz and 1209hz. Then 1 seconds of nothing. Then 3 seconds of sine waves at 697hz and 1336hz.\nWe will assume a sampling rate of 4000. This means that every second, we take 4000 samples of the audio. In our total time of 7 seconds, there are then be 4000 * 7 = 28000 samples."
  },
  {
    "objectID": "posts/MP7.html#data-generation-code",
    "href": "posts/MP7.html#data-generation-code",
    "title": "Masters Project 8/11/2022",
    "section": "4.2 Data Generation Code",
    "text": "4.2 Data Generation Code\nImporting required modules\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n\nFirst, we define get_signal_Hz. This function outputs a sine wave of a requested length and frequency.\nTo do so, it takes a frequency (Hz), the sampling rate (sample_rate), and length (length_ts_sec) as input. It returns a list representing the requested sample.\n\ndef get_signal_Hz(Hz, sample_rate, length_ts_sec):\n    # Create a 1 second long list of length sample_rate containing the radian values of sampled points\n    # on our sine wave\n    ts1sec = list(np.linspace(0,np.pi*2*Hz,sample_rate))\n    ## Multiply this list by length_ts_sec to get it to be the requested length\n    ts = ts1sec*length_ts_sec\n    # Return a list representing a sine wave of requested frequency and duration\n    return(list(np.sin(ts)))\n\nNow we can use get_signal_Hz to create our sound arrays representing the different analog numbers being dialed.\nFor 3 seconds of dialing ‘1’:  Two sine waves are created: one at 697Hz and one at 1209Hz.  So we need to use  get_signal_Hz(697, 4000, 3) and  get_signal_Hz(1209, 4000, 3)\n\nsample_rate   = 4000\nlength_ts_sec = 3\n\nts1  = np.array(get_signal_Hz(697, sample_rate,length_ts_sec)) \nts1 += np.array(get_signal_Hz(1209,sample_rate,length_ts_sec))\nts1  = list(ts1)\n\nNow we have 1 second of silence.  We just want an array of 4000 entries (our sampling rate) of [0]:  [0] * 4000\n\nts_silence = [0]*sample_rate*1\n\nLastly,\nFor 3 seconds of dialling ‘2’:  Two sine waves are created: one at 697Hz and one at 1336Hz.  So we need to use  get_signal_Hz(697, 4000, 3) and  get_signal_Hz(1336, 4000, 3)\n\nsample_rate   = 4000\nlength_ts_sec = 3\n\nts2  = np.array(get_signal_Hz(697, sample_rate,length_ts_sec)) \nts2 += np.array(get_signal_Hz(1336,sample_rate,length_ts_sec))\nts2  = list(ts2)\n\nNow finally we add them together. This is why we converted the arrays into lists, so that we can combine them:\n\nts = ts1 + ts_silence + ts2\n\nWe can use Audio from IPython.display to listen to time series audio files.\n\nfrom IPython.display import Audio\nAudio(ts, rate=sample_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/MP7.html#time-domain-plot",
    "href": "posts/MP7.html#time-domain-plot",
    "title": "Masters Project 8/11/2022",
    "section": "4.3 Time Domain Plot",
    "text": "4.3 Time Domain Plot\nNow that we have generated our test data, let us plot it in the time domain. This will show us how the amplitude of the signal changes over time.\n\n# The total length is the total number of samples, len(ts), divided by the sampling rate, sample_rate\ntotal_ts_sec = len(ts)/sample_rate\nprint(\"The total time series length = {} sec (N points = {}) \".format(total_ts_sec, len(ts)))\n\n# Plotting figure\nplt.figure(figsize=(20,3))\n# This plots all the 28000 values in ts, the y value is the value of an element in ts,\n# the x value corresponds to the index of the element\nplt.plot(ts)\n\n# For the y axis, the amplitude ticks are already correct by default\n\n# For the x axis, we need to convert the element index order (the sample number) into seconds.\n# Without this, it's 0-28000, with it, its 0 to 7\nplt.xticks(np.arange(0,len(ts),sample_rate),\n           np.arange(0,len(ts)/sample_rate,1))\n# plt.xticks takes two parameters, the ticks in the original data, \n# and the ticks in the converted data. \n# The original data is between 0-28000.\n# The converted data is between 0 and 28000/4000, 0 and 7 seconds. \n\n# Labelling axes and title:\nplt.ylabel(\"Amplitude\")\nplt.xlabel(\"Time (second)\")\nplt.title(\"The total length of time series = {} sec, sample_rate = {}\".format(len(ts)/sample_rate, sample_rate))\nplt.show()\n\nThe total time series length = 7.0 sec (N points = 28000) \n\n\n\n\n\nAs we can see, the first 0-3 seconds are a superposition of 697Hz and 1209Hz sign waves, 3-4 seconds are silent, and 4-7 are a superposition of 697Hz and 1336Hz sign waves. Seconds 4-7 appear more frequent than seconds 0-3, which makes sense considering 1336Hz > 1209 Hz.\nHowever, looking at the signal like this isn’t particularly useful. If we didn’t know which digits were dialed to create this signal, from this representation we couldn’t easily figure it out, because it doesn’t easily yield frequency information. For this reason, we instead plot sound signals in the frequency domain."
  },
  {
    "objectID": "posts/MP7.html#frequency-domain-plot",
    "href": "posts/MP7.html#frequency-domain-plot",
    "title": "Masters Project 8/11/2022",
    "section": "4.4 Frequency Domain Plot",
    "text": "4.4 Frequency Domain Plot\nWe will use a Discrete Fourier Transform for this task. \nOur fourier coefficient calculator function, get_xns, only calculates the Fourier coefficients up to the Nyquest limit.  This video at https://www.youtube.com/watch?t=501&v=mkGsMWi_j4Q&feature=youtu.be provides more information about the Nyquest limit. \nThe absolute value of each Fourier coefficient is also doubled to account for the symmetry of the Fourier coefficients around the Nyquest limit. \nThe Nyquest Limit = sampling rate / 2. It is impossible to calculate frequencies of signals above this value. This tells us that we if want to fully calculate the frequencies of waves that form an audio file, we need a sampling rate double the highest frequency wave in said file.\nFirst we need to code DFT:\n\n# This function takes an entry from a time series, Xs, along with the length of that time series,n,\n# and calculates the fourier coefficients for it.\ndef get_xn(Xs,n):\n    '''\n    calculate the Fourier coefficient X_n of \n    Discrete Fourier Transform (DFT)\n    '''\n    L  = len(Xs)\n    ks = np.arange(0,L,1)\n    xn = np.sum(Xs*np.exp((1j*2*np.pi*ks*n)/L))/L\n    return(xn)\n\n# This function takes a time series and returns all the fourier coefficients for its entries\n# upto the Nyquest limit.\ndef get_xns(ts):\n    '''\n    Compute Fourier coefficients only up to the Nyquest Limit Xn, n=1,...,L/2\n    and multiply the absolute value of the Fourier coefficients by 2, \n    to account for the symetry of the Fourier coefficients above the Nyquest Limit. \n    '''\n    mag = []\n    L = len(ts)\n    for n in range(int(L/2)): # Nyquest Limit\n        mag.append(np.abs(get_xn(ts,n))*2)\n    return(mag)\nmag = get_xns(ts)\n\nNow that we have the Fourier coefficients for our time series array, we can plot them to represent our audio using them:\n\n# Plotting figure\nplt.figure(figsize=(20,3))\nplt.plot(mag)\n\n# For the y axis, the Fourier Cofficient ticks are already correct by default\n# For the x axis, frequency in Hertz is NOT plotted. It's Fourier coefficient's frequency, k.\n\nplt.xlabel(\"Frequency (k)\")\nplt.title(\"Two-sided frequency plot\")\nplt.ylabel(\"|Fourier Coefficient|\")\nplt.show()\n\n\n\n\nThe y axis is fine, but the x axis is not frequency in Hertz. We need to fix this in order to tell which waves are present in the audio.\nWe can convert a (by default) Fourier coefficient’s frequency, k, into frequency in Hertz via\n\n\n\nimage.png\n\n\nIn our case, Sample Rate is 4000 and Total N of Sample Points is 28000.\n\ndef get_Hz_scale_vec(ks,sample_rate,Npoints):\n    freq_Hz = ks*sample_rate/Npoints\n    freq_Hz  = [int(i) for i in freq_Hz ] \n    return(freq_Hz)\n\n\n# Nxlim is the number of intervals to label along the xaxis\nNxlim = 30\n\n# Creating ks and ksHz for xticks.\nks   = np.linspace(0,len(mag),Nxlim)\nksHz = get_Hz_scale_vec(ks,sample_rate,len(ts))\n\n# Plotting\nplt.figure(figsize=(20,3))\nplt.plot(mag)\n# Converting the x axis with xticks\nplt.xticks(ks,ksHz)\n\n# Labelling\nplt.title(\"Frequency Domain\")\nplt.xlabel(\"Frequency (Hz)\")\nplt.ylabel(\"|Fourier Coefficient|\")\nplt.show()\n\n\n\n\nFrom this, we can deduce a few things about our audio file.  Firstly, there’s a peak around 689Hz that’s double in Fourier Coefficient value than the other signals. This must be our 697Hz sine waves, and is double the others because there are two of them.  Secondly, we see our 1209Hz and 1336Hz peaks, one peak for each sine wave.\nThe frequency domain plot is great because it shows us the signal’s underlying frequencies in Hz which we can interpret as we did. However, it is limited because it does not tell us anything about time (temporal information). E.g., when did the 1209Hz sine wave occur during the 7 seconds?\nA frequency domain plot is created from the frequency dependent Fourier Coefficients. In order to display temporal information however, we’d need a plot created from the time dependent Fourier Coefficients.\nA spectrogram is one such way plot."
  },
  {
    "objectID": "posts/MP7.html#spectrogram-code",
    "href": "posts/MP7.html#spectrogram-code",
    "title": "Masters Project 8/11/2022",
    "section": "5.1 Spectrogram Code",
    "text": "5.1 Spectrogram Code\nFirst there is a cell of code to create a spectrogram, then there is a cell of the code to plot it.\n\ndef create_spectrogram(ts,NFFT,noverlap = None):\n    '''\n          ts: original time series\n        NFFT: The number of data points used in each block for the DFT.\n          Fs: the number of points sampled per second, so called sample_rate\n    noverlap: The number of points of overlap between blocks. The default value is 128. \n    '''\n    # By default, we set no overlap between the blocks.\n    if noverlap is None:\n        # For no overlap, we use half the number of data points per DFT block\n        noverlap = NFFT/2\n    noverlap = int(noverlap)\n    \n    # Randomly pick numbers between 0 and the length of the time series\n    # NFFT-noverlap times\n    starts  = np.arange(0,len(ts),NFFT-noverlap,dtype=int)\n    \n    # starts is an array, so we can do elementwise comparison\n    # to remove windows that less than NFFT sample size\n    starts  = starts[starts + NFFT < len(ts)]\n    \n    # A list of all the Fourier Coeffients in the time series for all windows\n    xns = []\n    for start in starts: # for each window\n        # for each window, we apply a short term discrete fourier transform\n        # to do so, we get the Fourier Coeffients of said window\n        ts_window = get_xns(ts[start:start + NFFT]) \n        # append the window's Fourier Coeffients\n        xns.append(ts_window)\n    # specX is a transpose of xns\n    specX = np.array(xns).T\n    # as specX is an array, we can rescale it's (absolute) values easily\n    spec = 10*np.log10(specX)\n    \n    # assert checks a condition\n    # if it is True, then nothing happens, but if it's false, AssertionError occurs\n    # spec.shape[1] should equal the length of starts, so we just check it\n    assert spec.shape[1] == len(starts) \n    \n    # return starts, containing window info,\n    # and spec, the Fourier Coefficients\n    return(starts,spec)\n\nL = 256\nnoverlap = 84\nstarts, spec = create_spectrogram(ts,L,noverlap = noverlap)\n\n\ndef plot_spectrogram(spec,ks,sample_rate, L, starts, mappable = None):\n    # Create plot\n    plt.figure(figsize=(20,8))\n    # Plot the Fourier Coefficents\n    plt_spec = plt.imshow(spec,origin='lower')\n\n    # Creating the ticks for y\n    # The number of intervals to plot on the axis\n    Nyticks = 10\n    # ks and ksHz for yticks\n    ks      = np.linspace(0,spec.shape[0],Nyticks)\n    ksHz    = get_Hz_scale_vec(ks,sample_rate,len(ts))\n    # plotting\n    plt.yticks(ks,ksHz)\n    plt.ylabel(\"Frequency 10^3 (Hz)\")\n\n    ## Creating the ticks for x, involving manipulating the (non-Hertz frequency dependent) Fourier Coefficients.\n    # The number of intervals to plot on the axis\n    Nxticks = 10\n    # ts_spec and ts_spec_sec for xticks\n    ts_spec = np.linspace(0,spec.shape[1],Nxticks)\n    ts_spec_sec  = [\"{:4.2f}\".format(i) for i in np.linspace(0,total_ts_sec*starts[-1]/len(ts),Nxticks)]\n    # plotting\n    plt.xticks(ts_spec,ts_spec_sec)\n    plt.xlabel(\"Time (sec)\")\n    \n    \n    # Plotting labels/colours\n    plt.title(\"Spectrogram L={} Spectrogram.shape={}\".format(L,spec.shape))\n    plt.colorbar(mappable,use_gridspec=True)\n    plt.show()\n    return(plt_spec)\nplot_spectrogram(spec,ks,sample_rate,L,starts)\n\n\n\n\n<matplotlib.image.AxesImage>\n\n\nLooking at this, we can clearly see that the first 3 seconds had our 693Hz and 1209Hz sine waves, then we had our second of silence, and lastly our 3 seconds of 693Hz and 1336Hz sine waves!"
  },
  {
    "objectID": "posts/MP7.html#spectrograms-and-timefrequency-uncertainty",
    "href": "posts/MP7.html#spectrograms-and-timefrequency-uncertainty",
    "title": "Masters Project 8/11/2022",
    "section": "5.2 Spectrograms and Time/Frequency Uncertainty",
    "text": "5.2 Spectrograms and Time/Frequency Uncertainty\nLastly we can modify the spectrogram to see how the relationship between time and frequency resolution change. They are an uncertainty principle. If we have high frequency resolution, our time resolution is low and vise versa.\nThis manifests in spectrograms as blurry horizontal lines for frequency uncertainty. For time uncertainty, it manifests as blurry vertical lines, but it’s hard to see here because our signal is so simple.\nThe following three plots show that as frequency resolution gets better (decreases), time resolution gets worse (increases). We can (should) see the horizontal line get narrower while the vertical line gets more blurry.\n\nplt_spec1 = None \nfor iL, (L, bandnm) in enumerate(zip([150, 200, 400],[\"wideband\",\"middleband\",\"narrowband\"])):\n    print(\"{:20} time resolusion={:4.2f}sec, frequency resoulsion={:4.2f}Hz\".format(bandnm,L/sample_rate,sample_rate/L))\n    starts, spec = create_spectrogram(ts,L,noverlap = 1 )\n    plt_spec = plot_spectrogram(spec,ks,sample_rate, L, starts, \n                                 mappable = plt_spec1)\n    if iL == 0:\n        plt_spec1 = plt_spec\n\nwideband             time resoulsion=0.04sec, frequency resoulsion=26.67Hz\n\n\n\n\n\nmiddleband           time resoulsion=0.05sec, frequency resoulsion=20.00Hz\n\n\nC:\\Users\\Adnan\\AppData\\Local\\Temp\\ipykernel_33676\\363702639.py:30: MatplotlibDeprecationWarning: Starting from Matplotlib 3.6, colorbar() will steal space from the mappable's axes, rather than from the current axes, to place the colorbar.  To silence this warning, explicitly pass the 'ax' argument to colorbar().\n  plt.colorbar(mappable,use_gridspec=True)\n\n\n\n\n\nnarrowband           time resoulsion=0.10sec, frequency resoulsion=10.00Hz"
  },
  {
    "objectID": "posts/MP7.html#visualisation",
    "href": "posts/MP7.html#visualisation",
    "title": "Masters Project 8/11/2022",
    "section": "6.1 Visualisation",
    "text": "6.1 Visualisation\nThe following is ABERNETHY_20170521_021400.wav visualised in the time domain.\n\n\n\nVisualisation"
  },
  {
    "objectID": "posts/MP7.html#spectrogram-conversion-stft",
    "href": "posts/MP7.html#spectrogram-conversion-stft",
    "title": "Masters Project 8/11/2022",
    "section": "6.2 Spectrogram Conversion (STFT)",
    "text": "6.2 Spectrogram Conversion (STFT)\nThis is really simply done.  It’s just X = librosa.stft(x) for the STFT and then  Xdb = librosa.amplitude_to_db(abs(X)) to scale the amplitude to DB."
  },
  {
    "objectID": "posts/MP7.html#spectrogram",
    "href": "posts/MP7.html#spectrogram",
    "title": "Masters Project 8/11/2022",
    "section": "6.3 Spectrogram",
    "text": "6.3 Spectrogram\n\n\n\nSpectrogram"
  },
  {
    "objectID": "posts/MP8.html",
    "href": "posts/MP8.html",
    "title": "Masters Project 15/11/2022",
    "section": "",
    "text": "The eighth post from a series of posts about my Masters project with the Physics Department at Durham University."
  },
  {
    "objectID": "posts/MP8.html#during-the-meeting-we-discussed",
    "href": "posts/MP8.html#during-the-meeting-we-discussed",
    "title": "Masters Project 15/11/2022",
    "section": "3.1 During the meeting we discussed:",
    "text": "3.1 During the meeting we discussed:\n\nI used square windows implicitly in my spectrogram creation. I used blocks of 256, meaning splitting the audio array X(t) into windows of 256 values.\nChanging window lengths affects the time resolution and frequency resolution tradeoff.\nInstead of taking square windows, we could do others. For example, a Gaussian window. The latter is known as the Gabor transform, and I recall a paper in the project’s reference about it. A another transform to look into is the wavelit transform.\nI should compare my spectrogram creation with the predefined ones. I.E, for the 7 second case I did last week, or for even simpler cases.\nOn why there might be blurryness in my 7 second spectrogram. Imagine a sine wave with finite length. In the Fourier equation maths, the omega components of it will have a create peak at the sine wave’s frequency, but also at other places. This might be a contributing factor, but looking at other predefined spectrogram functions might yield some clues to do with my implementation.\nAnother reason might be because of the existence of two waves superimposing. Moire’s patterns are an example of this. Maybe check if there is blurryness in a one sine wave spectrogram.\nI could do a single sine wave Fourier transform and Spectrogram analytically, and compare it to my version to check for correctness.\nFor the fft, don’t code it yourself. Use Scipy’s implementation. Code the spectrogram yourself but not the fft.\nWe discussed the standard approach of audio to image for CNN use.\nWhy use an image in the firstplace, as in maybe the classification can be done without an CNN using the Fourier coefficients instead.\nVery interestingly, why are we using spectrograms to represent the audio? Spectrograms, because of their mod squared, lose the phase information. This is necessary for humans to visually understand the audio image, but not for computers. We could use something other than a standard spectrogram to retain the phase information. This would theoretically make reconstructing the images possible, and can let us tell directionality for spectrogram creation.\nOne thing we could try is, instead of giving a CNN the one standard spectrogram, we give two spectrograms. One is the normal spectrogram, representing Amplitude and Time. The other is representing phase and time. - The problem with representing phase visually is that a colour scheme from 0-2pi will be continuous for that interval, but after we go over 2pi, for example, as 2.1pi = .1pi, there is discontinuity in the colour scheme. To remedy this, we could supply three spectrograms. Two for the phase, to allow for a continuous phase colouring scheme.\nThe main motivation behind the stable diffusion approach. If I used the ‘standard’ method of finding audio files of different things online, say of a bird’s song and a forest environment, then combined them together, humans will be able to tell their combination is artificial. There’s a lot of changes you need to make, some very subtle, to make the combination match. These are very important to create good training data for ML models. In comparison, if I gave SD images of chairs and bears, it can create a very convincing image of a bear sitting on a chair. It can figure out these subtleties and create realistic images. SD could create realistic and labeled soundscapes in a way other methods cannot. Also can do directionality and the basic approach cannot.\nI am interested in audio directionality because I want SD prompts to be able to incorporate them. Say “this bird singing at 30 degrees, another bird singing at 80 degrees”. In general, the more variation we have in training data, the better an ML model will be. Directionality is important for this. Consequently, using spectrogram or spectrogram-like images that store directionality is needed."
  },
  {
    "objectID": "posts/MP8.html#sinewavecreation-function",
    "href": "posts/MP8.html#sinewavecreation-function",
    "title": "Masters Project 15/11/2022",
    "section": "4.1 SineWaveCreation Function",
    "text": "4.1 SineWaveCreation Function\nThe first thing I did was work on a function that can create a sine wave for a given frequency, sampling rate, and length:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n\ndef SineWaveCreation(Frequency,Sampling_Rate,TS_Len):\n    # Creating a timeseries that contains the sample values \n    # that map onto a sine wave\n    # This timeseries is one second long\n    TS_OneSec = list(np.linspace(0,np.pi*2*Frequency,Sampling_Rate))\n    # Multiplying the timeseries list to make it the desired length\n    TS_Len = TS_OneSec*TS_Len\n    # Transforming the timeseries with a sign function\n    # and returning it \n    return(np.sin(TS_Len))"
  },
  {
    "objectID": "posts/MP8.html#phase-addition",
    "href": "posts/MP8.html#phase-addition",
    "title": "Masters Project 15/11/2022",
    "section": "4.2 Phase Addition",
    "text": "4.2 Phase Addition\nModifying SineWaveCreation to allow for to plot with starting at a specified Phase.\n\ndef SineWaveCreation(Frequency,Sampling_Rate,TS_Len,Phase=0):\n    # Phase, in degrees, is the phase difference between the desired wave and \n    # a sine wave starting at the origin\n    # 360 deg is 2pi radians\n    # Radian = Degree * pi/180\n    Phase = Phase * np.pi / 180\n    TS_OneSec = list(np.linspace(0,np.pi*2*Frequency,Sampling_Rate))\n    TS_Len = TS_OneSec*TS_Len\n    TS_Len = np.array(TS_Len)\n    return(np.sin(TS_Len+Phase))\n\nTesting Creation with Phase Differences:\n\nWave = SineWaveCreationPhase(1,Sampling_Rate,TS_Len,0)\nWavePlotter(Wave,TS_Len,Sampling_Rate,[1])\n\nWave = SineWaveCreationPhase(1,Sampling_Rate,TS_Len,-180)\nWavePlotter(Wave,TS_Len,Sampling_Rate,[1],-180)\n\n\n\n\n\n\n\nTesting the lengths of the wave arrays:\n\nSampling_Rate = 4000\nTS_Len = 5\n\n\nFirstWave = SineWaveCreation(1,Sampling_Rate,TS_Len)\nSecondWave = SineWaveCreation(2,Sampling_Rate,TS_Len)\nlen(FirstWave),len(SecondWave)\n\n(20000, 20000)\n\n\nBoth wave arrays are 20000 long as expected. There are 5 seconds of 4000 samples, 5 * 4000 = 20000\nSuperimposing them is simply addition:\n\nCombinedWave = FirstWave + SecondWave"
  },
  {
    "objectID": "posts/MP8.html#sr4000",
    "href": "posts/MP8.html#sr4000",
    "title": "Masters Project 15/11/2022",
    "section": "7.1 Sr=4000",
    "text": "7.1 Sr=4000\nIf the sampling rate is high, say 4000, both looks like blocks.\n\nSR = 4000\nWaveInfoList = [[600,5,SR],[650,5,SR]]\n(Wave1,Wave2) = WaveCreationPlotter(WaveInfoList,False)\nCombinedWave = WaveCreationPlotter(WaveInfoList,True)"
  },
  {
    "objectID": "posts/MP8.html#sr1300",
    "href": "posts/MP8.html#sr1300",
    "title": "Masters Project 15/11/2022",
    "section": "7.2 Sr=1300",
    "text": "7.2 Sr=1300\nIf it is set to twice the maximum frequency, 650 * 2 = 1300:\n\nSR = 1300\nWaveInfoList = [[600,5,SR],[650,5,SR]]\n(Wave1,Wave2) = WaveCreationPlotter(WaveInfoList,False)\nCombinedWave = WaveCreationPlotter(WaveInfoList,True)"
  },
  {
    "objectID": "posts/MP8.html#sr600",
    "href": "posts/MP8.html#sr600",
    "title": "Masters Project 15/11/2022",
    "section": "7.3 Sr=600",
    "text": "7.3 Sr=600\nIf it’s set to the value of the first wave:\n\nSR = 600\nWaveInfoList = [[600,5,SR],[650,5,SR]]\n(Wave1,Wave2) = WaveCreationPlotter(WaveInfoList,False)\nCombinedWave = WaveCreationPlotter(WaveInfoList,True)"
  },
  {
    "objectID": "posts/MP8.html#sr650",
    "href": "posts/MP8.html#sr650",
    "title": "Masters Project 15/11/2022",
    "section": "7.4 Sr=650",
    "text": "7.4 Sr=650\nIf it’s set to the value of the second wave:\n\nSR = 650\nWaveInfoList = [[600,5,SR],[650,5,SR]]\n(Wave1,Wave2) = WaveCreationPlotter(WaveInfoList,False)\nCombinedWave = WaveCreationPlotter(WaveInfoList,True)"
  },
  {
    "objectID": "posts/MP8.html#sr1",
    "href": "posts/MP8.html#sr1",
    "title": "Masters Project 15/11/2022",
    "section": "7.5 Sr=1",
    "text": "7.5 Sr=1\nIf it’s set to 1\n\nSR = 1\nWaveInfoList = [[600,5,SR],[650,5,SR]]\n(Wave1,Wave2) = WaveCreationPlotter(WaveInfoList,False)\nCombinedWave = WaveCreationPlotter(WaveInfoList,True)"
  },
  {
    "objectID": "posts/MP8.html#sr100",
    "href": "posts/MP8.html#sr100",
    "title": "Masters Project 15/11/2022",
    "section": "7.6 Sr=100",
    "text": "7.6 Sr=100\nIf it’s set to 100\n\nSR = 100\nWaveInfoList = [[600,5,SR],[650,5,SR]]\n(Wave1,Wave2) = WaveCreationPlotter(WaveInfoList,False)\nCombinedWave = WaveCreationPlotter(WaveInfoList,True)"
  },
  {
    "objectID": "posts/MP8.html#test-value",
    "href": "posts/MP8.html#test-value",
    "title": "Masters Project 15/11/2022",
    "section": "7.7 Test Value",
    "text": "7.7 Test Value\nModify SR to different values for testing.\n\nSR = 532\nWaveInfoList = [[600,5,SR],[650,5,SR]]\n(Wave1,Wave2) = WaveCreationPlotter(WaveInfoList,False)\nCombinedWave = WaveCreationPlotter(WaveInfoList,True)"
  },
  {
    "objectID": "posts/MP8.html#simple-animation",
    "href": "posts/MP8.html#simple-animation",
    "title": "Masters Project 15/11/2022",
    "section": "9.1 Simple Animation",
    "text": "9.1 Simple Animation\nAs a simple test, I plotted a red sine wave, then a green one, and then their superposition as a blue wave.\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\nWaveAnimationDict = {}\n\ndef one_frame(d,WaveInfoList):\n    global WaveAnimationDict\n    #print(WaveInfoList)\n    # d is the frame number, passed automatically\n    # The first time running, d=0\n    \n    # one_frame iterates through frames with d\n    # For d=0, I want the 1Hz wave to be plotted\n    # For d=1, I want the 2Hz wave to be plotted\n    # For d=2, I want the superposition to be plotted\n    \n    #print(d)\n    # clear the axis we created before running this function\n    ax.clear() \n    \n    # Turn WaveInfo into WaveArrays\n    if d==0:\n        WaveInfoList = np.array([WaveInfoList[0]])\n        WaveList = WaveCreationPlotter(WaveInfoList,False,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='red')\n        \n    elif d==1:\n        WaveInfoList = np.array([WaveInfoList[1]])\n        WaveList = WaveCreationPlotter(WaveInfoList,False,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='green')\n        \n    elif d==2:\n        WaveList = WaveCreationPlotter(WaveInfoList,True,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='blue')\n    \n\n    WaveAnimationDict[str(d)] = Wave\n    \n    # create dictionary. it stores frame number and wave.\n    # we can do different keys for wave1, wave2, and wave3\n     \n    # Plotting each frame\n    ax.set_xticks(np.arange(0,len(Wave),Sampling_Rate),\n               np.arange(0,len(Wave)/Sampling_Rate,1))\n    ax.set_ylim((-2,2))\n    ax.set_ylabel(\"Amplitude\")\n    ax.set_xlabel(\"Time (Seconds)\")\n    ax.set_title(\"Simple Superposition Animation\")\n\n\nWaveInfoList = [[1,5,4000],[2,5,4000]]\nfig,ax = plt.subplots()\nanimation = FuncAnimation(fig, one_frame, frames=3, fargs = (WaveInfoList, ), interval=500, repeat=True)\n# frame is the number of times to run one_frame\n# interval is the number of miliseconds between frames\nplt.close()\nHTML(animation.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nNow doing the same, but displaying the waves on the same frame.\n\nWaveAnimationDict = {}\n\ndef one_frame(d,WaveInfoList):\n    global WaveAnimationDict\n    #print(WaveInfoList)\n    #print(d)\n    ax.clear() \n    \n    # Certain code for the first three iterations\n    if d==0:\n        WaveInfoList = np.array([WaveInfoList[0]])\n        WaveList = WaveCreationPlotter(WaveInfoList,False,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='red')\n        \n    elif d==1:\n        WaveInfoList = np.array([WaveInfoList[1]])\n        WaveList = WaveCreationPlotter(WaveInfoList,False,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='green')\n        ax.plot(WaveAnimationDict['0'],color='red')\n        \n    elif d==2:\n        WaveList = WaveCreationPlotter(WaveInfoList,True,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='blue')\n        ax.plot(WaveAnimationDict['0'],color='red')\n        ax.plot(WaveAnimationDict['1'],color='green')\n    \n    WaveAnimationDict[str(d)] = Wave\n    \n    # Plotting each frame\n    ax.set_xticks(np.arange(0,len(Wave),Sampling_Rate),\n               np.arange(0,len(Wave)/Sampling_Rate,1))\n    ax.set_ylim((-2,2))\n    ax.set_ylabel(\"Amplitude\")\n    ax.set_xlabel(\"Time (Seconds)\")\n    ax.set_title(\"Simple Superposition Animation 2\")\n\n\nWaveInfoList = [[1,5,4000],[2,5,4000]]\nfig,ax = plt.subplots()\nanimation = FuncAnimation(fig, one_frame, frames=3, fargs = (WaveInfoList, ), interval=1000, repeat=True)\nplt.close()\nHTML(animation.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/MP8.html#phase-difference-animation",
    "href": "posts/MP8.html#phase-difference-animation",
    "title": "Masters Project 15/11/2022",
    "section": "9.2 Phase Difference Animation",
    "text": "9.2 Phase Difference Animation\nWave1, red, stays the same.  Wave2, green, moves to the right (as phase difference increases).  Wave3, blue, is the superposition of both. \n\nWaveAnimationDict = {}\n\ndef one_frame(d,WaveInfoList,PhaseDiff,PhaseInc):\n    global WaveAnimationDict\n    ax.clear() \n    # Certain code for the first three iterations/frames\n    if d==0:\n        WaveInfoList = np.array([WaveInfoList[0]])\n        WaveList = WaveCreationPlotter(WaveInfoList,False,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='red')\n    elif d==1:\n        WaveInfoList = np.array([WaveInfoList[1]])\n        WaveList = WaveCreationPlotter(WaveInfoList,False,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='green')\n        ax.plot(WaveAnimationDict['0'],color='red')\n    elif d==2:\n        WaveList = WaveCreationPlotter(WaveInfoList,True,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='blue')\n        ax.plot(WaveAnimationDict['0'],color='red')\n        ax.plot(WaveAnimationDict['1'],color='green')\n    else: # For the fouth and onwards iterations/frames\n        PhaseDiff = PhaseDiff + (d-2)*PhaseInc\n        # The first wave stays the same\n        ax.plot(WaveAnimationDict['0'],color='red')\n        # The second wave is recalculated with PhaseDiff\n        PWaveInfoList = WaveInfoList[1].copy()\n        if len(WaveInfoList[1]) == 3:\n            PWaveInfoList.append(PhaseDiff)\n        else:\n            PWaveInfoList[3] = PWaveInfoList[3] + (d-2)*PhaseInc\n        WaveInfoList = np.array([PWaveInfoList])\n        WaveList = WaveCreationPlotter(WaveInfoList,False,False)\n        Wave = WaveList[0]\n        ax.plot(Wave,color='green')\n        # The third wave is recalculated as the new superposition of the previous 2\n        CombinedWave = WaveAnimationDict['0'] + Wave\n        ax.plot(CombinedWave,color='blue')\n    \n    WaveAnimationDict[str(d)] = Wave\n    \n    # Plotting each frame\n    ax.set_xticks(np.arange(0,len(Wave),Sampling_Rate),\n               np.arange(0,len(Wave)/Sampling_Rate,1))\n    ax.set_ylim((-2,2))\n    ax.set_ylabel(\"Amplitude\")\n    ax.set_xlabel(\"Time (Seconds)\")\n    ax.set_title(\"Phase Difference = {}\".format(PhaseDiff))\n\n\n# Initial Phase Difference\nPhaseDiff = -180\n# Phase difference increments by frame\nPhaseInc = -10\n\nWaveInfoList = [[1,5,4000],[1,5,4000,PhaseDiff]]\nfig,ax = plt.subplots()\nanimation = FuncAnimation(fig, one_frame, frames=39, fargs = (WaveInfoList, PhaseDiff, PhaseInc), interval=250, repeat=True)\nplt.close()\nHTML(animation.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/MP8.html#common-research-questions",
    "href": "posts/MP8.html#common-research-questions",
    "title": "Masters Project 15/11/2022",
    "section": "10.1 Common research questions",
    "text": "10.1 Common research questions\nWhat is an ‘acoustic’ community in a habit?  What transmission properties of the environment shaped signal structure?  How animal acoustic signals vary in space and time?"
  },
  {
    "objectID": "posts/MP8.html#acoustic-analysis-workflow",
    "href": "posts/MP8.html#acoustic-analysis-workflow",
    "title": "Masters Project 15/11/2022",
    "section": "10.2 Acoustic Analysis Workflow:",
    "text": "10.2 Acoustic Analysis Workflow:\n\n\n\nAAW.png\n\n\nThis is about statistical approaches (‘classical’). The literature review I’m familiar with is about ML approaches. Most of the problem is often the annotations, the latter steps like feature extraction can straightforward. The speaker emphasises this point."
  },
  {
    "objectID": "posts/MP8.html#annotation-format.",
    "href": "posts/MP8.html#annotation-format.",
    "title": "Masters Project 15/11/2022",
    "section": "10.3 Annotation Format.",
    "text": "10.3 Annotation Format.\nYou give a sound file, say 5 minutes long. For each label, we give when time the signal starts and ends. We can also give the stereo channel if it is two channel, and the sound’s frequency range."
  },
  {
    "objectID": "posts/MP8.html#ohun-automatic-detection-package",
    "href": "posts/MP8.html#ohun-automatic-detection-package",
    "title": "Masters Project 15/11/2022",
    "section": "10.4 Ohun automatic detection package",
    "text": "10.4 Ohun automatic detection package\nA package for detection (classification). First we manually add labels (annotations/reference) to a subset of data. We do this on a spectrogram. E.g. \nAnd find the detection parameters that are the best at finding these labels. Then apply it to the rest of our data as needed."
  },
  {
    "objectID": "posts/MP8.html#template-based-detection",
    "href": "posts/MP8.html#template-based-detection",
    "title": "Masters Project 15/11/2022",
    "section": "10.5 Template-based detection",
    "text": "10.5 Template-based detection\nTemplate-based detection (TBD) uses cross correlation to tell how similar two audios are. TBD uses mel frequency or Fourier spectrograms.\nIt takes a template, an example you think is similar to most other relevant signals. Then it compares the amplitude of the template with each of the steps across the recording.\nOn the graph below the spectrogram, if the audio goes above the line, it implies a good match between the template and the sound in the recording. This is/can be used with a threshold. Everything above the threshold we set will be seen as a detection, and everything below ignored.\n\n\n\nTBD.png"
  },
  {
    "objectID": "posts/MP8.html#feature-extractionquantity-variation",
    "href": "posts/MP8.html#feature-extractionquantity-variation",
    "title": "Masters Project 15/11/2022",
    "section": "10.6 Feature Extraction/Quantity variation",
    "text": "10.6 Feature Extraction/Quantity variation\nThis is using the annotation format columns/data and creating new columns (features) which make classification easier.\nA package warbleR has some useful features it can create.\n\n10.6.1 Spectral and Mel frequency Spectrogram features\nE.g. 27 features relating to the distribution of energy in the time and frequency domain and contours. Features relating to the harmonicity (related to phase?) of the signals. These features are like an absolute measure of the spectrogram features. Like the pitch.\n\n\n10.6.2 Pairwise acoustic similarity: time-frequency cross-correlation and dynamic time warping.\nSpectrographic cross-correlation. A pairwise similarity metric. Unlike the previous features, these ones are about understanding the similarities (correlations) between sounds. How similar two sounds are based on their spectrogram, it’s a great way to understand spectrogram structure.\nThe module returns a similarity matrix. Each element correlates to how similar signals are. The diagonals are 1 because a signal is exactly the same to itself."
  },
  {
    "objectID": "posts/MP8.html#python-packages",
    "href": "posts/MP8.html#python-packages",
    "title": "Masters Project 15/11/2022",
    "section": "10.7 Python Packages",
    "text": "10.7 Python Packages\nThere are more not listed.  scikit-maad for feature extraction.  .biosound too."
  },
  {
    "objectID": "Test Posts/post-with-code/index.html",
    "href": "Test Posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "Test Posts/welcome/index.html",
    "href": "Test Posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]