{
 "cells": [
  {
   "cell_type": "raw",
   "id": "61dc2fd5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"fast.ai Lesson 3\"\n",
    "author: \"Adnan Jinnah\"\n",
    "date: \"2022-09-19\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "jupyter: python3\n",
    "categories: [fastai]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602403fe",
   "metadata": {},
   "source": [
    "# Intro\n",
    "3: Neural Net Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e0ea7",
   "metadata": {},
   "source": [
    "# Lesson Overview\n",
    "In this lesson the mechanisms behind deep learning are explored. Unlike the previous lesson, which was focused on applications and many pieces of new software, this lesson was focused on reading through a textbook chapter and understanding how exactly deep learning operates.\n",
    "\n",
    "I suppose this knowledge will be required in order to understand how best to create good deep learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60847a6b",
   "metadata": {},
   "source": [
    "# The topics covered, briefly\n",
    "- How are images represented in a computer.\n",
    "- How are files in datasets structured.\n",
    "- Python functionality: what is list comprehension, and fact that NumPy/PyTorch are much faster than pure python.\n",
    "- Tensors: a list like structure used in deep learning. Ranks indicating dimensions and shapes indicating the length of each axis.\n",
    "- Loss functions: RMSE and L1 norm. Two loss functions, similar but RMSE penalises larger errors more.\n",
    "- SGD: stochastic gradient descent. A way to make a model learn by updating its weights automatically.\n",
    "- The difference between loss and metrics: Loss is for updating weights, metrics are for human evaluation.\n",
    "- Mini-batches. Each epoch, instead of using one piece of training data to update weights (pure SGD), or using all the training data simultaneously to update the weights (GD), splitting the data into random batches (subsets) of the training data to update the weights.\n",
    "- The sigmoid function: a smooth curve between 0 and 1 used in deep learning as a loss function. It is used because it has a meaningful derivative so allows good weight updates.\n",
    "- DataLoader class: a function to take a dataset and split it into random mini-batches. DataLoaders contains a training a training and validation DataLoader.\n",
    "- ReLU: a rectified linear unit. It's just a linear function, a line, with negative values = 0. \n",
    "- Activation functions. Also known as nonlinear functions, these are placed between layers of linear functions in neural networks so that the linear functions do not combine into another linear function.\n",
    "- The Universal Approximation Theorem: that two linear layers with an activation function inbetween can approximate any function given the correct weights and enough nodes. In practice however we use more than two linear layers because it works better.\n",
    "\n",
    "Much more content was covered in much more detail, including how to program a deep learning model from scratch in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a079de",
   "metadata": {},
   "source": [
    "# Links\n",
    "The course page for this sessions is https://course.fast.ai/Lessons/lesson3.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository at https://github.com/exiomius/PDL-Lesson-3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
