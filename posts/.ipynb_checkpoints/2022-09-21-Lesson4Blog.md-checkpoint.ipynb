{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2c754cc3",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"fast.ai Lesson 4\"\n",
    "author: \"Adnan Jinnah\"\n",
    "date: \"2022-09-21\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "jupyter: python3\n",
    "categories: [fastai]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49a9d6",
   "metadata": {},
   "source": [
    "# Intro\n",
    "4: Natural Language (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d993f3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Lesson Overview\n",
    "This lesson was an introduction to natural language processing (NLP). NLP includes tasks such as text classification (e.g. positive or negative sentiment) and text generation (creating new text from a prompt). It appears to be an immensely growing field and as a result certain modules are struggling to keep up to date with the state-of-the-art. A new technique called transformers has recently become popular, and FastAI's library for example has not been updated with its functionality yet, despite the authors working on it. As a result, this lesson's lecture and notebooks covers HuggingFace's transformer module, while the textbook still uses the older FastAI methods.\n",
    "\n",
    "This dilemma reflects a common issue with growing fields. As the state-of-the-art changes so quickly, one must take care to be flexible and be comfortable with learning many modules and ways of doing things. It doesn't seem to bad however, because the fundamentals behind the techniques are still not often changed, so learning it once properly will make learning newer techniques quick. Furthermore, updated modules often have really good documentation, tutorials and discussions available. HuggingFace seems to have some brilliant transformer documentation across their webpages and GitHub. Lastly, this 'dilemma' actually might be pretty entertaining. Seeing your field grow with changes and improvements and having to learn about them may keep working pretty interesting. In a way it is like also having to be wear the hat of an academic as a part of your usual job role. The variation could be fun. \n",
    "\n",
    "# Notebooks covered\n",
    "\n",
    "This lesson covered two notebooks,[Getting started with NLP for absolute beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners) and [Iterate like a grandmaster!](https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster/notebook)\n",
    "\n",
    "## \"Getting started with NLP for absolute beginners\" covers:\n",
    "- How to use Kaggle. Including how to use Kaggle datasets on your own PC and how to submit entries to their competitions.\n",
    "- How to use Pandas and Transformers DataFrames to view data, and get it into the correct format for NLP model training.\n",
    "- How to use Transformers to train and classify. \n",
    "- The difference between training, validation, and testing data. An emphasis has been put on creating good validation data with a quote by Dr Rachel Thomas stating that often a poorly chosen validation dataset results in a disconnect between development and deployment performance. Her [article](https://www.fast.ai/posts/2017-11-13-validation-sets.html) describes this in more detail.\n",
    "- Training data is to train. Validation data is get an idea of generalizable performance, but often it is limited in doing so. This is often either because it hasn't been chosen prudently enough or because one has accidentally overfitted to it. Imagine a model to predict the price of a stock. Randomly selecting points to be validation data is poor because that is not how the model will be used in practice and is a much easier task. Selecting some amount of further price movement to be validation data makes sense, but your model may overfit to the specific movement pattern of that timeframe. \n",
    "- The Pearson Correlation Coefficient, r, as a metric is discussed. Emphasis is put on firstly trying metrics on datasets to understand them, rather than delving immediately into the maths. Doing so for example yielding the fact that r is really sensitive to outliers. \n",
    "- On outliers. Outliers are important parts of the data and mustn't be removed without reason. In our housing income and average number of rooms correlation example, there are outliers that could lead to some insights about the data. Perhaps the outliers are from a specific type of house or a specific geographical area. In this case, it may make more sense to use separate models to train on and predict separate clusters of data.\n",
    "- On hyperparameters. Learning rate is the most important in this case. The idea is to find the largest value that doesn't result in failed training. FastAI provides a learning rate finder to help you, but Transformers does not.\n",
    "\n",
    "## \"Iterate like a grandmaster!\" covers:\n",
    "- How a grandmaster Kaggle competitor works. He focused on creating an effective validation set and iterating rapidly to find changes which improve validation set results. These skills carry over to real projects.\n",
    "- For the patent classification, the input is anchor and label is target. In the test data, there are anchors not present in the training data. Thus we should make sure there are anchors in the validation data that are not in the training data.\n",
    "- Pick a learning rate and batch size that fits your GPU. This means picking them so that we can iterate rapidly to test things out.\n",
    "- Pick a reasonable weight decay.\n",
    "- Pick a small number of epochs, like 3, to test with. This is because in practice much of the performance will be made in those. Thus there is no need to run many epochs every time you try a change. If there is not improvement within a few epochs, then your change is likely not very significant. Later on, when you want to more thoroughly evaluate a change, you can use more epochs and cross-validation.\n",
    "- Pick a class to setup your arguments for your trainer. Transformers by default uses one, but FastAI has others.\n",
    "- You need a stable validation accuracy from your epochs to know whether your future changes is making improvements. To know whether your predictions are stable, run the model from scratch a few times, say 3, and check how much it varies.\n",
    "- To make changes easier, create a function to setup tokenization and a function to setup model creation. Then you can pass parameters quickly to create new models.\n",
    "- In this case, previously we tokenised with a special token sep to indicate seperate entities in our input. Simply changing sep to s resulting in a big performance increase.\n",
    "- Instead of using the same special token to indicate separate entities in our input, using different special tokens for each entity could better inform the model that each entity is different.\n",
    "- Simply changing all text to lowercase can often help a little too.\n",
    "- There's so many things you could try. In this notebook, most of the iteration was done by changing tokenisation, but also playing around with the other parameters might yield better results. However, instead of trying to optimise the factors already present, there are other ideas you can try. Firstly fine tuning your general language model using just patent data. Or using a model pretrained on legal vocabulary instead of general vocabulary. Using a different type of model, not in terms of architecture but a model created for a different task. One of our columns is 'context', which is a code e.g. B7 referring to a patent context. Instead of using the code, we could replace it with a description found online. There's so many things you can try especially if you think a little creatively.\n",
    "- Remember for the final submission, to train on your validation data as well.\n",
    "\n",
    "# A subset of this chapter's questions and my shortened answers are as follows:\n",
    "\n",
    "- What is self-supervised learning? When you don't use labeled data. You make the model divide the data into input and label itself. Self-supervised learning is often used to train models to be used as a transfer model for a different task such as classification.\n",
    "- What is a language model? A model trained to predict the next word of an input text. \n",
    "- Why do we fine-tune language models? Because the pretrained model wouldn't have had specifically been calibrated for your specific task. \n",
    "- What is Universal Language Model Fine-Tuning? It is an approach to NLP including transfer models. In order: Find/train a general language model, transfer/fine tune it to make a task specific language model, transfer/fine tune it to make a final model for the task at hand.\n",
    "- How does one need to prepare their data for a language model? Tokenisation, Numericalization and DataLoading must be done. \n",
    "- What is Tokenisation? Tokenisation: Convert the text into a list of words/characters/substrings. The way you do this has to the same throughout all the models you use. If you grab a model online, you have to tokenise your models in the same way.\n",
    "- What is Numericalization? Numericalization: First make a vocab list/dictionary of all the words used in the dataset. They will all be given unique numbers to identify them. Then convert the tokenised text into a list of these numbers.\n",
    "- What is DataLoading? In FastAI LMDataLoader automatically sets the last token in a sequence as a label, as well as other important data preparation processes.\n",
    "- Many details about tokenisation are discussed including special characters, rules, and repeated characters.\n",
    "- Why would a model tokenise some words as 'unknown'? Because there is a limit to the number of words/tokens the vocab list/embedding matrix contains. \n",
    "- Why is padding needed for text classification but not language modeling? Because PyTorch's DataLoader(s) uses tensors that can only store elements of the same type and size. In text classification, the text/documents in tensors are of varying size. In language modeling, they are not.\n",
    "- What is an embedding matrix? I need to spend more time on this because it is a little complicated. It contains a list of vector representations of all the tokens present in the vocab list. \n",
    "- What is perplexity? A performance metric used for judge NLP models. \n",
    "- What is gradual unfreezing? It's a way of training a model. It is unfreezing one layer at a time. Unfreeze a layer, then training that layer’s parameters, then unfreezing another layer and then train, etc until all layers are unfrozen.\n",
    "- Why is text generation always likely to be ahead of automatic identification of machine-generated texts? Because classifier identification models can be used to create better generation models. And in a way by definition, a better classifier model can only be trained on a better generation model after it has already been released online.\n",
    "\n",
    "# Links\n",
    "\n",
    "The course page for this sessions is https://course.fast.ai/Lessons/lesson4.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my github repository at https://github.com/exiomius/PDL-Lesson-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
