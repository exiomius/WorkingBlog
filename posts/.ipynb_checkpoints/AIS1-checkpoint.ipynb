{
 "cells": [
  {
   "cell_type": "raw",
   "id": "938848c6",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"AIS 1: Artificial General Intelligence\"\n",
    "author: \"Adnan Jinnah\"\n",
    "date: \"2022-10-22\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "jupyter: python3\n",
    "categories: [AIS]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab697dd",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This is the first post in a series about AI safety (AIS). I am apart of the effective altruism AI discussion group at Durham, and these posts will be detailing my experience and notes about the topics we discuss.\n",
    "(This post is still in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24956b9",
   "metadata": {},
   "source": [
    "# Course Details\n",
    "We will be following the AGISF curriculum at https://www.agisafetyfundamentals.com/ai-alignment-curriculum. This includes 8 weeks of readings and discussions, followed by a final project. The Core readings and homework can be found here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7b0fc",
   "metadata": {},
   "source": [
    "# Homework\n",
    "For week 1, the homework is to read all 5 core readings in Week 1 the AGISF Curriculum and answer down answers to some questions.\n",
    "This week, it's Exercise 1, and Discussion Prompts 2,3 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32e4f9",
   "metadata": {},
   "source": [
    "## Core Reading 1: Four Background Claims\n",
    "Claim 1: Humans have a very general ability to solve problems and achieve goals across diverse domains. \n",
    "We generally call this intelligence/general intelligence but there's isn't a former definition for intelligence. This makes sense because unlike a definite property like height, it's impossible to define how intelligent a human is let alone make a mathematical definition.\n",
    "Whatever intelligence may be, we might be able to replicate it in code, or maybe not. Do we even need to know what intelligence is to replicate it?\n",
    "\n",
    "Alternative view: General intelligence doesn't exist. Actually humans just have a collection of separate specific modules/functions that add up together to seem as if we have a general problem solving ability. Computers can't get general intelligence, they can only get better at specific tasks. Even if this view was true, what's theoretically stopping a computer from getting good at enough separate specific tasks until, like a human, it seems to have general intelligence?\n",
    "\n",
    "Short response: The author finds this \"disparate modules\" hypothesis implausible because humans can gain skill in areas that early humans (I presume evolved for specific tasks) have no experience in (I presume tasks unevolved for).. He thinks general intelligence probably comprises from a number of different cognitive modules and interactions, isn't not just one module called the general intelligence module that humans have. As a result of these modules and interactions, humans have cognitive adaptability more so than chimpanzees for example. \n",
    "\n",
    "I agree that general intelligence probably comprises from a number of different cognitive modules and their interactions. It can't be a specific module that is just general intelligence that is well defined and can be replicated easily. However, his rebuttle of the disparate modules hypothesis doesn't exactly make sense to me. Even if we've not been evolved to do these new tasks, if we were evolved a large enough amount of task specific modules, then we can learn to do them anyway?\n",
    "\n",
    "Why this claim matters: because humans became dominant because they are more intelligent. We share chimpanzees as ancestors, then a few million years later gained this 'general intelligence'. A few million years by evolutionary standards isn't very long. This implies there a few key ideas/changes between chimpanzees and the first humans that result in our intelligence. If we knew these, then perhaps we could make very intelligent AI systems.\n",
    "\n",
    "## Claim 2: AI systems could become much more intelligent than humans.\n",
    "At MIRI, they don't have conviction about when smarter-than-human AI will be developed, but expect that a) they will eventually be developed (probably within a century) and b) they can become significantly more intelligent than any human.\n",
    "\n",
    "Alternative view 1: The human brain does something special that can't be replicated by a computer.\n",
    "\n",
    "Short response: Brains are physical systems. If certain versions of the Church-Turning thesis hold, then computers theoretically can replicate the functional input/output behaviour of any physical system, including the brain. I think this means that the brain is a physical system, a collection of physical objects that take an input and form and output, a computer can replicate the process: it can take the same input(s) and produce the same output(s). Even if there was something like qualia, the qualitative component of consciousness, that can't be replicated, it doesn't matter, unless it's important for our brain's problem-solving computation process of taking an input and giving an output. Computers can replicate the brain's problem-solving computation process and problem solve/gain intelligence regardless.\n",
    "\n",
    "(The Church-Turning thesis states that a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine. A turning machine can implement any computer algorithm.) \n",
    "\n",
    "Alternative view 2: That the algorithms at the root of general intelligence are too complex for human beings to be able to program for many centuries.\n",
    "\n",
    "Short response: Looking at the previous claim's conclusion, the cognitive advantage/general intelligence in humans took a extremely short evolutionary time frame. The general intelligence part of humans that sets us apart from less intelligent species mustn't be extremely complicated because of this. The building blocks of general intelligence must be present in chimpanzees too, but they didn't have the shift we did. I'm not sure I agree because I question the assumption that evolutionary time reflects complexity.\n",
    "\n",
    "Alternative view 3: Humans are already at or near peak physically possible intelligence. Thus, although we may be able to build human-equivalent intelligent machines, we won’t be able to build superintelligent (smarter-than-human) machines.\n",
    "\n",
    "Short response: It seems very possible within the boundaries of physics to run a computer simulation of the human brain thousands of times faster than a real one. I think it really depends what we mean by speed. Does speed matter to reasoning? If there a ideal speed for the human brain that results in the best reasoning? Can a computer reach that speed reliably and so outreason a human?\n",
    "\n",
    "Computers can probably use computational resources more effectively than humans do, even at the same speed, if we program them correctly. Overall I think that humans are very suspect to issues with reasoning such as stress or bias, so if we can build human-equivalent intelligent machines, surely we can optimise them against these to make them smarter-than-humans.\n",
    "\n",
    "Why this claim matters: Already we know human-designed machines are orders of magnitudes better than any biological creature we know of at certain tasks. For example, cars and horses. We can build machines better than animals for tasks we care about, and cut out biological mechanisms irrelevant for it such as reproduction. We could build machines with the task of (narrow, even if only for specific tasks) intelligence that are better than humans without biological mechanisms like stress or bias tainting reason. This can solve the world's biggest problems by scientific and technological innovation, improving the world at unprecedented pace.\n",
    "\n",
    "Claim 3: If we create highly intelligent AI systems, their decisions will shape the future.\n",
    "\n",
    "\n",
    "Claim 4: Highly intelligent AI systems won’t be beneficial by default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174261e",
   "metadata": {},
   "source": [
    "## Core Reading 2: AGI safety from first principles (Ngo, 2020) \n",
    "(from section 1 to end of 2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39cd12",
   "metadata": {},
   "source": [
    "## Core Reading 3: More is different for AI (Steinhardt, 2022) \n",
    "(only introduction, second post, third post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bab52",
   "metadata": {},
   "source": [
    "## Core Reading 4: “Most important century” series summary (Karnofsky, 2021a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd0679",
   "metadata": {},
   "source": [
    "## Core Reading 5: Forecasting transformative AI: the “biological anchors” method in a nutshell (Karnofsky, 2021b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000e2b",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "As discussed in Ngo (2020), Legg and Hutter define intelligence as “an agent’s ability to achieve goals in a wide range of environments”: a definition of intelligence in terms of the outcomes it leads to. An alternative approach is to define intelligence in terms of the cognitive skills (memory, planning, etc) which intelligent agents used to achieve their desired outcomes. What are the key cognitive skills which should feature in such a definition of intelligence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24acb1",
   "metadata": {},
   "source": [
    "## Discussion Prompts 2\n",
    "One intuition for how to think about very smart AIs: imagine speeding up human intellectual development by a factor of X. If an AI could do the same quality of research as a top scientist, but 10 or 100 times faster, and with the ability to make thousands of copies, how would you use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f73238",
   "metadata": {},
   "source": [
    "## Discussion Prompts 3\n",
    "How frequently do humans build technologies where some of the details of why they work aren’t understood by anyone? What, if anything, makes AI different from other domains? Would it be very surprising if we built AGI without understanding very much about how its thinking process works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b452f94",
   "metadata": {},
   "source": [
    "## Discussion Prompts 5\n",
    "What are the most plausible ways for the hypothesis “we will eventually build AGIs which have transformative impacts on the world” to be false? How likely are they?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
