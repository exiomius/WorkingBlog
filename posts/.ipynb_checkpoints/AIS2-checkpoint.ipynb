{
 "cells": [
  {
   "cell_type": "raw",
   "id": "938848c6",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"AIS 2: Goals and Misalignment\"\n",
    "author: \"Adnan Jinnah\"\n",
    "date: \"2022-11-02\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "jupyter: python3\n",
    "categories: [AIS]\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab697dd",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This is the second post in a series about AI safety (AIS). These posts will contain my rewritten notes of and comments about the core readings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24956b9",
   "metadata": {},
   "source": [
    "# Course Details\n",
    "We will be following the AGISF curriculum at https://www.agisafetyfundamentals.com/ai-alignment-curriculum. This includes 8 weeks of readings and discussions, followed by a final project. The Core readings and homework can be found there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55c022",
   "metadata": {},
   "source": [
    "# Homework\n",
    "The homework this week is to read the four core readings of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4bdc1",
   "metadata": {},
   "source": [
    "# Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020)\n",
    "\n",
    "Specification gaming is satisfying the literal specification of an object without doing it in (a) the intended way. It's like a monkey's paw scenario, you make a wish (a specification or goal) and the model does it, but not how you would like.\n",
    "\n",
    "We consider specification gaming from two perspectives: from developing reinforcement learning (RL) algorithms and from aligned algorithms.\n",
    "\n",
    "We might not care whether the model does specification gaming for certain tasks. In fact, gaming the system can be seen as innovation in some cases, and lead us to understand new strategies about how to approach problems. However, if we want to build aligned agents, that solve problems in a way we understand and morally agree with, specification gaming can be a huge problem.\n",
    "\n",
    "For building aligned agents, designing task specifications (reward functions, environments, etc) that accurately reflect the intentions of humans tends to be very difficult. As reinforcement algorithms improve, this will only get harder. Therefore, it's imperative that as RL develops, so does research in the ability to design task specifications.\n",
    "\n",
    "Specification correctness is a spectrum from high to low. When combined with RL, high correctness leads to innovation, I.E, desirable novel solutions. With low correctness, we get 'gaming the system', I.E, undesirable novel solutions.\n",
    "\n",
    "One way we can cause specification gaming is by poorly designed reward shaping. For example, by rewarding the model for archiving intermediate goals instead of the full goal. In a racing game, for rewarding green blocks, not for winning the race. This can result in the model looping in circles to get as many green blocks as possible.\n",
    "\n",
    "To try and solve specification gaming, instead of trying to carefully design a perfect specification, we could get the model to learn the reward function from human feedback. There is another article detailing this.\n",
    "\n",
    "Specification gaming can also occur from exploiting bugs in the simulation. For instance, by exploiting the Physics engine.\n",
    "\n",
    "These issues were brought up in the context of video games or simulations, but are very concerning in real life context too. For example, a traffic avoiding model may game the traffic system by exploiting software bugs in traffic routing infrastructure. There are many things like this that designers may not have realised. As tasks get increasingly more complex, this only gets harder.\n",
    "\n",
    "The question is, it is possible to design agents that correct for developer's false assumptions about model procedure instead of gaming them?\n",
    "\n",
    "One such assumption is that the agent's actions do not affect the task specification. In simulations this is often true, but in the real world, the agent's actions could change the environment or specifications of the task. A traffic model, instead of directing users properly, could instead psychologically nudge them to satisfied by passing certain landmarks, which achieves the specification of user satisfaction, but by manipulating the objective/specification. This is reward tampering. A extreme example is an AI directly hacking the computer running it to manually set its reward signal to be very high.\n",
    "\n",
    "We want to implement \"avoidance of reward tampering\", to try and remove incentives to tamper with the reward channel.\n",
    "\n",
    "To summarise specification gaming:\n",
    "\n",
    "1. How do we capture human intent properly in a reward function? <br>\n",
    "2. How do we avoid making mistaken (implicit) assumptions about the domain/environment, or design agents that correct these mistaken assumptions for us instead of gaming them. <br>\n",
    "3. How do we avoid reward tampering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98175a8",
   "metadata": {},
   "source": [
    "# Goal misgeneralization in deep reinforcement learning (Langosco et al., 2022) (ending after section 3.3)\n",
    "(Those with less background in reinforcement learning can skip the parts of section 2.1 focused on formal definitions.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba802c",
   "metadata": {},
   "source": [
    "# Superintelligence, Chapter 7: The superintelligent will (Bostrom, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f64bd",
   "metadata": {},
   "source": [
    "# What misalignment looks like as capabilities scale (Ngo, 2022) \n",
    "(only the section titled Realistic training processes lead to the development of misaligned goals, including phases 1, 2 and 3) (30 mins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
