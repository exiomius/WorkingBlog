{
 "cells": [
  {
   "cell_type": "raw",
   "id": "48e299ec",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"fast.ai Lesson 6\"\n",
    "author: \"Adnan Jinnah\"\n",
    "date: \"2022-09-26\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "jupyter: python3\n",
    "categories: [fastai]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae751ef",
   "metadata": {},
   "source": [
    "# Intro\n",
    "6: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e699c5",
   "metadata": {},
   "source": [
    "# Lesson Overview\n",
    "This lesson focused on the main alternate to deep learning: Random Forests and Gradient Boosting. There is an excellent summary at the end of the book's chapter giving context into why and when to use deep learning and/or RF/GB. Essentially, the latter is great for tabular data, and the former for more complex messy input like natural language, images etc. How to create a Random Forest from scratch is covered, as well as some links describing Gradient Boosting in more depth. Lastly, the notebooks [road to the top part 1](https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1) and [part 2](https://www.kaggle.com/code/jhoward/small-models-road-to-the-top-part-2) are covered giving more insight into how to create good ML models in a competitive setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2633688",
   "metadata": {},
   "source": [
    "# The topics covered, briefly\n",
    "- Random Forests: how to create them from scratch, how they are hard to do incorrectly, how they are hard to overfit, how their hyperparamaters work, what bagging and out-of-bag error is, how they are great for tabular data and are especially good due to their easy understandability and interpretation.\n",
    "- OneR vs TwoR classifier differences and creation.\n",
    "- By comparing OOB error with validation error, you can tell if there are issues with the way your validation data is constructed and/or if there is data leakage.\n",
    "- A list of vision models to use sorted by iteration speed and accuracy.\n",
    "- More insight how to compete in machine learning competitions by creating good validation sets and iterating quickly.\n",
    "- Issues with model iteration speed and CPU/GPU usage. Ideally, pick a model that can utilise both correctly instead of being bottlenecked by either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51c47b",
   "metadata": {},
   "source": [
    "# Lecture Notes\n",
    "- Binary splits. They find and use the best value to split a dataset into two parts to predict the dependent variable.\n",
    "- We found Sex to be the best predictor from the columns for surviving the titanic, with the score to be about 0.4, a score being a measure of how good the split is at predicting. \n",
    "- If we predicted all women to survive and all men to die, it would do a decent job. This is a called a OneR classifier.\n",
    "- How to improve this? Use a TwoR classifier, look into the male and female groups separately and use another column to split again!\n",
    "- This gives us a decision tree! A series of binary splits. DecisionTreeClassifier, from sklearn, does it for us. It takes leafnodesize as a variable, meaning for the final groups at the end, at minimum how many datapoints need to be in them.\n",
    "- Gini is like score, it's another metric for measuring how good the split is. \n",
    "- TwoR might not necessarily be better than OneR at predicting, especially for small datasets. \n",
    "- For tabular data, always use a decision tree. It requires very little preprocessing (don't have to convert categorical data, for example) and is a good baseline.\n",
    "- Bagging: Build many decision trees using different binary splits and subsections of the training data. Each tree will have an unbiased amount of error: error that is random and uncorrelated. If we average out the errors that are random and uncorrelated we can get very little error left! So build an ensemble of decision trees! This is called a random forest.\n",
    "- For datasets of decent size, Random Forests almost always are better than OneR. \n",
    "- A feature importance plot tells us how important each feature is! It's amazing for understandability. Maybe for a tabular dataset, we use a random forest feature importance plot first for a baseline and understanding of feature importance, then create a DL model with that comparison and understanding in mind.\n",
    "- After about 30 trees, the improvement for random trees doesn't improve massively. Roughly < 100 works best. More forests does not cause overfitting, in fact, having too few trees might. This is the same as having a small ensemble of overfitted models could be overfitted, but a large ensemble will not be overfitted.\n",
    "- Because when creating trees, we didn't train each one on all of the training data, say on a random subsection of 0.75 of it. We can then use the final 0.25 section as validation data for each tree. Then average this error across all our trees. This is called the OOB (out-of-bag) error. \n",
    "- For the random forest, we might be able to get away without using validation data to see if there's overfitting by using OOB instead. \n",
    "- Random forests are amazing for model interpretation. It gives confidence in predictions, which columns are strong predictors, how related are columns are to eachother etc.\n",
    "- If all the trees are predicting very different things for the same entry, then we are not very confident. \n",
    "- A partial dependance plot uses the forest's predictions to set all the other data equal and tells us how two variables are related. There is an important distinction. Say we want to know how car price depends on the year of manufacture. We could just plot a graph of car prices and manufacture year. This however would depend on other variables: say another present column, air conditioning (yes/no), greatly improves the price. In contrast, PDP will get the prediction input data and make all the columns' data the same except for year of manufacture. Then it will make predictions for car price using that, and plot it against year of manufacture. Unlike the first plot, it tells us a different measure of how year of manufacture affects price.\n",
    "- Gradient boosting is another meta technique using binary splits like random forests. It's can more accurate but can overfit so can be done wrong. More info here: https://explained.ai/gradient-boosting/.\n",
    "\n",
    "## The lecture than goes through a more in depth notebooks than Iterate like a grandmaster called [road to the top part 1](https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1) and [part 2](https://www.kaggle.com/code/jhoward/small-models-road-to-the-top-part-2):\n",
    "- Just squish images, it's better than cropping. Padding would take more precious CPU power for iteration? (See point below). TTA, augmenting input images, can improve accuracy too.\n",
    "- There is another notebook: https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning, showing the best models for vision to use for quick iteration and testing.\n",
    "- The fastai learning rate suggestions are a bit conservative so you can pick higher than them.\n",
    "- Create different notebooks for different approaches. Duplicate and rename them.\n",
    "- Kaggle's GPUs aren't amazing, but for Jeremy his home PC ran training so much faster. The problem wasn't GPU, it was CPU, the Kaggle CPU indicator showed it as full. This is a image input problem. Loading them up requires CPU power. Simply resizing the images by 1/4 quickened up iteration by 4x but without sacrificing accuracy! Perhaps larger image sizes aren't so important. But later on, with some data augmentation, using larger image sizes helped.\n",
    "- Since the GPU was barely used, you might as well switch to a model that's more demanding. It probably won't take much longer.\n",
    "- Keep upto date with new model architecture. ConvNext is a great default speed and performance for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2644b",
   "metadata": {},
   "source": [
    "# Links\n",
    "The course page for this sessions is https://course.fast.ai/Lessons/lesson6.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
